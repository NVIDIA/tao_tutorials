{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34aba6e",
   "metadata": {},
   "source": [
    "### Notebook to demonstrate TAO RT-DETR Object Detection and Distillation through FTMS \n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "---\n",
    "\n",
    "### TAO Distillation \n",
    "\n",
    "Knowledge distillation is a powerful new feature in NVIDIA TAO that enables the training of smaller, faster, and more efficient models without sacrificing accuracy. This technique works by leveraging a high-performing, pre-trained teacher model to guide the training of a smaller student model. During training, the student learns not only from the ground truth labels but also from the richer outputs of the teacher, which often capture subtle patterns and generalizations that raw labels alone cannot provide.\n",
    "\n",
    "By mimicking the behavior of the teacher, the student model can achieve higher accuracy than it would through conventional training methods alone. This is especially valuable when training lightweight models intended for edge or real-time applications, where computational resources are limited but high accuracy is still critical.\n",
    "\n",
    "Knowledge distillation is ideal in two key scenarios: (1) when your small model isn't reaching the desired accuracy, and (2) when your larger model performs well but is too slow or resource-intensive for deployment. In both cases, distillation can bridge the gap between speed and accuracy, helping you deploy high-performing models that meet real-world constraints.\n",
    "\n",
    "<img src=\"assets/distillation_rtdetr_workflow_diagram.png\" width=\"500\"/>\n",
    "\n",
    "---\n",
    "The following table showcases a study we did with convnext_large and convnext_tiny on COCO dataset. The student model achieved much better mAP than finetuning it directly on the COCO dataset.\n",
    "| RT-DETR models            | Pretrained weights | mAP  |\n",
    "|---------------------------|--------------------|------|\n",
    "| Baseline (convnext_tiny)  | ImageNet 22K       | 50.0 |\n",
    "| Teacher (convnext_large)  | ImageNet 22K       | 53.0 |\n",
    "| Distilled (convnext_tiny) | ImageNet 22K       | 51.2 |\n",
    "\n",
    "---\n",
    "\n",
    "### Sample prediction of a trained RT-DETR model\n",
    "\n",
    "<img align=\"center\" src=\"sample_images/detection_sample.jpg\" width=\"640\">\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "The output of the notebook is to train a larger object detection model as a teacher and a light weight distilled student object detection model that does better than training the student model from scratch.\n",
    "\n",
    "Detailed instructions about the E2E workflow are covered in the TAO [documentation](https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/object_detection/rt_detr.html).\n",
    "\n",
    "---\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "This notebook will show how to take an RT-DETR hard hat detection model with a ConvNext Large backbone and distill it into a model 1/4 of the size with the same accuracy using TAO fine tuning microservices. \n",
    "\n",
    "1) Configure Connection to TAO FTMS \n",
    "2) Login & Create Cloud Workspace\n",
    "3) Register Train, Validation and Test datasets\n",
    "4) Train Teacher model\n",
    "5) Distill Student model\n",
    "6) Export student model and test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8cd6a",
   "metadata": {},
   "source": [
    "---\n",
    "### Requirements\n",
    "Prior to running this notebook you must have: \n",
    "1) A TAO FTMS server.  [(Setup Guide here)](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)\n",
    "2) The sample detection dataset from the [hardhat_detection_coco_dataset_format.ipynb](https://github.com/NVIDIA/tao_tutorials/tree/main/notebooks/tao_api_starter_kit/dataset_prepare/hardhat_detection_coco/hardhat_detection_coco_dataset_format.ipynb) notebook uploaded to your cloud storage.\n",
    "3) Set the `<>` enclosed variables with values in the Configuration section of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a811e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Debugging Finetuning Microservice and Jobs\n",
    "\n",
    "When working with the TAO API, you may encounter issues at different stages. Use the following guidance to debug effectively:\n",
    "\n",
    "#### 1. Dataset, Experiment, or Workspace CRUD Operation Errors\n",
    "\n",
    "If you encounter errors related to creating, reading, updating, or deleting datasets, experiments, or workspaces **and the error messages are not clear**, check the logs of the TAO API service pods:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
    "```\n",
    "\n",
    "#### 2. Errors During Job Launch\n",
    "\n",
    "For issues that occur **while launching a job**, check both the app and workflow pods:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
    "kubectl logs -f <pod name starting with tao-api-workflow-pod>\n",
    "```\n",
    "\n",
    "#### 3. Errors After Job Launch\n",
    "\n",
    "If errors occur **after a job has been launched**, inspect the job pod logs:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f tao-api-sts-<job_id>-0\n",
    "```\n",
    "\n",
    "> **Note:**  \n",
    "> Run these `kubectl` commands on the machine where your Kubernetes service is deployed.\n",
    "\n",
    "#### Additional Debugging Tips\n",
    "\n",
    "- **Job logs are automatically uploaded** to your cloud workspace at:  \n",
    "  `/results/<job_id>/microservices_log.txt`\n",
    "\n",
    "- **You can also view logs via the Jobs API endpoint:**  \n",
    "  ```\n",
    "  /api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs\n",
    "  ```\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Error Type                | Where to Check Logs                                      |\n",
    "|-------------------------- |---------------------------------------------------------|\n",
    "| CRUD operation errors     | `tao-api-app-pod`                                       |\n",
    "| Job launch errors         | `tao-api-app-pod`, `tao-api-workflow-pod`               |\n",
    "| Post-launch job errors    | `tao-api-sts-<job_id>-0`                                |\n",
    "| All job logs (cloud)      | `/results/<job_id>/microservices_log.txt`               |\n",
    "| All job logs (API)        | `/api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a51252",
   "metadata": {},
   "source": [
    "---\n",
    "### Performance Benchmarks\n",
    "\n",
    "#### Execution Time Breakdown\n",
    "\n",
    "The following table shows the approximate time required for each stage of the RT-DETR distillation workflow:\n",
    "\n",
    "| **Stage** | **Duration** | **Description** |\n",
    "|-----------|--------------|-----------------|\n",
    "| Train Dataset Pull | **15min** | Train dataset verification and preprocessing |\n",
    "| Val/Test Dataset Pull | **3min 30s** | Validation and test dataset verification |\n",
    "| Teacher Model Training | **6hr** | Teacher model training (ConvNext-Large backbone, 80 epochs) |\n",
    "| Teacher Model Evaluation | **4min 45s** | Performance assessment of teacher model |\n",
    "| Student Model Distillation | **3hr** | Knowledge distillation training (80 epochs) |\n",
    "| Student Model Evaluation | **3min 45s** | Performance assessment of distilled model |\n",
    "| ONNX + TensorRT Export | **3min + 7min** | Model optimization and engine generation |\n",
    "| TensorRT Inference | **7min** | High-performance inference testing |\n",
    "| **Total Time** | **9hr 45min** | **Complete end-to-end workflow** |\n",
    "\n",
    "#### Test Environment Specifications\n",
    "\n",
    "| **Component** | **Specification** |\n",
    "|---------------|-------------------|\n",
    "| **GPU** | 1x NVIDIA A40 |\n",
    "| **Training Dataset** | 4,000 images (1,010 MB total) |\n",
    "| **Validation Dataset** | 500 images (125 MB total) |\n",
    "| **Test Dataset** | 500 images (125 MB total) |\n",
    "| **Training Epochs** | 80 (both teacher and student) |\n",
    "| **Model Architecture** | RT-DETR with ConvNext-Large â†’ ConvNext-Tiny |\n",
    "| **Task** | Hard hat detection (2 classes) |\n",
    "\n",
    "#### Performance Factors\n",
    "\n",
    "> **Important Note:**  \n",
    "> Actual execution times may vary significantly based on:\n",
    "> \n",
    "> - **Hardware Configuration**: GPU type, memory, and compute capability\n",
    "> - **Storage Performance**: Local disk I/O speed and cloud storage latency\n",
    "> - **Network Conditions**: Bandwidth and latency to cloud storage\n",
    "> - **System Load**: Other concurrent processes and resource utilization\n",
    "> - **Dataset Size**: Number of images and total data volume\n",
    "> - **Batch Size**: Larger batch sizes can improve training stability and speed\n",
    "> - **Model Configuration**: Backbone architecture, epochs, and hyperparameters\n",
    "> - **Distillation Settings**: Temperature, alpha, and loss function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706129f",
   "metadata": {},
   "source": [
    "## Configuration \n",
    "\n",
    "Fill in all `<>` enclosed variables with relevant values under this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f18c62",
   "metadata": {},
   "source": [
    "### TAO FTMS Host & Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d990d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for your TAO FTMS server \n",
    "host_url = \"<HOST_URL>\"\n",
    "ngc_key = \"<NGC_API_KEY>\"\n",
    "ngc_org_name = \"<NGC_ORG_NAME>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb014e8",
   "metadata": {},
   "source": [
    "### Cloud Storage Setup & Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud bucket details to access datasets and store experiment results\n",
    "cloud_metadata = {}\n",
    "cloud_metadata[\"name\"] = \"tao_workspace\"  # A Representative name for this cloud info\n",
    "cloud_metadata[\"cloud_type\"] = \"aws\"  # If it's AWS, HuggingFace or Azure\n",
    "cloud_metadata[\"cloud_specific_details\"] = {}\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] = \"<BUCKET_REGION>\"  # Bucket region\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = \"<BUCKET_NAME>\"  # Bucket name\n",
    "# Access and Secret for AWS\n",
    "cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = \"<ACCESS_KEY>\"\n",
    "cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = \"<SECRET_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcd61b",
   "metadata": {},
   "source": [
    "### Dataset Paths in Cloud Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX ME - adjust paths to point to datasets in your cloud storage. If using S3 Bucket do not include the bucket name in the path. \n",
    "train_dataset_path =  \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/hard_hat_detection_coco/train\"\n",
    "eval_dataset_path = \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/hard_hat_detection_coco/val\"\n",
    "test_dataset_path = \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/hard_hat_detection_coco/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e6ec8",
   "metadata": {},
   "source": [
    "### Model Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No changes needed \n",
    "model_name = \"rtdetr\"\n",
    "ds_type = \"object_detection\"\n",
    "ds_format = \"coco\"\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fa2d1",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7df8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use NGC Key to login to FTMS \n",
    "data = json.dumps({\"ngc_org_name\": ngc_org_name,\n",
    "                   \"ngc_key\": ngc_key,\n",
    "                   \"enable_telemetry\": True})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"token\" in response.json().keys()\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/orgs/{ngc_org_name}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481c261",
   "metadata": {},
   "source": [
    "## Create cloud workspace\n",
    "This workspace will be the place where your datasets reside and results of TAO FTMS jobs will be pushed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloud workspace\n",
    "data = json.dumps(cloud_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/workspaces\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert \"id\" in response.json().keys()\n",
    "workspace_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e533f",
   "metadata": {},
   "source": [
    "## Register datasets with FTMS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce63deb",
   "metadata": {},
   "source": [
    "TAO FTMS requires datasets in your cloud storage to be registered to produce a unique ID that can be attached to training jobs. This step only needs to be done once and then you can use the dataset across any experiments that support the dataset format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95c65d",
   "metadata": {},
   "source": [
    "### List Registered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "datasets = response.json()[\"datasets\"]\n",
    "for rsp in datasets:\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"id\" in rsp_keys\n",
    "    assert \"type\" in rsp_keys\n",
    "    assert \"format\" in rsp_keys\n",
    "    assert \"name\" in rsp_keys\n",
    "\n",
    "print(response)\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in datasets:\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e88d47",
   "metadata": {},
   "source": [
    "If you have already registered your datasets, then you can directly set their IDs in the following cell to avoid creating duplicate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_id = None \n",
    "eval_dataset_id = None \n",
    "test_dataset_id = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322dd907",
   "metadata": {},
   "source": [
    "### Train Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac58297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "if train_dataset_id is None: \n",
    "    train_dataset_metadata = {\"type\": ds_type,\n",
    "                              \"format\": ds_format,\n",
    "                              \"workspace\":workspace_id,\n",
    "                              \"cloud_file_path\": train_dataset_path,\n",
    "                              \"use_for\": [\"training\"],\n",
    "                              \"name\": \"hardhat_detection_train\"\n",
    "                              }\n",
    "    data = json.dumps(train_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    train_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94015da",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba026cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create eval dataset\n",
    "if eval_dataset_id is None: \n",
    "    eval_dataset_metadata = {\"type\": ds_type,\n",
    "                             \"format\": ds_format,\n",
    "                             \"workspace\":workspace_id,\n",
    "                             \"cloud_file_path\": eval_dataset_path,\n",
    "                             \"use_for\": [\"evaluation\"],\n",
    "                             \"name\" : \"hardhat_detection_val\" \n",
    "                             }\n",
    "    data = json.dumps(eval_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    eval_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4207b",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing dataset for inference\n",
    "if test_dataset_id is None: \n",
    "    test_dataset_metadata = {\"type\": ds_type,\n",
    "                             \"format\":ds_format,\n",
    "                             \"workspace\":workspace_id,\n",
    "                             \"cloud_file_path\": test_dataset_path,\n",
    "                             \"use_for\": [\"testing\"],\n",
    "                             \"name\": \"hardhat_detection_test\"\n",
    "                             }\n",
    "    data = json.dumps(test_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    test_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3110e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91e7b6",
   "metadata": {},
   "source": [
    "## Create Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379c0b9",
   "metadata": {},
   "source": [
    "Before we can run any jobs such as training, evaluation, distillation or inference, we must create an experiment to setup the network architecture and associated datsets. Then we can chain several jobs together to create our trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4918e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_key = \"tlt_encode\"\n",
    "checkpoint_choose_method = \"best_model\"\n",
    "data = json.dumps({\"network_arch\":model_name,\n",
    "                   \"encryption_key\":encode_key,\n",
    "                   \"checkpoint_choose_method\":checkpoint_choose_method,\n",
    "                   \"workspace\": workspace_id,\n",
    "                   \"train_datasets\":[train_dataset_id],\n",
    "                   \"eval_dataset\":eval_dataset_id,\n",
    "                   \"inference_dataset\":test_dataset_id,\n",
    "                   \"calibration_dataset\":train_dataset_id})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"id\" in response.json()\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "experiment_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc3dde",
   "metadata": {},
   "source": [
    "When a job is submitted, we will receive a unique ID to reference back to it. We will store these IDs in the following ```job_map``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c570269",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ff676",
   "metadata": {},
   "source": [
    "## Train Teacher Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb0b2b",
   "metadata": {},
   "source": [
    "Distillation first requires a large, high accuracy teacher model. To get this teacher model, we will train RT-DETR with a ConvNext Large backbone on our hard hat detection dataset. The size of this model will be 252 million parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473d281",
   "metadata": {},
   "source": [
    "The table lists all the supported teacher and student backbones. For example, you can use `RT-DETR+convnextv2_huge` as the teacher and distill a `RT-DETR+resnet18` student.\n",
    "\n",
    "| Supported teacher backbones           | Supported student backbones |\n",
    "|---------------------------------------|-----------------------------|\n",
    "| convnext_tiny/small/base/large/xlarge | convnext_tiny/small/large   |\n",
    "| convnextv2_nano/tiny/base/large/huge  | efficientvit_b0/b1          |\n",
    "| resnet18/34/50/101                    | resnet34/50                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989cb03e",
   "metadata": {},
   "source": [
    "### Retrieve default training spec "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6902e9",
   "metadata": {},
   "source": [
    "Before launching a training a job, we can retrieve the default training spec for our network architecture to use as a starting point to configure the training parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "# full_schema = response.json()\n",
    "# print(json.dumps(full_schema, indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30387a",
   "metadata": {},
   "source": [
    "### Configure Training Spec \n",
    "\n",
    "Now we can customize the json spec object and set our dataset, training and model parameters. No changes are needed unless you are using a custom dataset or model architecture.\n",
    "\n",
    "#### Understanding the Spec Schema Structure\n",
    "\n",
    "Before customizing the training spec, it's helpful to understand the available configuration options and their valid values. The `get_bounds_of_field` utility function helps you explore the schema structure and find valid parameter values if they are defined on the backend (for some fields it might not be defined).\n",
    "\n",
    "**How to use the schema exploration utility:**\n",
    "\n",
    "```python\n",
    "from get_bounds import get_bounds_of_field\n",
    "\n",
    "# Example: Get valid values for backbone type\n",
    "# Note the conversion from dictionary access notation to list notation:\n",
    "# From: [\"model\"][\"backbone\"]\n",
    "# To:   [\"model\", \"backbone\"]\n",
    "print(get_bounds_of_field(full_schema, [\"model\", \"backbone\"]))\n",
    "```\n",
    "\n",
    "**Key differences in notation:**\n",
    "- **Dictionary access format**: `[\"model\"][\"backbone\"]` - This is how you would access nested values in Python\n",
    "- **Schema path format**: `[\"model\", \"backbone\"]` - This is the format expected by the `get_bounds_of_field` function\n",
    "\n",
    "---\n",
    "\n",
    "#### Schema Navigation Tips\n",
    "\n",
    "1. **Nested Structure**: The schema follows a hierarchical structure where each level represents a configuration category\n",
    "2. **Path Format**: Always use a list of strings `[\"level1\", \"level2\", \"level3\"]` when calling `get_bounds_of_field`\n",
    "3. **Validation**: This helps ensure you're using valid parameter values before submitting your training job\n",
    "4. **Documentation**: Use these explorations to understand what options are available for your specific model architecture\n",
    "\n",
    "\n",
    "No changes to the default spec are needed unless you are using a custom dataset format or want to modify the model architecture and training parameters.\n",
    "For more details about the configurable parameters, refer to the documentation on [RT-DETR](https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/object_detection/rt_detr.html#creating-an-experiment-spec-file) in TAO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0223ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize train model specs\n",
    "specs[\"train\"][\"num_epochs\"] = 80\n",
    "specs[\"train\"][\"checkpoint_interval\"] = 20\n",
    "specs[\"train\"][\"validation_interval\"] = 5\n",
    "specs[\"train\"][\"optim\"][\"lr_backbone\"] = 0.0005\n",
    "specs[\"train\"][\"optim\"][\"lr\"] = 0.001\n",
    "specs[\"train\"][\"optim\"][\"lr_steps\"] = [1000]\n",
    "specs[\"train\"][\"optim\"][\"momentum\"] = 0.9\n",
    "specs[\"train\"][\"precision\"] = \"bf16\"\n",
    "specs[\"train\"][\"activation_checkpoint\"] = True \n",
    "specs[\"train\"][\"ema\"] = False \n",
    "specs[\"train\"][\"num_gpus\"] = 1\n",
    "\n",
    "specs[\"dataset\"][\"batch_size\"] = 8 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False \n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1 #RT-DETR requires + 1 to number  of classes \n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416] #set to match image size in dataset \n",
    "specs[\"dataset\"][\"augmentation\"][\"train_spatial_size\"] = [416, 416] #set to match image size in dataset \n",
    "specs[\"dataset\"][\"augmentation\"][\"distortion_prob\"] = 0.3\n",
    "specs[\"dataset\"][\"augmentation\"][\"iou_crop_prob\"] = 0.3\n",
    "\n",
    "specs[\"model\"][\"backbone\"] = \"convnext_large\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d1047",
   "metadata": {},
   "source": [
    "### Submit Training Job  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d85289",
   "metadata": {},
   "source": [
    "With our training spec configured, we can now submit a training job. All jobs follow the same flow of retreiving the default spec, customizing it then submitting the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9405bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "action = \"train\"\n",
    "data = json.dumps({\"parent_job_id\":None,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"train_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d453e",
   "metadata": {},
   "source": [
    "After submitting the training job, an ID is returned that we can use to monitor the job progress. The following cell will continuously print the latest status until the job is complete. This notebook will track all of the job IDs in the ```job_map``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map[\"train_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099cfd6",
   "metadata": {},
   "source": [
    "If you need to cancel the job for any reason, you can uncomment and run the following cell. You can also configure the endpoint to end with ```:pause``` or ```:resume``` instead of ```:cancel``` to temporarily stop and start the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173db7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train_\" + model_name]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "\n",
    "# response = requests.post(endpoint, headers=headers)\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022174a5",
   "metadata": {},
   "source": [
    "If the job runs into any errors or if you want to check the job logs, you can uncomment and run the following cell to view the job logs. Alternatively, you can view the job logs in your cloud workspace under the path /results/<job_id>/microservices_log.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train_\" + model_name]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "\n",
    "# response = requests.get(endpoint, headers=headers)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae519ec",
   "metadata": {},
   "source": [
    "## Evaluate Teacher Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cb9fd",
   "metadata": {},
   "source": [
    "Once our teacher model has been trained, we can evaluate it on the test dataset to get detection KPIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5b869",
   "metadata": {},
   "source": [
    "### Receive default evaluation spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d65e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f240d7",
   "metadata": {},
   "source": [
    "### Customize Evaluation Spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f102ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"batch_size\"] = 8 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False\n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1\n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416]\n",
    "\n",
    "specs[\"model\"][\"backbone\"] = \"convnext_large\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02feb31",
   "metadata": {},
   "source": [
    "### Submit Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed43595",
   "metadata": {},
   "source": [
    "Note that for this job we will set the ```parent_job_id``` parameter in the body of the request to the completed training job. This is required to pass the trained model from the training job into our evaluation job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"evaluate\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"evaluate_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by running this cell\n",
    "job_id = job_map[\"evaluate_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07effc2c",
   "metadata": {},
   "source": [
    "## Distill Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492f6d0",
   "metadata": {},
   "source": [
    "With our teacher model trained, we can run a distill job and specify the smaller ConvNext Tiny backbone to be used as the student model. This will reduce the model from 252 million parameters to only 65.7 million while achieving similar accuracy. This will lead to a great speed up when inferencing the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c49c4",
   "metadata": {},
   "source": [
    "### Configure Distillation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b02531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize train model specs\n",
    "specs[\"train\"][\"num_epochs\"] = 80\n",
    "specs[\"train\"][\"checkpoint_interval\"] = 20\n",
    "specs[\"train\"][\"validation_interval\"] = 5\n",
    "specs[\"train\"][\"optim\"][\"lr_backbone\"] = 0.0005\n",
    "specs[\"train\"][\"optim\"][\"lr\"] = 0.001\n",
    "specs[\"train\"][\"optim\"][\"lr_steps\"] = [1000]\n",
    "specs[\"train\"][\"optim\"][\"momentum\"] = 0.9\n",
    "specs[\"train\"][\"precision\"] = \"bf16\"\n",
    "specs[\"train\"][\"activation_checkpoint\"] = True \n",
    "specs[\"train\"][\"ema\"] = False \n",
    "specs[\"train\"][\"num_gpus\"] = 1\n",
    "\n",
    "specs[\"dataset\"][\"batch_size\"] = 8 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False \n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1 \n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416]\n",
    "specs[\"dataset\"][\"augmentation\"][\"train_spatial_size\"] = [416, 416]\n",
    "specs[\"dataset\"][\"augmentation\"][\"distortion_prob\"] = 0.3\n",
    "specs[\"dataset\"][\"augmentation\"][\"iou_crop_prob\"] = 0.3\n",
    "\n",
    "#Configure student model parameters \n",
    "specs[\"model\"][\"backbone\"] = \"convnext_tiny\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "#Configure teacher model parameters from previous training job \n",
    "specs[\"distill\"] = {}\n",
    "specs[\"distill\"][\"teacher\"] = {}\n",
    "specs[\"distill\"][\"teacher\"][\"backbone\"] = \"convnext_large\"\n",
    "specs[\"distill\"][\"teacher\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"distill\"][\"teacher\"][\"dec_layers\"] = 6 \n",
    "specs[\"distill\"][\"teacher\"][\"enc_layers\"] = 1 \n",
    "specs[\"distill\"][\"teacher\"][\"num_queries\"] = 300 \n",
    "specs[\"distill\"][\"bindings\"] = [{\"teacher_module_name\": \"srcs\", \"student_module_name\": \"srcs\", \"criterion\": \"IOU\", \"weight\": 20}]\n",
    "\n",
    "\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c34a97",
   "metadata": {},
   "source": [
    "### Submit Distillation Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea5647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "action = \"distill\"\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "job_map[\"distill_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map[\"distill_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1c3f9",
   "metadata": {},
   "source": [
    "## Evaluate Distilled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab9d9e",
   "metadata": {},
   "source": [
    "### Configure Evaluation Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805886f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f69fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"batch_size\"] = 8 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False \n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1\n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416]\n",
    "\n",
    "specs[\"model\"][\"backbone\"] = \"convnext_tiny\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1474d0c",
   "metadata": {},
   "source": [
    "### Submit Evaluation Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"distill_\" + model_name] #parent is now distillation job. This evaluation will use the distilled model. \n",
    "action = \"evaluate\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"evaluate_distilled_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"evaluate_distilled_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e7c2c",
   "metadata": {},
   "source": [
    "## TRT Engine Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574906c",
   "metadata": {},
   "source": [
    "Now that we have a small, accurate distilled model, it can be exported to ONNX format then turned into an optimzed TensorRT engine for deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72250e1b",
   "metadata": {},
   "source": [
    "### Export Model to ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/export/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ada368",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"batch_size\"] = 8 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False\n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1 \n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416]\n",
    "specs[\"dataset\"][\"augmentation\"][\"train_spatial_size\"] = [416, 416]\n",
    "\n",
    "specs[\"model\"][\"backbone\"] = \"convnext_tiny\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "specs[\"export\"][\"input_height\"] = 416\n",
    "specs[\"export\"][\"input_width\"] = 416\n",
    "\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"distill_\" + model_name] #parent is now distillation job. This will export the distilled model.  \n",
    "action = \"export\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"export_distilled_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0814d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"export_distilled_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315bec6",
   "metadata": {},
   "source": [
    "### Convert ONNX to TRT Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f738413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/gen_trt_engine/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"batch_size\"] = 8 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False\n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1 \n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416]\n",
    "specs[\"dataset\"][\"augmentation\"][\"train_spatial_size\"] = [416, 416]\n",
    "\n",
    "specs[\"model\"][\"backbone\"] = \"convnext_tiny\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"FP16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"export_distilled_\" + model_name]\n",
    "action = \"gen_trt_engine\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"model_gen_trt_engine_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['model_gen_trt_engine_' + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88d5a1",
   "metadata": {},
   "source": [
    "### Inference TRT Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce293bee",
   "metadata": {},
   "source": [
    "Finally we can use our distilled and optimized student model to inference on our test set and receive the annotated results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa188b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"batch_size\"] = 1 #If OOM error, decrease batch size\n",
    "specs[\"dataset\"][\"workers\"] = 4\n",
    "specs[\"dataset\"][\"remap_mscoco_category\"] = False \n",
    "specs[\"dataset\"][\"num_classes\"] = num_classes + 1 \n",
    "specs[\"dataset\"][\"augmentation\"][\"eval_spatial_size\"] = [416, 416]\n",
    "specs[\"dataset\"][\"augmentation\"][\"train_spatial_size\"] = [416, 416]\n",
    "\n",
    "specs[\"model\"][\"backbone\"] = \"convnext_tiny\"\n",
    "specs[\"model\"][\"train_backbone\"] = True \n",
    "specs[\"model\"][\"return_interm_indices\"] = [1,2,3] \n",
    "specs[\"model\"][\"dec_layers\"] = 6\n",
    "specs[\"model\"][\"enc_layers\"] = 1 \n",
    "specs[\"model\"][\"num_queries\"] = 300\n",
    "\n",
    "specs[\"inference\"][\"input_height\"] = 416 \n",
    "specs[\"inference\"][\"input_width\"] = 416\n",
    "specs[\"inference\"][\"outline_width\"] = 5\n",
    "specs[\"inference\"][\"color_map\"] = {}\n",
    "specs[\"inference\"][\"color_map\"][\"helmet\"] = \"red\"\n",
    "specs[\"inference\"][\"color_map\"][\"head\"] = \"blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"model_gen_trt_engine_\" + model_name]\n",
    "action = \"inference\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"inference_trt_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"inference_trt_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally run this section to delete the datasets and experiment results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete experiment <a class=\"anchor\" id=\"head-23\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete train dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete val dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete test dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
