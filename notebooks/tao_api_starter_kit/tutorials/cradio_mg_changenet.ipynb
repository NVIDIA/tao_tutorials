{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to demonstrate TAO Multi-Golden ChangeNet Classification\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "### TAO Multi-Golden ChangeNet Classification with C-RADIO \n",
    "\n",
    "This notebook will walk through using Multi-Golden ChangeNet Classification with a CRADIO pretrained backbone for defect classification. \n",
    "\n",
    "Multi-Golden ChangeNet is a model architecture that allows multiple reference or 'golden' samples to be used as a reference when detecting if an object has a defect. This model is often used for manufacturing and quality assurance to automatically determine if parts are manufactured correctly. \n",
    "\n",
    "C-RADIO is a new foundation model that can be used as a backbone for a variety of tasks. In this example we will show how a pretrained CRADIO backbone can be used with Multi-Golden ChangeNet.\n",
    "\n",
    "For more information about the Foundation Models supported in TAO, and the down-stream tasks integrated with it, refer to the TAO [Documentation](https://docs.nvidia.com/tao/tao-toolkit/text//foundation_models/overview.html#foundation-models)\n",
    "\n",
    "<img src=\"assets/finetuning_workflow_diagram.png\" width=\"600\">\n",
    "\n",
    "| **Foundation Model Architecture** | **Task Head** |\n",
    "| :-- | :-- |\n",
    "| NvDINOv2 | Visual ChangeNet |\n",
    "| C-RADIOv2 | Visual ChangeNet |\n",
    "| RADIOv2.5 | Visual ChangeNet |\n",
    "\n",
    "---\n",
    "\n",
    "### Sample inference from the Visual ChangeNet-Classification model\n",
    "\n",
    "| **Pass** | **Fail** |\n",
    "| :--: | :--: |\n",
    "|<img align=\"center\" title=\"no defects\" src=\"https://github.com/vpraveen-nv/model_card_images/blob/main/cv/notebook/optical_inspection/pass.png?raw=true\" width=\"400\" > no defects | <img align=\"center\" title=\"missing component\" src=\"https://github.com/vpraveen-nv/model_card_images/blob/main/cv/notebook/optical_inspection/defect.png?raw=true\" width=\"400\" >  missing component |\n",
    "\n",
    "\n",
    "To show this model in action, we will use the MVTec-AD datset and format it into a classification task to identify if the given image is defective or not defective.\n",
    "\n",
    "---\n",
    "### The workflow in a nutshell\n",
    "This notebook will walk through using Multi-Golden ChangeNet Classification with a CRADIO pretrained backbone for defect classification. \n",
    "\n",
    "1) Configure Connection to TAO FTMS \n",
    "2) Login & Create Cloud Workspace\n",
    "3) Register Train, Validation and Test datasets\n",
    "4) Train ChangeNet model \n",
    "5) Evaluate ChangeNet model \n",
    "6) Export model and test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Requirements\n",
    "Prior to running this notebook you must have: \n",
    "1) A TAO FTMS server.  [(Setup Guide here)](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)\n",
    "2) The sample mvtec-ad dataset from the [mvtec_ad_mgcn_dataset_format.ipynb](https://github.com/NVIDIA/tao_tutorials/blob/main/notebooks/tao_api_starter_kit/dataset_prepare/mvtec_ad_mg_changenet/mvtec_ad_mgcn_dataset_format.ipynb) notebook uploaded to your cloud storage.\n",
    "3) Set the `<>` enclosed values with relevant in the Configuration section of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Debugging Finetuning Microservice and Jobs\n",
    "\n",
    "When working with the TAO API, you may encounter issues at different stages. Use the following guidance to debug effectively:\n",
    "\n",
    "#### 1. Dataset, Experiment, or Workspace CRUD Operation Errors\n",
    "\n",
    "If you encounter errors related to creating, reading, updating, or deleting datasets, experiments, or workspaces **and the error messages are not clear**, check the logs of the TAO API service pods:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
    "```\n",
    "\n",
    "#### 2. Errors During Job Launch\n",
    "\n",
    "For issues that occur **while launching a job**, check both the app and workflow pods:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
    "kubectl logs -f <pod name starting with tao-api-workflow-pod>\n",
    "```\n",
    "\n",
    "#### 3. Errors After Job Launch\n",
    "\n",
    "If errors occur **after a job has been launched**, inspect the job pod logs:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f tao-api-sts-<job_id>-0\n",
    "```\n",
    "\n",
    "> **Note:**  \n",
    "> Run these `kubectl` commands on the machine where your Kubernetes service is deployed.\n",
    "\n",
    "#### Additional Debugging Tips\n",
    "\n",
    "- **Job logs are automatically uploaded** to your cloud workspace at:  \n",
    "  `/results/<job_id>/microservices_log.txt`\n",
    "\n",
    "- **You can also view logs via the Jobs API endpoint:**  \n",
    "  ```\n",
    "  /api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs\n",
    "  ```\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Error Type                | Where to Check Logs                                      |\n",
    "|-------------------------- |---------------------------------------------------------|\n",
    "| CRUD operation errors     | `tao-api-app-pod`                                       |\n",
    "| Job launch errors         | `tao-api-app-pod`, `tao-api-workflow-pod`               |\n",
    "| Post-launch job errors    | `tao-api-sts-<job_id>-0`                                |\n",
    "| All job logs (cloud)      | `/results/<job_id>/microservices_log.txt`               |\n",
    "| All job logs (API)        | `/api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Performance Benchmarks\n",
    "\n",
    "#### Execution Time Breakdown\n",
    "\n",
    "The following table shows the approximate time required for each stage of the Multi-Golden ChangeNet classification workflow:\n",
    "\n",
    "| **Stage** | **Duration** | **Description** |\n",
    "|-----------|--------------|-----------------|\n",
    "| Train Dataset Pull | **15min** | Train dataset verification and preprocessing |\n",
    "| Val/Test Dataset Pull | **3min 30s** | Validation and test dataset verification |\n",
    "| Model Finetuning | **18hr** | ChangeNet training with C-RADIO backbone (80 epochs) |\n",
    "| Model Evaluation | **8min 45s** | Performance assessment and metrics calculation |\n",
    "| ONNX Export | **4min** | Model conversion to ONNX format |\n",
    "| TensorRT Export | **10min** | TensorRT engine optimization and generation |\n",
    "| TensorRT Inference | **7min** | High-performance inference testing |\n",
    "| **Total Time** | **18hr 50min** | **Complete end-to-end workflow** |\n",
    "\n",
    "#### Test Environment Specifications\n",
    "\n",
    "| **Component** | **Specification** |\n",
    "|---------------|-------------------|\n",
    "| **GPU** | 1x NVIDIA A40 |\n",
    "| **Training Dataset** | 4,000 images (1,010 MB total) |\n",
    "| **Validation Dataset** | 500 images (125 MB total) |\n",
    "| **Test Dataset** | 500 images (125 MB total) |\n",
    "| **Training Epochs** | 80 |\n",
    "| **Model Architecture** | Multi-Golden ChangeNet with C-RADIO-v2b backbone |\n",
    "| **Task** | Manufacturing defect detection (2 classes: defective/non-defective) |\n",
    "| **Dataset** | MVTec-AD (Anomaly Detection) |\n",
    "\n",
    "\n",
    "#### Performance Factors\n",
    "\n",
    "> **Important Note:**  \n",
    "> Actual execution times may vary significantly based on:\n",
    "> \n",
    "> - **Hardware Configuration**: GPU type, memory, and compute capability\n",
    "> - **Storage Performance**: Local disk I/O speed and cloud storage latency\n",
    "> - **Network Conditions**: Bandwidth and latency to cloud storage\n",
    "> - **System Load**: Other concurrent processes and resource utilization\n",
    "> - **Dataset Size**: Number of images and total data volume\n",
    "> - **Batch Size**: Larger batch sizes can improve training stability and speed\n",
    "> - **Model Configuration**: Backbone architecture, epochs, and hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration \n",
    "\n",
    "Fill in all `<>` enclosed variables with relevant values under this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAO FTMS Host & Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for your TAO FTMS server \n",
    "host_url = \"<HOST_URL>\"\n",
    "ngc_key = \"<NGC_API_KEY>\"\n",
    "ngc_org_name = \"<NGC_ORG_NAME>\"\n",
    "\n",
    "docker_env_vars = {}\n",
    "\n",
    "# If you're using a PTM from a private organization like NVAIE, uncomment the following line and add your legacy NGC API Key.\n",
    "# docker_env_vars['TAO_API_KEY'] = '<NGC_LEGACY_API_KEY>' #Set to NGC Legacy API Key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Storage Setup & Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud bucket details to access datasets and store experiment results\n",
    "cloud_metadata = {}\n",
    "cloud_metadata[\"name\"] = \"tao_workspace\"  # A Representative name for this cloud info\n",
    "cloud_metadata[\"cloud_type\"] = \"aws\"  # If it's AWS, HuggingFace or Azure\n",
    "cloud_metadata[\"cloud_specific_details\"] = {}\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] =\"<BUCKET_REGION>\"  # Bucket region\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = \"<BUCKET_NAME>\"  # Bucket name\n",
    "# Access and Secret for AWS\n",
    "cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = \"<ACCESS_KEY>\"\n",
    "cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = \"<SECRET_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Paths in Cloud Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX ME - adjust paths to point to datasets in your cloud storage. If using S3 Bucket do not include the bucket name in the path. \n",
    "train_dataset_path =  \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/mvtec_ad_mgcn/train\"\n",
    "eval_dataset_path = \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/mvtec_ad_mgcn/val\"\n",
    "test_dataset_path = \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/mvtec_ad_mgcn/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"visual_changenet_classify\"\n",
    "ds_type = \"visual_changenet_classify\"\n",
    "ds_format = \"default\"\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate NGC_PERSONAL_KEY\n",
    "data = json.dumps({\"ngc_org_name\": ngc_org_name,\n",
    "                   \"ngc_key\": ngc_key,\n",
    "                   \"enable_telemetry\": True})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "assert response.ok, response.text\n",
    "assert \"token\" in response.json().keys()\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/orgs/{ngc_org_name}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cloud workspace\n",
    "This workspace will be the place where your datasets reside and results of TAO FTMS jobs will be pushed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloud workspace\n",
    "data = json.dumps(cloud_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/workspaces\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert \"id\" in response.json().keys()\n",
    "workspace_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register datasets with FTMS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAO FTMS requires datasets in your cloud storage to be registered to produce a unique ID that can be attached to training jobs. This step only needs to be done once and then you can use the dataset across any experiments that support the dataset format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Registered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "datasets = response.json()[\"datasets\"]\n",
    "for rsp in datasets:\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"id\" in rsp_keys\n",
    "    assert \"type\" in rsp_keys\n",
    "    assert \"format\" in rsp_keys\n",
    "    assert \"name\" in rsp_keys\n",
    "\n",
    "print(response)\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in datasets:\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already registered the datasets with TAO FTMS, then fill in the following cell with the dataset IDs to avoid creating duplicate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_id = None \n",
    "eval_dataset_id = None \n",
    "test_dataset_id = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "if train_dataset_id is None: \n",
    "    train_dataset_metadata = {\"type\": ds_type,\n",
    "                              \"format\": ds_format,\n",
    "                              \"workspace\":workspace_id,\n",
    "                              \"cloud_file_path\": train_dataset_path,\n",
    "                              \"use_for\": [\"training\"],\n",
    "                              \"name\": \"mvtec_mgcn_train\"\n",
    "                              }\n",
    "    data = json.dumps(train_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    assert response.ok, response.text\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    train_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.ok, response.text\n",
    "\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create eval dataset\n",
    "if eval_dataset_id is None: \n",
    "    eval_dataset_metadata = {\"type\": ds_type,\n",
    "                             \"format\": ds_format,\n",
    "                             \"workspace\":workspace_id,\n",
    "                             \"cloud_file_path\": eval_dataset_path,\n",
    "                             \"use_for\": [\"evaluation\"],\n",
    "                             \"name\" : \"mvtec_mgcn_val\"\n",
    "                             }\n",
    "    data = json.dumps(eval_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    assert response.ok, response.text\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    eval_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.ok, response.text\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing dataset for inference\n",
    "if test_dataset_id is None: \n",
    "    test_dataset_metadata = {\"type\": ds_type,\n",
    "                             \"format\":ds_format,\n",
    "                             \"workspace\":workspace_id,\n",
    "                             \"cloud_file_path\": test_dataset_path,\n",
    "                             \"use_for\": [\"testing\"],\n",
    "                             \"name\": \"mvtec_mgcn_test\"\n",
    "                             }\n",
    "    data = json.dumps(test_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    assert response.ok, response.text\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    test_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.ok, response.text\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run any jobs such as training, evaluation or inference, we must create an experiment to setup the network architecture and associated datsets. Then we can chain several jobs together to create our trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_key = \"tlt_encode\"\n",
    "checkpoint_choose_method = \"best_model\"\n",
    "data = json.dumps({\"network_arch\":model_name,\n",
    "                   \"encryption_key\":encode_key,\n",
    "                   \"checkpoint_choose_method\":checkpoint_choose_method,\n",
    "                   \"workspace\": workspace_id,\n",
    "                   \"train_datasets\":[train_dataset_id],\n",
    "                   \"eval_dataset\":eval_dataset_id,\n",
    "                   \"inference_dataset\":test_dataset_id,\n",
    "                   \"calibration_dataset\":train_dataset_id,\n",
    "                   \"docker_env_vars\": docker_env_vars})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.ok, response.text\n",
    "assert \"id\" in response.json()\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "experiment_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a job is submitted, we will receive a unique ID to reference back to it. We will store these IDs in the following ```job_map``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Pretrained Model\n",
    "\n",
    "To help bootstrap the model, we can start the model with pre-trained weights that have already seen a large number of images. This will help reduce the data and time required to finetune your model. Several pretrained models are available from NGC. TAO FTMS will automatically pull PTMs available to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "endpoint = f\"{base_url}/experiments:base\"\n",
    "params = {\"network_arch\": model_name}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "response_json = response.json()[\"experiments\"]\n",
    "\n",
    "for rsp in response_json:\n",
    "    rsp_keys = rsp.keys()\n",
    "    if \"encryption_key\" not in rsp.keys():\n",
    "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys\n",
    "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_map = {\"visual_changenet_classify\" : \"crdaiov2_vit:c_radio_v2_base\"}   \n",
    "endpoint = f\"{base_url}/experiments:base\"\n",
    "params = {\"network_arch\": model_name}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "response_json = response.json()[\"experiments\"]\n",
    "\n",
    "# Search for ptm with given ngc path\n",
    "ptm = []\n",
    "for rsp in response_json:\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"ngc_path\" in rsp_keys\n",
    "    if rsp[\"ngc_path\"].endswith(pretrained_map[model_name]):\n",
    "        assert \"id\" in rsp_keys\n",
    "        ptm_id = rsp[\"id\"]\n",
    "        ptm = [ptm_id]\n",
    "        print(\"Metadata for model with requested NGC Path\")\n",
    "        print(rsp)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm_information = {\"base_experiment\":ptm}\n",
    "data = json.dumps(ptm_information)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve default training spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching a training a job, we can retrieve the default training spec for our network architecture to use as a starting point to configure the training parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.ok, response.text\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "# full_schema = response.json()\n",
    "# print(json.dumps(full_schema, indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Spec\n",
    "\n",
    "Now we can customize the JSON spec object and set our dataset, training and model parameters. The training specification defines all the hyperparameters, model architecture settings, and training configurations that will be used during the training process.\n",
    "\n",
    "#### Understanding the Spec Schema Structure\n",
    "\n",
    "Before customizing the training spec, it's helpful to understand the available configuration options and their valid values. The `get_bounds_of_field` utility function helps you explore the schema structure and find valid parameter values if they are defined on the backend (for some fields it might not be defined).\n",
    "\n",
    "**How to use the schema exploration utility:**\n",
    "\n",
    "```python\n",
    "from get_bounds import get_bounds_of_field\n",
    "\n",
    "# Example: Get valid values for backbone type\n",
    "# Note the conversion from dictionary access notation to list notation:\n",
    "# From: [\"model\"][\"backbone\"][\"type\"] \n",
    "# To:   [\"model\", \"backbone\", \"type\"]\n",
    "print(get_bounds_of_field(full_schema, [\"model\", \"backbone\", \"type\"]))\n",
    "```\n",
    "\n",
    "**Key differences in notation:**\n",
    "- **Dictionary access format**: `[\"model\"][\"backbone\"][\"type\"]` - This is how you would access nested values in Python\n",
    "- **Schema path format**: `[\"model\", \"backbone\", \"type\"]` - This is the format expected by the `get_bounds_of_field` function\n",
    "\n",
    "---\n",
    "\n",
    "#### Schema Navigation Tips\n",
    "\n",
    "1. **Nested Structure**: The schema follows a hierarchical structure where each level represents a configuration category\n",
    "2. **Path Format**: Always use a list of strings `[\"level1\", \"level2\", \"level3\"]` when calling `get_bounds_of_field`\n",
    "3. **Validation**: This helps ensure you're using valid parameter values before submitting your training job\n",
    "4. **Documentation**: Use these explorations to understand what options are available for your specific model architecture\n",
    "\n",
    "\n",
    "No changes to the default spec are needed unless you are using a custom dataset format or want to modify the model architecture and training parameters.\n",
    "For more details about the configurable parameters, refer to the documentation on [VisualChangeNet](https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/visual_changenet/visual_changenet_classify.html#creating-a-training-experiment-spec-file) in TAO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize train model specs\n",
    "specs[\"train\"][\"num_epochs\"] = 80\n",
    "specs[\"train\"][\"checkpoint_interval\"] = 20\n",
    "specs[\"train\"][\"validation_interval\"] = 5\n",
    "specs[\"dataset\"][\"classify\"][\"batch_size\"]= 8\n",
    "specs[\"model\"][\"classify\"][\"eval_margin\"]= 0.3\n",
    "specs[\"model\"][\"classify\"][\"embed_dec\"] = 30\n",
    "specs[\"model\"][\"classify\"][\"difference_module\"] = \"learnable\"\n",
    "specs[\"model\"][\"classify\"][\"learnable_difference_modules\"] = 4\n",
    "specs[\"model\"][\"decode_head\"][\"use_summary_token\"] = True\n",
    "specs[\"model\"][\"decode_head\"][\"use_summary_token\"] = True\n",
    "specs[\"model\"][\"decode_head\"][\"feature_strides\"] = [4,8,16,32]\n",
    "specs[\"model\"][\"backbone\"][\"type\"] = \"c_radio_v2_vit_base_patch16_224\"\n",
    "specs[\"train\"][\"classify\"][\"loss\"] = \"ce\"\n",
    "specs[\"train\"][\"optim\"][\"lr\"] = 0.001\n",
    "specs[\"dataset\"][\"classify\"][\"input_map\"] = {}\n",
    "specs[\"dataset\"][\"classify\"][\"image_ext\"] = \".png\"\n",
    "specs[\"dataset\"][\"classify\"][\"num_input\"] = 1\n",
    "specs[\"dataset\"][\"classify\"][\"num_golden\"] = 4\n",
    "specs[\"dataset\"][\"classify\"][\"augmentation_config\"][\"augment\"] = True\n",
    "specs[\"dataset\"][\"classify\"][\"augmentation_config\"][\"random_rotate\"][\"enable\"] = False \n",
    "specs[\"task\"] = \"classify\"\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Training Job  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training spec configured, we can now submit a training job. All jobs follow the same flow of retreiving the default spec, customizing it then submitting the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "action = \"train\"\n",
    "data = json.dumps({\"parent_job_id\":None,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.ok, response.text\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"train_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting the training job, an ID is returned that we can use to monitor the job progress. The following cell will continuously print the latest status until the job is complete. This notebook will track all of the job IDs in the ```job_map``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map[\"train_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to cancel the job for any reason, you can uncomment and run the following cell. You can also configure the endpoint to end with ```:pause``` or ```:resume``` instead of ```:cancel``` to temporarily stop and start the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train_\" + model_name]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "\n",
    "# response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the job runs into any errors or if you want to check the job logs, you can uncomment and run the following cell to view the job logs. Alternatively, you can view the job logs in your cloud workspace under the path /results/<job_id>/microservices_log.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train_\" + model_name]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "\n",
    "# response = requests.get(endpoint, headers=headers)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Once our model has been been trained, we can evaulate it on the test datset to get accuracy KPIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive default evaluation spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.ok, response.text\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Evaluation Spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"classify\"][\"batch_size\"]= 8\n",
    "specs[\"model\"][\"classify\"][\"eval_margin\"]= 0.3\n",
    "specs[\"model\"][\"classify\"][\"embed_dec\"] = 30\n",
    "specs[\"model\"][\"classify\"][\"difference_module\"] = \"learnable\"\n",
    "specs[\"model\"][\"classify\"][\"learnable_difference_modules\"] = 4\n",
    "specs[\"model\"][\"decode_head\"][\"use_summary_token\"] = True\n",
    "specs[\"model\"][\"decode_head\"][\"feature_strides\"] = [4,8,16,32]\n",
    "specs[\"model\"][\"backbone\"][\"type\"] = \"c_radio_v2_vit_base_patch16_224\"\n",
    "specs[\"train\"][\"classify\"][\"loss\"] = \"ce\"\n",
    "\n",
    "specs[\"dataset\"][\"classify\"][\"input_map\"] = {}\n",
    "specs[\"dataset\"][\"classify\"][\"image_ext\"] = \".png\"\n",
    "specs[\"dataset\"][\"classify\"][\"num_input\"] = 1\n",
    "specs[\"dataset\"][\"classify\"][\"num_golden\"] = 4\n",
    "specs[\"dataset\"][\"classify\"][\"augmentation_config\"][\"augment\"] = False \n",
    "specs[\"task\"] = \"classify\"\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this job we will set the ```parent_job_id``` parameter in the body of the request to the completed training job. This is required to pass the trained model from the training job into our evaluation job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"evaluate\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"evaluate_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by running this cell\n",
    "job_id = job_map[\"evaluate_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.ok, response.text\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRT Engine Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a finetuned model, it can be exported to ONNX format then turned into an optimzed TensorRT engine for deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model to ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/export/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.ok, response.text\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"classify\"][\"batch_size\"]= 8\n",
    "specs[\"model\"][\"classify\"][\"eval_margin\"]= 0.3\n",
    "specs[\"model\"][\"classify\"][\"embed_dec\"] = 30\n",
    "specs[\"model\"][\"classify\"][\"difference_module\"] = \"learnable\"\n",
    "specs[\"model\"][\"classify\"][\"learnable_difference_modules\"] = 4\n",
    "specs[\"model\"][\"decode_head\"][\"use_summary_token\"] = True\n",
    "specs[\"model\"][\"decode_head\"][\"feature_strides\"] = [4,8,16,32]\n",
    "specs[\"model\"][\"backbone\"][\"type\"] = \"c_radio_v2_vit_base_patch16_224\"\n",
    "specs[\"train\"][\"classify\"][\"loss\"] = \"ce\"\n",
    "\n",
    "specs[\"dataset\"][\"classify\"][\"input_map\"] = {}\n",
    "specs[\"dataset\"][\"classify\"][\"image_ext\"] = \".png\"\n",
    "specs[\"dataset\"][\"classify\"][\"num_input\"] = 1\n",
    "specs[\"dataset\"][\"classify\"][\"num_golden\"] = 4\n",
    "specs[\"dataset\"][\"classify\"][\"augmentation_config\"][\"augment\"] = False \n",
    "specs[\"task\"] = \"classify\"\n",
    "\n",
    "specs[\"export\"][\"input_channel\"] = 3 \n",
    "specs[\"export\"][\"input_width\"] = 224\n",
    "specs[\"export\"][\"input_height\"] = 224 \n",
    "specs[\"export\"][\"batch_size\"] = -1 \n",
    "\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name] #parent is now training job. This will export the trained model.  \n",
    "action = \"export\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"export_trained_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"export_trained_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.ok, response.text\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ONNX to TRT Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/gen_trt_engine/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.ok, response.text\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"classify\"][\"batch_size\"]= 8\n",
    "specs[\"model\"][\"classify\"][\"eval_margin\"]= 0.3\n",
    "specs[\"model\"][\"classify\"][\"embed_dec\"] = 30\n",
    "specs[\"model\"][\"classify\"][\"difference_module\"] = \"learnable\"\n",
    "specs[\"model\"][\"classify\"][\"learnable_difference_modules\"] = 4\n",
    "specs[\"model\"][\"decode_head\"][\"use_summary_token\"] = True\n",
    "specs[\"model\"][\"decode_head\"][\"feature_strides\"] = [4,8,16,32]\n",
    "specs[\"model\"][\"backbone\"][\"type\"] = \"c_radio_v2_vit_base_patch16_224\"\n",
    "specs[\"train\"][\"classify\"][\"loss\"] = \"ce\"\n",
    "\n",
    "specs[\"dataset\"][\"classify\"][\"input_map\"] = {}\n",
    "specs[\"dataset\"][\"classify\"][\"image_ext\"] = \".png\"\n",
    "specs[\"dataset\"][\"classify\"][\"num_input\"] = 1\n",
    "specs[\"dataset\"][\"classify\"][\"num_golden\"] = 4\n",
    "specs[\"dataset\"][\"classify\"][\"augmentation_config\"][\"augment\"] = False \n",
    "specs[\"task\"] = \"classify\"\n",
    "\n",
    "specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"FP16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"export_trained_\" + model_name]\n",
    "action = \"gen_trt_engine\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"model_gen_trt_engine_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['model_gen_trt_engine_' + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.ok, response.text\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference TRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use our optimized model to inference on our test set and receive the annotated results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.ok, response.text\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs[\"dataset\"][\"classify\"][\"batch_size\"]= 1\n",
    "specs[\"model\"][\"classify\"][\"eval_margin\"]= 0.3\n",
    "specs[\"model\"][\"classify\"][\"embed_dec\"] = 30\n",
    "specs[\"model\"][\"classify\"][\"difference_module\"] = \"learnable\"\n",
    "specs[\"model\"][\"classify\"][\"learnable_difference_modules\"] = 4\n",
    "specs[\"model\"][\"decode_head\"][\"use_summary_token\"] = True\n",
    "specs[\"model\"][\"decode_head\"][\"feature_strides\"] = [4,8,16,32]\n",
    "specs[\"model\"][\"backbone\"][\"type\"] = \"c_radio_v2_vit_base_patch16_224\"\n",
    "specs[\"train\"][\"classify\"][\"loss\"] = \"ce\"\n",
    "\n",
    "specs[\"dataset\"][\"classify\"][\"input_map\"] = {}\n",
    "specs[\"dataset\"][\"classify\"][\"image_ext\"] = \".png\"\n",
    "specs[\"dataset\"][\"classify\"][\"num_input\"] = 1\n",
    "specs[\"dataset\"][\"classify\"][\"num_golden\"] = 4\n",
    "specs[\"dataset\"][\"classify\"][\"augmentation_config\"][\"augment\"] = False \n",
    "specs[\"task\"] = \"classify\"\n",
    "\n",
    "specs[\"inference\"][\"batch_size\"] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"model_gen_trt_engine_\" + model_name]\n",
    "action = \"inference\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"inference_trt_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"inference_trt_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.ok, response.text\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally run this section to delete the datasets and experiment results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete experiment <a class=\"anchor\" id=\"head-23\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "print(response)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete train dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete val dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete test dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.ok, response.text\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
