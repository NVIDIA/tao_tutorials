{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3478e43b",
   "metadata": {},
   "source": [
    "### Notebook to demonstrate Synthetic data generation workflow\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "---\n",
    "\n",
    "### TAO data generation with StyleganXL\n",
    "\n",
    "StyleGAN-XL is a synthetic dataset generation model for us to generate more images for downstream tasks such as classification and segmentation. The techniques could be powerful especially when the existing real dataset is not sufficient for training downstream models.\n",
    "\n",
    "**Warning:**\n",
    "The StyleganXL pre-trained model used for finetuning in this notebook is for non-commercial usage.\n",
    "\n",
    "<img src=\"assets/stylegan_sdg.png\" width=\"600\">\n",
    "\n",
    "---\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "This notebook will show how to train a StyleGAN-XL model - a conditional GAN model followed by evaluation of the model and inference that can generate images from one of the six classes in the `NEU_Metal_Surface_Defects_Data/train` dataset.\n",
    "\n",
    "1) Configure Connection to TAO FTMS \n",
    "2) Login & Create Cloud Workspace\n",
    "3) Register Train datasets\n",
    "4) Choose Pretrained model\n",
    "5) Train and evaluate Data-generation model\n",
    "6) Run inference to generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae949c67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Requirements\n",
    "Prior to running this notebook you must have: \n",
    "1) A TAO FTMS server.  [(Setup Guide here)](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)\n",
    "2) The sample stylegan_xl dataset from the [data_generation.ipynb](https://github.com/NVIDIA/tao_tutorials/tree/main/notebooks/tao_api_starter_kit/dataset_prepare/data_generation.ipynb) notebook uploaded to your cloud storage.\n",
    "3) Set the `<>` enclosed variables with values in the Configuration section of the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Debugging Finetuning Microservice and Jobs\n",
    "\n",
    "When working with the TAO API, you may encounter issues at different stages. Use the following guidance to debug effectively:\n",
    "\n",
    "#### 1. Dataset, Experiment, or Workspace CRUD Operation Errors\n",
    "\n",
    "If you encounter errors related to creating, reading, updating, or deleting datasets, experiments, or workspaces **and the error messages are not clear**, check the logs of the TAO API service pods:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
    "```\n",
    "\n",
    "#### 2. Errors During Job Launch\n",
    "\n",
    "For issues that occur **while launching a job**, check both the app and workflow pods:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
    "kubectl logs -f <pod name starting with tao-api-workflow-pod>\n",
    "```\n",
    "\n",
    "#### 3. Errors After Job Launch\n",
    "\n",
    "If errors occur **after a job has been launched**, inspect the job pod logs:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f tao-api-sts-<job_id>-0\n",
    "```\n",
    "\n",
    "> **Note:**  \n",
    "> Run these `kubectl` commands on the machine where your Kubernetes service is deployed.\n",
    "\n",
    "#### Additional Debugging Tips\n",
    "\n",
    "- **Job logs are automatically uploaded** to your cloud workspace at:  \n",
    "  `/results/<job_id>/microservices_log.txt`\n",
    "\n",
    "- **You can also view logs via the Jobs API endpoint:**  \n",
    "  ```\n",
    "  /api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs\n",
    "  ```\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Error Type                | Where to Check Logs                                      |\n",
    "|-------------------------- |---------------------------------------------------------|\n",
    "| CRUD operation errors     | `tao-api-app-pod`                                       |\n",
    "| Job launch errors         | `tao-api-app-pod`, `tao-api-workflow-pod`               |\n",
    "| Post-launch job errors    | `tao-api-sts-<job_id>-0`                                |\n",
    "| All job logs (cloud)      | `/results/<job_id>/microservices_log.txt`               |\n",
    "| All job logs (API)        | `/api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5332ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Performance Benchmarks\n",
    "\n",
    "#### Execution Time Breakdown\n",
    "\n",
    "The following table shows the approximate time required for each stage of the StyleGAN-XL workflow:\n",
    "\n",
    "| **Stage** | **Duration** | **Description** |\n",
    "|-----------|--------------|-----------------|\n",
    "| Dataset Convert | **2m 45s** | Image preprocessing and resolution conversion |\n",
    "| Training | **1hr 30min** | Model training with 10 epochs |\n",
    "| Evaluation | **18min** | FID score calculation and model assessment |\n",
    "| Inference | **21min** | Synthetic image generation |\n",
    "| **Total Time** | **2hr 12min** | **Complete end-to-end workflow** |\n",
    "\n",
    "\n",
    "#### Test Environment Specifications\n",
    "\n",
    "| **Component** | **Specification** |\n",
    "|---------------|-------------------|\n",
    "| **GPU** | 1x NVIDIA A40 |\n",
    "| **Training Dataset** | 1,656 images (66 MB total) |\n",
    "| **Validation Dataset** | 72 images (2.9 MB total) |\n",
    "| **Test Dataset** | 72 images (2.9 MB total) |\n",
    "| **Image Resolution** | 64x64 pixels |\n",
    "| **Training Epochs** | 10 |\n",
    "\n",
    "#### Performance Factors\n",
    "\n",
    "> **Important Note:**  \n",
    "> Actual execution times may vary significantly based on:\n",
    "> \n",
    "> - **Hardware Configuration**: GPU type, memory, and compute capability\n",
    "> - **Storage Performance**: Local disk I/O speed and cloud storage latency\n",
    "> - **Network Conditions**: Bandwidth and latency to cloud storage\n",
    "> - **System Load**: Other concurrent processes and resource utilization\n",
    "> - **Dataset Size**: Number of images and total data volume\n",
    "> - **Batch Size**: Larger batch sizes can improve training stability and speed\n",
    "> - **Model Configuration**: Resolution, epochs, and other hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to use FTMS to `train`, `evaluate`, and `inference` with StyleGAN-XL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration \n",
    "\n",
    "Fill in all `<>` enclosed variables with relevant values under this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAO FTMS Host & Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for your TAO FTMS server \n",
    "host_url = \"<HOST_URL>\"\n",
    "ngc_key = \"<NGC_API_KEY>\"\n",
    "ngc_org_name = \"<NGC_ORG_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Storage Setup & Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud bucket details to access datasets and store experiment results\n",
    "cloud_metadata = {}\n",
    "cloud_metadata[\"name\"] = \"tao_workspace\"  # A Representative name for this cloud info\n",
    "cloud_metadata[\"cloud_type\"] = \"aws\"  # If it's AWS, HuggingFace or Azure\n",
    "cloud_metadata[\"cloud_specific_details\"] = {}\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] = \"<BUCKET_REGION>\"  # Bucket region\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = \"<BUCKET_NAME>\"  # Bucket name\n",
    "# Access and Secret for AWS\n",
    "cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = \"<ACCESS_KEY>\"\n",
    "cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = \"<SECRET_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Paths in Cloud Storage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see the dataset folder structure required for the models supported in this notebook, visit the notebooks under dataset_prepare like for [this notebook](../dataset_prepare/segmentation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX ME - adjust paths to point to datasets in your cloud storage. If using S3 Bucket do not include the bucket name in the path. \n",
    "train_dataset_path =  \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/stylegan_train\"\n",
    "eval_dataset_path = \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/stylegan_val\"\n",
    "test_dataset_path = \"/<PATH_TO_DATASET_IN_CLOUD_STORAGE>/stylegan_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes needed \n",
    "model_name = \"stylegan_xl\"\n",
    "ds_type = \"stylegan_xl\"\n",
    "ds_format = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toggle AutoML params\n",
    "[AutoML documentation](https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_enabled = False\n",
    "automl_algorithm = \"<bayesian/hyperband>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use NGC Key to login to FTMS \n",
    "data = json.dumps({\"ngc_org_name\": ngc_org_name,\n",
    "                   \"ngc_key\": ngc_key,\n",
    "                   \"enable_telemetry\": True})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"token\" in response.json().keys()\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/orgs/{ngc_org_name}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cloud workspace\n",
    "This workspace will be the place where your datasets reside and results of TAO FTMS jobs will be pushed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloud workspace\n",
    "data = json.dumps(cloud_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/workspaces\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "assert \"id\" in response.json().keys()\n",
    "workspace_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register datasets with FTMS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAO FTMS requires datasets in your cloud storage to be registered to produce a unique ID that can be attached to training jobs. This step only needs to be done once and then you can use the dataset across any experiments that support the dataset format.\n",
    "\n",
    "StyleganXL requires three dataset - one each for training, evaluation and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Registered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "datasets = response.json()[\"datasets\"]\n",
    "for rsp in datasets:\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"id\" in rsp_keys\n",
    "    assert \"type\" in rsp_keys\n",
    "    assert \"format\" in rsp_keys\n",
    "    assert \"name\" in rsp_keys\n",
    "\n",
    "print(response)\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in datasets:\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already registered your datasets, then you can directly set their IDs in the following cell to avoid creating duplicate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_id = None \n",
    "eval_dataset_id = None \n",
    "test_dataset_id = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a dataset object by linking the cloud workspace, path in the cloud bucket and it's dataset type, format and \n",
    "\n",
    "Optional fields include the intention for the dataset object - training/evaluation/testing and a name to differentiate multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "if train_dataset_id is None: \n",
    "    train_dataset_metadata = {\"type\": ds_type,\n",
    "                              \"format\": ds_format,\n",
    "                              \"workspace\":workspace_id,\n",
    "                              \"cloud_file_path\": train_dataset_path,\n",
    "                              \"use_for\": [\"training\"],\n",
    "                              \"name\": \"stylegan_metal_surface_defects_train\"\n",
    "                              }\n",
    "    data = json.dumps(train_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    train_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will check if the dataset files in the mentioned folder matches with the required files for this model\n",
    "\n",
    "If the required files are present, the dataset_metadata's status will be updated to `pull_complete` else `invalid_pull`\n",
    "\n",
    "We poll the status of verification every 5 seconds, for larger datasets, this might take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataset - same process of creating a dataset object and wait for verification of files to be complete like Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create eval dataset\n",
    "if eval_dataset_id is None: \n",
    "    eval_dataset_metadata = {\"type\": ds_type,\n",
    "                             \"format\": ds_format,\n",
    "                             \"workspace\":workspace_id,\n",
    "                             \"cloud_file_path\": eval_dataset_path,\n",
    "                             \"use_for\": [\"evaluation\"],\n",
    "                             \"name\" : \"stylegan_metal_surface_defects_eval\" \n",
    "                             }\n",
    "    data = json.dumps(eval_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    eval_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset - same process of creating a dataset object and wait for verification of files to be complete like Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing dataset for inference\n",
    "if test_dataset_id is None: \n",
    "    test_dataset_metadata = {\"type\": ds_type,\n",
    "                             \"format\":ds_format,\n",
    "                             \"workspace\":workspace_id,\n",
    "                             \"cloud_file_path\": test_dataset_path,\n",
    "                             \"use_for\": [\"testing\"],\n",
    "                             \"name\": \"stylegan_metal_surface_defects_test\"\n",
    "                             }\n",
    "    data = json.dumps(test_dataset_metadata)\n",
    "    \n",
    "    endpoint = f\"{base_url}/datasets\"\n",
    "    \n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"id\" in response.json().keys()\n",
    "    \n",
    "    test_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset convert Action for all three datasets <a class=\"anchor\" id=\"head-8\"></a>\n",
    "\n",
    "This action is for **Image Preprocessing**:\n",
    "\n",
    "StyleGAN-XL requires square, fixed-resolution images for training. The tool can crop and/or resize images to meet the resolution requirements of StyleGAN-XL's progressive training workflow. This training process involves starting with lower resolutions (e.g., `16x16`) and progressively increasing to higher resolutions (e.g., `256x256`). Consequently, we need multiple versions of the dataset, such as `16x16`, `32x32`, `64x64`, `128x128`, and `256x256`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a job is submitted, we will receive a unique ID to reference back to it. We will store these IDs in the following ```job_map``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset Convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset convert action\n",
    "convert_action = \"dataset_convert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve default dataset convert spec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching a dataset convert job, we can retrieve the default spec for our action to use as a starting point to configure the dataset convert parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}/specs/{convert_action}/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(response)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "\n",
    "convert_specs = response.json()[\"default\"]\n",
    "\n",
    "print(json.dumps(convert_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Dataset Convert Spec \n",
    "\n",
    "Now we can customize the json spec object. No changes are needed unless you want to try different resolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "convert_specs[\"resolution\"] = [64,64]  # Use the value set here for modifying config during train,eval,export,inference\n",
    "print(json.dumps(convert_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Train Dataset convert Job  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dataset convert spec configured, we can now submit a dataset convert job. All jobs follow the same flow of retreiving the default spec, customizing it, then submitting the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "action = convert_action\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action, \"specs\":convert_specs,\n",
    "                #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "convert_id = response.json()\n",
    "job_map[\"train_\"+convert_action] = convert_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting the train dataset convert job, an ID is returned that we can use to monitor the job progress.\n",
    "\n",
    "The following cell will continuously print the latest status until the job is complete in success or failure state.\n",
    "More information on the runnning/completed status of a job will be visible in the response json\n",
    "\n",
    "This notebook will track all of the job IDs in the ```job_map``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = convert_id\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True) \n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val Dataset Convert - same process like train dataset convert: retrieving spec, configuring spec, submitting and monitoring job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve default dataset convert spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}/specs/{convert_action}/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(response)\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "convert_specs = response.json()[\"default\"]\n",
    "\n",
    "print(json.dumps(convert_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Dataset Convert Spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "convert_specs[\"resolution\"] = [64,64]  # Use the value set here for modifying config during train,eval,export,inference\n",
    "print(json.dumps(convert_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Val Dataset convert Job  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "action = convert_action\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action, \"specs\":convert_specs,\n",
    "                #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "convert_id = response.json()\n",
    "job_map[\"train_\"+convert_action] = convert_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = convert_id\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True) \n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset Convert - same process like train dataset convert: retrieving spec, configuring spec, submitting and monitoring job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve default dataset convert spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}/specs/{convert_action}/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(response)\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "convert_specs = response.json()[\"default\"]\n",
    "\n",
    "print(json.dumps(convert_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Dataset Convert Spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "convert_specs[\"resolution\"] = [64,64]  # Use the value set here for modifying config during train,eval,export,inference\n",
    "print(json.dumps(convert_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Test Dataset convert Job  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "action = convert_action\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action, \"specs\":convert_specs,\n",
    "                #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "convert_id = response.json()\n",
    "job_map[\"train_\"+convert_action] = convert_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = convert_id\n",
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True) \n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run any jobs such as training, evaluation or inference, we must create an experiment to setup the network architecture and associated datasets. Then we can chain several jobs together to create our trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_key = \"nvidia_tao\"\n",
    "checkpoint_choose_method = \"best_model\"\n",
    "data = json.dumps({\"network_arch\":model_name,\n",
    "                   \"encryption_key\":encode_key,\n",
    "                   \"checkpoint_choose_method\":checkpoint_choose_method,\n",
    "                   \"workspace\": workspace_id,\n",
    "                   \"train_datasets\":[train_dataset_id],\n",
    "                   \"eval_dataset\":eval_dataset_id,\n",
    "                   \"inference_dataset\":test_dataset_id})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"id\" in response.json()\n",
    "\n",
    "experiment_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign PTM <a class=\"anchor\" id=\"head-12\"></a>\n",
    "\n",
    "Search for the PTM on NGC for the SSL model chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "endpoint = f\"{base_url}/experiments:base\"\n",
    "params = {\"network_arch\": model_name}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "response_json = response.json()[\"experiments\"]\n",
    "\n",
    "for rsp in response_json:\n",
    "    rsp_keys = rsp.keys()\n",
    "    if \"encryption_key\" not in rsp.keys():\n",
    "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys\n",
    "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assigning pretrained models to different networks\n",
    "# From the output of previous cell make the appropriate changes to this map if you want to change the default PTM backbone.\n",
    "# Changing the default backbone here requires changing default spec/config during train/infer etc like for example\n",
    "# If you are changing the ptm to resnet34, then you have to modify the config key num_layers if it exists to 34 manually\n",
    "pretrained_map = {\"stylegan_xl\" : \"stylegan_xl:trainable_v1.0\",\n",
    "                  }\n",
    "no_ptm_models = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get pretrained model\n",
    "if model_name not in no_ptm_models:\n",
    "    endpoint = f\"{base_url}/experiments:base\"\n",
    "    params = {\"network_arch\": model_name}\n",
    "    response = requests.get(endpoint, params=params, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    response_json = response.json()[\"experiments\"]\n",
    "\n",
    "    # Search for ptm with given ngc path\n",
    "    ptm = []\n",
    "    for rsp in response_json:\n",
    "        rsp_keys = rsp.keys()\n",
    "        assert \"ngc_path\" in rsp_keys\n",
    "        if rsp[\"ngc_path\"].endswith(pretrained_map[model_name]):\n",
    "            assert \"id\" in rsp_keys\n",
    "            ptm_id = rsp[\"id\"]\n",
    "            ptm = [ptm_id]\n",
    "            print(\"Metadata for model with requested NGC Path\")\n",
    "            print(rsp)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name not in no_ptm_models:\n",
    "    ptm_information = {\"base_experiment\":ptm}\n",
    "    data = json.dumps(ptm_information)\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "    response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions <a class=\"anchor\" id=\"head-14\"></a>\n",
    "\n",
    "For all actions:\n",
    "1. Get default spec schema and derive the default values\n",
    "2. Modify defaults if needed\n",
    "3. Post spec dictionary to the service\n",
    "4. Run model action\n",
    "5. Monitor job using retrieve\n",
    "6. Download results using job download endpoint (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-15-automl-params\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if automl_enabled:\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "    while True:\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        if response.status_code == 404:\n",
    "            if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "                print(\"Base experiment spec file is being downloaded\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"automl_default_parameters\" in response.json().keys()\n",
    "    automl_params = response.json()[\"automl_default_parameters\"]\n",
    "    print(json.dumps(automl_params, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set AutoML related configurations <a class=\"anchor\" id=\"head-15-automl-algo-params\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if automl_enabled:\n",
    "    # Choose any metric that is present in the kpi dictionary present in the model's status.json. \n",
    "    # Example status.json for each model can be found in the respective section in NVIDIA TAO DOCS here: https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/index.html\n",
    "    metric=\"kpi\"\n",
    "\n",
    "    #Refer to parameter list mentioned in the above links and add/remove any extra parameter in addition to the default enabled ones in automl_specs\n",
    "\n",
    "    automl_information = {\"automl_enabled\": True,\n",
    "                          \"automl_algorithm\": automl_algorithm,\n",
    "                          \"automl_max_recommendations\": 20, # Only for bayesian\n",
    "                          \"automl_R\": 27, # Only for hyperband\n",
    "                          \"automl_nu\": 3, # Only for hyperband\n",
    "                          \"epoch_multiplier\": 1, # Only for hyperband\n",
    "                          # Warning: The parameters that are disabled are not tested by TAO, so there might be unexpected behaviour in overriding this\n",
    "                          \"override_automl_disabled_params\": False,\n",
    "                          \"automl_hyperparameters\": str(automl_params)}\n",
    "    data = json.dumps({\"metric\":metric, \"automl_settings\": automl_information})\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "    response = requests.patch(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    \n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve default training spec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching a training a job, we can retrieve the default training spec for our network architecture to use as a starting point to configure the training parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "# full_schema = response.json()\n",
    "# print(json.dumps(full_schema, indent=4)) ## Uncomment for verbose schema\n",
    "assert \"default\" in response.json().keys()\n",
    "train_specs = response.json()[\"default\"]\n",
    "print(json.dumps(train_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Spec \n",
    "\n",
    "Now we can customize the json spec object and set our dataset, training and model parameters. No changes are needed unless you are using a custom dataset or model architecture.\n",
    "\n",
    "#### Understanding the Spec Schema Structure\n",
    "\n",
    "Before customizing the training spec, it's helpful to understand the available configuration options and their valid values. The `get_bounds_of_field` utility function helps you explore the schema structure and find valid parameter values if they are defined on the backend (for some fields it might not be defined).\n",
    "\n",
    "**How to use the schema exploration utility:**\n",
    "\n",
    "```python\n",
    "from get_bounds import get_bounds_of_field\n",
    "\n",
    "# Example: Get valid values for backbone type\n",
    "# Note the conversion from dictionary access notation to list notation:\n",
    "# From: [\"dataset\"][\"common\"][\"img_resolution\"]\n",
    "# To:   [\"dataset\", \"common\", \"img_resolution\"]\n",
    "print(get_bounds_of_field(full_schema, [\"dataset\", \"common\", \"img_resolution\"]))\n",
    "```\n",
    "\n",
    "**Key differences in notation:**\n",
    "- **Dictionary access format**: `[\"dataset\"][\"common\"][\"img_resolution\"]` - This is how you would access nested values in Python\n",
    "- **Schema path format**: `[\"dataset\", \"common\", \"img_resolution\"]` - This is the format expected by the `get_bounds_of_field` function\n",
    "\n",
    "---\n",
    "\n",
    "#### Schema Navigation Tips\n",
    "\n",
    "1. **Nested Structure**: The schema follows a hierarchical structure where each level represents a configuration category\n",
    "2. **Path Format**: Always use a list of strings `[\"level1\", \"level2\", \"level3\"]` when calling `get_bounds_of_field`\n",
    "3. **Validation**: This helps ensure you're using valid parameter values before submitting your training job\n",
    "4. **Documentation**: Use these explorations to understand what options are available for your specific model architecture\n",
    "\n",
    "\n",
    "No changes to the default spec are needed unless you are using a custom dataset format or want to modify the model architecture and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Override any of the parameters listed in the previous cell as required\n",
    "train_specs[\"train\"][\"num_gpus\"] = 1\n",
    "train_specs[\"train\"][\"num_epochs\"] = 10\n",
    "train_specs[\"train\"][\"checkpoint_interval\"] = 10\n",
    "train_specs[\"train\"][\"validation_interval\"] = 10\n",
    "train_specs[\"model\"][\"generator\"][\"stem\"][\"resolution\"] = 64  # Set this to number set for dataset convert resolution\n",
    "train_specs[\"dataset\"][\"common\"][\"img_resolution\"] = 64  # Set this to number set for dataset convert resolution\n",
    "print(json.dumps(train_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Training Job  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training spec configured, we can now submit a training job. All jobs follow the same flow of retrieving the default spec, customizing it, then submitting the job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this job we will set the ```parent_job_id``` parameter in the body of the request to the last completed dataset convert job. This is required to make sure the training can start only after a successfull dataset convert and not result in runtime errors because of failed dataset convert jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = convert_id\n",
    "action = \"train\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":train_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "job_map[\"train_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "# For automl: Training times for different models benchmarked on 1 GPU V100 machine can be found here: https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#results-of-automl-experiments\n",
    "\n",
    "job_id = job_map[\"train_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to cancel the job for any reason, you can uncomment and run the following cell. You can also configure the endpoint to end with ```:pause``` or ```:resume``` instead of ```:cancel``` to temporarily stop and start the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train_\" + model_name]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "\n",
    "# response = requests.post(endpoint, headers=headers)\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate <a class=\"anchor\" id=\"head-17\"></a>\n",
    "\n",
    "In this section, we run the `stylegan_xl` evaluation to assess the performance of the trained StyleGAN-XL model by comparing the FID scores between real and synthetic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive default evaluation spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"default\" in response.json().keys()\n",
    "\n",
    "eval_specs = response.json()[\"default\"]\n",
    "print(json.dumps(eval_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Evaluation Spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "eval_specs[\"model\"][\"generator\"][\"stem\"][\"resolution\"] = 64  # Set this to number set for dataset convert resolution\n",
    "eval_specs[\"dataset\"][\"common\"][\"img_resolution\"] = 64  # Set this to number set for dataset convert resolution\n",
    "print(json.dumps(eval_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this job we will set the ```parent_job_id``` parameter in the body of the request to the completed training job. This is required to pass the trained model from the training job into our evaluation job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"evaluate\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":eval_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "job_map[\"evaluate_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"evaluate_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Trained checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we run the `stylegan_xl` inference to generate synthetic images using the trained StyleGAN-XL model.\n",
    "\n",
    "This StyleGAN-XL model is trained as a conditional GAN that can generate images from one of the six classes in the `NEU_Metal_Surface_Defects_Data/train` dataset. In the `Customize Inference spec`, the default `class_idx` will be `0`, but you can override it to any value between `0` and `5` to generate images from different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "assert \"default\" in response.json().keys()\n",
    "tao_inference_specs = response.json()[\"default\"]\n",
    "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes to specs if necessary\n",
    "tao_inference_specs[\"inference\"][\"class_idx\"] = 0  # Class you want to generate data for\n",
    "tao_inference_specs[\"model\"][\"generator\"][\"stem\"][\"resolution\"] = 64  # Set this to number set for dataset convert resolution\n",
    "tao_inference_specs[\"dataset\"][\"common\"][\"img_resolution\"] = 64  # Set this to number set for dataset convert resolution\n",
    "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this job we will set the ```parent_job_id``` parameter in the body of the request to the completed training job. This is required to pass the trained model from the training job into our inference job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"inference\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":tao_inference_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "job_map[\"inference_tao_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"inference_tao_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The generated images will be available in your cloud storage at /results/<inference_job_id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally run this section to delete the datasets and experiment results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete experiment <a class=\"anchor\" id=\"head-22\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete train dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete val dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete test dataset <a class=\"anchor\" id=\"head-24\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
