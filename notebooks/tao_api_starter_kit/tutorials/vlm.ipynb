{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Finetuning Microservices Workflow with TAO \n",
    "\n",
    "[NVIDIA TAO](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html) is a framework for customizing and optimizing vision-related models, to achieve higher accuracy and better performance. In TAO 6.25.10 release, we introduce VLM into our Finetuning Microservices (FTMS). This allows customers to finetune pre-trained VLMs like **Cosmos Reason**, with video/image-text data at scale.\n",
    "\n",
    "This Notebook will go over the steps to **finetune [Cosmos Reason](https://huggingface.co/nvidia/Cosmos-Reason1-7B) with [TAO FTMS](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_overview.html)**. And how to achieve higher accuracy with **AutoML**. \n",
    "\n",
    "\n",
    "For details on example fine-tuning use cases, please check out our two fine-tuning cookbooks for Cosmos Reason: [Reason for Visual Q&A in ITS](https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason1/intelligent-transportation/post_training.html) and [Reason for Warehouse Safety](https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason1/spatial-ai-warehouse/post_training.html). \n",
    "\n",
    "![Finetuning Workflow](../example_images/finetuning_workflow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [TAO FTMS Prerequisites](#head-1)\n",
    "1. [Dataset Preparation for VLM Fine-tuning](#head-3)\n",
    "1. [Experiments](#head-4)\n",
    "1. [AutoML Configuration](#head-4-4) \n",
    "1. [Launch Fine-Tuning](#head-5)\n",
    "1. [Model Evaluate](#head-6)\n",
    "1. [Model Quantization](#head-7)\n",
    "1. [Inference and Inference Microservice](#head-8)\n",
    "1. [Finish Experiment and Cleanup](#head-9)\n",
    "1. [Model Deployment](#head-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "### 1.1 TAO API service\n",
    "\n",
    "The TAO API is a cloud service for end‚Äëto‚Äëend model development. With a few calls you can import cloud datasets, pull pretrained models and default specs from the Nvidia Cloud Registry (NGC), train, evaluate, optimize, and export models for edge/cloud deployment‚Äî all on GPU‚Äëpowered, multi‚Äënode clusters.\n",
    "\n",
    "To get started with TAO APIs:\n",
    "\n",
    "**Hardware and Software Minimum Requirements:**\n",
    "\n",
    "- Minimum 8x A100 GPUs with at least 80 GiB GPU memory.\n",
    "- OS: Ubuntu 22.04+\n",
    "- Drivers: 570+\n",
    "- CUDA: 12.8+\n",
    "- Python: 3.12+\n",
    "\n",
    "**Setup TAO APIs**\n",
    "\n",
    "- Follow [TAO API deployment steps](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#deployment-steps) 1-8\n",
    "- After successfully installing these steps, you will have a server setup with Kubernetes (K8s) and TAO APIs\n",
    "\n",
    "### 1.2 Set Required Parameters\n",
    "\n",
    "Before running this notebook, ensure you have the following information:\n",
    "\n",
    "1. **Host URL:** The host URL is the external access point to a Kubernetes service, constructed using the node‚Äôs IP address and the service‚Äôs exposed NodePort. Example: http://<ip_address>:<port_number>\n",
    "1. **Finetuning mode:**: Full SFT or PEFT LORA\n",
    "1. **NGC Key:** Your NGC (NVIDIA GPU Cloud) API key.\n",
    "1. **Huggingface token:** Huggingface token obtained from [here](https://huggingface.co/settings/tokens).\n",
    "1. **NGC Organization Name:** The name of your NGC organization.\n",
    "1. **Cloud Storage Details:** Set your cloud storage details (e.g., bucket name, region).\n",
    "1. **Datasets Path:** The path of datasets relative to the cloud storage bucket.\n",
    "\n",
    "Replace the **FIXME** placeholders in the code cells below with the appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_name\n",
    "%store -r finetuning_mode\n",
    "%store -r automl_algorithm\n",
    "%store -r automl_max_recommendations\n",
    "%store -r base_url\n",
    "%store -r headers\n",
    "%store -r workspace_id\n",
    "%store -r train_dataset_id\n",
    "%store -r eval_dataset_id\n",
    "%store -r experiment_id\n",
    "%store -r job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cosmos-rl\"\n",
    "finetuning_mode = \"lora\" # FIXME2. lora or full\n",
    "%store model_name\n",
    "%store finetuning_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure AutoML Parameters\n",
    "\n",
    "[AutoML documentation](https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#getting-started)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Configuration\n",
    "automl_algorithm = \"bayesian\"\n",
    "automl_max_recommendations = 5  # Number of AutoML experiments to run\n",
    "%store automl_algorithm\n",
    "%store automl_max_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Set API service's host information\n",
    "\n",
    "The steps in 1.1 will install a k8 server and TAO APIs, and once that is installed, you will need a host_url to call the APIs running on the current host.\n",
    "To access the host_url: \n",
    "\n",
    "In the host machine, node ip_address and port number can be obtained as follows,\n",
    "- **ip_address**: hostname -i\n",
    "- **port_number**: kubectl get service ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port number is 32080 for K8s deployment and 8090 for docker-compose deployment\n",
    "# IP address is the IP address of the host machine for K8s deployment and localhost for docker-compose deployment\n",
    "host_url = \"http://<ip_address>:<port_number>\" # FIXME1. eg: https://10.137.149.22:32080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Set NGC Personal key for authentication and NGC org to access API services\n",
    "\n",
    "- **ngc_key**: [How to access NGC key](https://docs.nvidia.com/ai-enterprise/deployment/spark-rapids-accelerator/latest/appendix-ngc.html#ngc-api-key)\n",
    "- **ngc_org_name**: [How to access NGC org Name](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#accessing-ngc-org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = \"<ngc_personal_key>\" # FIXME2. Make sure to add NGC Personal key\n",
    "hf_token = \"<huggingface_token>\" # FIXME3. Add your Huggingface token - needed so that Huggingface doesn't rate limit you.\n",
    "ngc_org_name = \"nvstaging\" # FIXME4. Add your NGC ORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Login to the host <a class=\"anchor\" id=\"head-2\"></a>\n",
    "The JWT (JSON Web Token) is a secure authentication mechanism used by the TAO Finetuning Microservices. When you authenticate with your NGC credentials, the API returns this token, which is then used for all subsequent API calls. This token has a limited lifetime and represents your authenticated session.\n",
    "\n",
    "The following cell ensures you are able to access the service and generate a JWT Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate NGC_PERSONAL_KEY\n",
    "data = json.dumps({\"ngc_org_name\": ngc_org_name,\n",
    "                   \"ngc_key\": ngc_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/orgs/{ngc_org_name}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "%store base_url\n",
    "%store headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create cloud workspace\n",
    "This creates a workspace that links your TAO Finetuning Microservices session to your cloud storage. The API will use these credentials to:\n",
    "\n",
    "- Pull datasets from your bucket\n",
    "- Store training results and checkpoints\n",
    "- Upload evaluation results\n",
    "\n",
    "If you want to have different workspaces for datasets and experiments, duplicate the workspace creation part and adjust the metadata accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME5 Cloud bucket details to access datasets and store experiment results\n",
    "cloud_metadata = {\n",
    "    \"name\": \"tao_workspace\",\n",
    "    \"cloud_type\": \"aws\",\n",
    "    \"cloud_specific_details\": {\n",
    "        \"cloud_region\": \"us-west-1\",\n",
    "        \"cloud_bucket_name\": \"\",\n",
    "        \"access_key\": \"\",\n",
    "        \"secret_key\": \"\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps(cloud_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/workspaces\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "workspace_id = response.json()[\"id\"]\n",
    "%store workspace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataset Preparation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "For Cosmos-RL finetuning, we expect the directory tree to follow this structure:\n",
    "\n",
    "```\n",
    "<any folder in cloud bucket>/\n",
    "‚îú‚îÄ‚îÄ images.tar.gz\n",
    "‚îú‚îÄ‚îÄ annotations.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "For this experiment, we will demonstrate on the [Physical AI Spatial Intelligence Warehouse dataset](https://huggingface.co/datasets/nvidia/PhysicalAI-Spatial-Intelligence-Warehouse/tree/main). This is a completely synthetic dataset of a warehouse with 95K images along with around 500k annotations : Q&A pairs with related meta information in LLaVA format for VLM training.  Tasks included distance, counting, multiple-choice grounding, and spatial relation reasoning.\n",
    "\n",
    "Below example shows the RGB frame, depth map, annotated regions, the corresponding question, and sample answers.\n",
    "The distribution of question types demonstrated the diversity of reasoning skills required across tasks.\n",
    "\n",
    "<img src=\"assets/data_overview.png\" width=\"960\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Sample JSON Entry\n",
    "\n",
    "Specifically, the annotation contains several additional attributes compared to general [LLaVa format](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md):\n",
    "\n",
    "- **normalized_answer** field for quantitative evaluation with accuracy and error metrics between ground-truth and predicted answer.\n",
    "- **freeform_answer** field, which is the original answer from 'gpt'.\n",
    "- **rle** denotes the corresponding masks per object in pycoco format.\n",
    "- **category** denotes the question category. The categories are left_right, multi_choice_question(mcq), distance, and count.\n",
    "\n",
    "Here's an example of the annotation format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"9d17ba0ab1df403db91877fe220e4658\",\n",
    "    \"image\": \"000190.png\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"<image>\\nCould you measure the distance between the pallet <mask> and the pallet <mask>?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"The pallet [Region 0] is 6.36 meters from the pallet [Region 1].\"\n",
    "      }\n",
    "    ],\n",
    "    \"rle\": [\n",
    "      {\n",
    "      \"size\": [\n",
    "          1080,\n",
    "          1920\n",
    "      ],\n",
    "      \"counts\": \"bngl081MYQ19010ON2jMDmROa0ol01_RO2^m0`0PRODkm0o0bQOUO[n0U2N2M3N2N2N3L3N2N1N1WO_L]SO\"\n",
    "      },\n",
    "      {\n",
    "      \"size\": [\n",
    "          1080,\n",
    "          1920\n",
    "      ],\n",
    "      \"counts\": \"^PmU1j1no000000000000000000001O0000000000001O0000000000001O0000000000001O0000000000\"\n",
    "      }\n",
    "    ],\n",
    "    \"category\": \"distance\",\n",
    "    \"normalized_answer\": \"6.36\",\n",
    "    \"freeform_answer\": \"The pallet [Region 0] is 6.36 meters from the pallet [Region 1].\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the data pre-processing scripts [here](https://github.com/nvidia-cosmos/cosmos-cookbook/blob/main/docs/recipes/post_training/reason1/spatial-ai-warehouse/post_training.md#data-preprocessing) and then transfer the processed data onto your cloud storage for both your train and evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME6: Set paths relative to cloud bucket\n",
    "train_dataset_path =  \"/data/cosmos_rl_warehouse_train\" # example train data is at workspace_dir/data/\n",
    "eval_dataset_path = \"/data/cosmos_rl_warehouse_eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set dataset formats\n",
    "\n",
    "The dataset format parameters define how your data is structured:\n",
    "- `ds_type = \"vlm\"`: Indicates this is a Vision-Language Model dataset\n",
    "- `ds_format = \"llava\"`: Uses the default format expected by Cosmos-RL\n",
    "\n",
    "The \"default\" format for VLM datasets typically includes:\n",
    "- Video files or image sequences\n",
    "- Text annotations/captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = \"vlm\"\n",
    "ds_format = \"llava\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create and pull train dataset\n",
    "\n",
    "Add all the training dataset metadata to *datasets* api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "train_dataset_metadata = {\"type\": ds_type,\n",
    "                          \"format\": ds_format,\n",
    "                          \"workspace\":workspace_id,\n",
    "                          \"cloud_file_path\": train_dataset_path,\n",
    "                          \"use_for\": [\"training\"]\n",
    "                          }\n",
    "\n",
    "data = json.dumps(train_dataset_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "train_dataset_id = response.json()[\"id\"]\n",
    "%store train_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell checks the download progress till the dataset pull is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create and pull evaluation dataset\n",
    "\n",
    "Similar to training data, add eval dataset metadata as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation dataset\n",
    "eval_dataset_metadata = {\"type\": ds_type,\n",
    "                          \"format\": ds_format,\n",
    "                          \"workspace\":workspace_id,\n",
    "                          \"cloud_file_path\": eval_dataset_path,\n",
    "                          \"use_for\": [\"evaluation\"]\n",
    "                          }\n",
    "\n",
    "data = json.dumps(eval_dataset_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "eval_dataset_id = response.json()[\"id\"]\n",
    "%store eval_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell checks the download progress till the dataset pull is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 List the created datasets <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(response)\n",
    "\n",
    "datasets = response.json()[\"datasets\"]\n",
    "for rsp in datasets:\n",
    "    rsp_keys = rsp.keys()\n",
    "\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in datasets:\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "In the TAO finetuning microservices, experiments are used for workflow management with the following key features:\n",
    "- **Workflow Chaining:** Chain multiple model actions together with defined dependencies\n",
    "- **Dependency Management:** Create structured workflows with clear dependencies between actions\n",
    "- **Metadata Configuration:** Each experiment can incorporate various metadata:\n",
    "    - Docker environment variables\n",
    "    - Cloud workspace assignment for storing model action results\n",
    "    - Pretrained model to be used in the workflow\n",
    "    - Datasets that are to be used in the workflow\n",
    "\n",
    "### 4.1 Create experiment for VLM workflow\n",
    "\n",
    "Define the experiment arguments\n",
    "\n",
    "- network_arch\n",
    "- workspace id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps({\"network_arch\":model_name,\n",
    "                   \"workspace\": workspace_id})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "experiment_id = response.json()[\"id\"]\n",
    "%store experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 List experiments\n",
    "\n",
    "Validate that the experiment is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "params = {\"network_arch\": \"cosmos-rl\"}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "\n",
    "print(response)\n",
    "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose list output\n",
    "print(\"model id\\t\\t\\t     network architecture\")\n",
    "for rsp in response.json()[\"experiments\"]:\n",
    "    rsp_keys = rsp.keys()\n",
    "    print(rsp[\"name\"], rsp[\"id\"],rsp[\"network_arch\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Assign train, eval datasets\n",
    "\n",
    "Set dataset configuration for training and evaluation.\n",
    "\n",
    "- Set the docker env variable: we are using HF token to pull the evalution dataset from HF.\n",
    "- Define train_datasets and eval_datasets from above train and eval data ids (check section 3.3 and 3.4)\n",
    "- add dataset_information to *experiments id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docker_env_vars = {\"HF_TOKEN\": hf_token}\n",
    "dataset_information = {\"train_datasets\":[train_dataset_id],\n",
    "                       \"eval_dataset\": eval_dataset_id,\n",
    "                       \"calibration_dataset\": eval_dataset_id,\n",
    "                       \"docker_env_vars\": docker_env_vars\n",
    "                       }\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Update Experiment with AutoML Paramaters <a class=\"anchor\" id=\"head-4-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.1 View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"automl_default_parameters\" in response.json().keys()\n",
    "automl_params = response.json()[\"automl_default_parameters\"]\n",
    "print(json.dumps(automl_params, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoML Parameters Configuration\n",
    "\n",
    "The `automl_params` list retrieved above contains the **default AutoML hyperparameters** that have been carefully chosen for this model architecture.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- ‚úÖ **Recommended**: Use the default parameters as-is ‚Äî they are handpicked and validated for optimal performance\n",
    "- üîß **Customizable**: You can add, modify, or remove parameters based on your specific requirements\n",
    "- ‚ö†Ô∏è **Important**: Any modifications to this list will directly affect the AutoML experiment behavior\n",
    "\n",
    "1. **`custom.vision.fps`**\n",
    "   - Video sampling rate in frames per second for vision-language models. Higher FPS captures more temporal information but increases memory usage.\n",
    "   - Valid range: 1-3\n",
    "\n",
    "1. **`train.epoch`**\n",
    "   - Total number of training epochs (complete passes through the dataset).\n",
    "   - Valid range: 10-20\n",
    "\n",
    "1. **`train.optm_lr`**\n",
    "   - Peak learning rate for optimizer. Actual LR follows warmup and cosine decay schedule.\n",
    "   - Valid range: 0-inf\n",
    "   - Default: 1e-06\n",
    "\n",
    "1. **`policy.lora.r`**\n",
    "   - LoRA rank (dimension of low-rank decomposition matrices). Higher rank = more expressive but more parameters to train.\n",
    "   - Lower values (4-8) for efficient fine-tuning, higher values (16-64) for better performance on complex tasks.\n",
    "   - Valid range: 1-256\n",
    "   - Default: 8\n",
    "\n",
    "1. **`policy.lora.lora_alpha`**\n",
    "   - LoRA scaling factor that controls the magnitude of LoRA updates. Typically set equal to or double the rank.\n",
    "   - Formula: effective_lr = lora_alpha / r (or lora_alpha / sqrt(r) if use_rslora=True)\n",
    "   - Valid range: 1-256\n",
    "   - Default: 8.0\n",
    "\n",
    "1. **`policy.lora.lora_dropout`**\n",
    "   - Dropout probability applied to LoRA layers for regularization. Helps prevent overfitting.\n",
    "   - 0.0 = no dropout (recommended for small datasets), 0.05-0.1 for larger datasets\n",
    "   - Valid range: 0.0-0.5\n",
    "   - Default: 0.0\n",
    "\n",
    "1. **`train.optm_decay_type`**\n",
    "    - Type of learning rate decay schedule after warmup phase completes.\n",
    "    - **linear**: Linear decay from peak LR to min LR (weight: 0.1)\n",
    "    - **sqrt**: Square root decay, slower initial decay (weight: 0.1)\n",
    "    - **cosine**: Cosine annealing, smooth decay (weight: 0.4, recommended)\n",
    "    - **none**: No decay, constant LR after warmup (weight: 0.4)\n",
    "    - Valid options: linear, sqrt, cosine, none\n",
    "    - Default: linear\n",
    "    - **Recommendation**: Use 'cosine' (40% weight) or 'none' (40% weight) for most tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "\n",
    "The final `automl_params` configuration (after any modifications) will be used to launch the AutoML training experiments in the subsequent cells.\n",
    "\n",
    "1. **Review Available Parameters** (next cell)\n",
    "   - Lists all trainable parameters you can add to `automl_params`\n",
    "   - Helps you discover additional hyperparameters to tune\n",
    "\n",
    "2. **Get Parameter Details** (optional)\n",
    "   - Use the `:get_automl_param_details` endpoint to inspect specific parameters\n",
    "   - View valid ranges, data types, and default values before tuning\n",
    "\n",
    "3. **Update Parameter Ranges** (optional)\n",
    "   - Use the `:update_automl_param_ranges` endpoint to customize search spaces\n",
    "   - Narrow or expand ranges based on your domain knowledge\n",
    "\n",
    "4. **Launch AutoML Experiment** (upcoming cells)\n",
    "   - The AutoML system will:\n",
    "     - Sample hyperparameter combinations from the defined ranges\n",
    "     - Train multiple models in parallel\n",
    "     - Track performance metrics for each configuration\n",
    "     - Identify the best performing model\n",
    "\n",
    "5. **Monitor AutoML Progress**\n",
    "   - Use the `:automl_details` endpoint to track experiment status\n",
    "   - View results, configurations, and performance for each trial\n",
    "   - Identify which hyperparameters impact performance most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def flatten_dict(nested_dict, parent_key='', sep='.'):\n",
    "    items = []\n",
    "    for key, value in nested_dict.items():\n",
    "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            items.extend(flatten_dict(value, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)\n",
    "\n",
    "default_train_spec = copy.deepcopy(response.json()[\"default\"])\n",
    "param = flatten_dict(default_train_spec)\n",
    "for k, v in param.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get AutoML Parameter Details\n",
    "\n",
    "Retrieve detailed information about specific parameters that can be tuned by AutoML. This endpoint returns the parameter's data type, default values, valid ranges, and constraints.\n",
    "\n",
    "##### Request Parameters\n",
    "\n",
    "- **parameters**: Comma-separated list of parameter paths to query (e.g., `\"train.optm_betas,train.epoch\"`)\n",
    "\n",
    "##### Response Structure\n",
    "\n",
    "For each parameter, you'll receive:\n",
    "- **parameter**: The full config path\n",
    "- **value_type**: Data type (e.g., `list_2`, `int`, `float`, `str`)\n",
    "- **default**: Default configuration including:\n",
    "  - `default_value`: The default value(s)\n",
    "  - `valid_min`: Minimum allowed value(s)\n",
    "  - `valid_max`: Maximum allowed value(s)\n",
    "  - `valid_options`: List of allowed discrete values (if applicable)\n",
    "  - `math_cond`: Mathematical constraints\n",
    "  - `depends_on`: Parameters this depends on\n",
    "- **custom**: Any custom overrides applied\n",
    "\n",
    "##### Example Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"parameter\": \"train.optm_betas\",\n",
    "    \"value_type\": \"list_2\",\n",
    "    \"default\": {\n",
    "        \"default_value\": [0.9, 0.999],\n",
    "        \"valid_min\": [0.8, 0.9],\n",
    "        \"valid_max\": [0.95, 0.999]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This information helps you understand valid ranges before setting up AutoML experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}:get_automl_param_details\"\n",
    "\n",
    "params = {\"parameters\": \"train.optm_decay_type\"}\n",
    "\n",
    "response = requests.get(endpoint, headers=headers, params=params)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update AutoML Parameter Ranges\n",
    "\n",
    "Define the hyperparameter search space for AutoML optimization. This allows you to customize which parameters AutoML will tune and their allowed ranges.\n",
    "\n",
    "##### Configurable Parameters\n",
    "\n",
    "The `parameter_ranges` array allows you to specify:\n",
    "- **parameter**: The config path (e.g., `train.optm_betas`, `train.epoch`)\n",
    "- **valid_min**: Minimum value(s) for the parameter\n",
    "- **valid_max**: Maximum value(s) for the parameter\n",
    "\n",
    "**Note:** Array parameters like `optm_betas` require array values for min/max, while scalar parameters use single values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_custom_range = [\n",
    "    {\n",
    "        \"parameter\": \"train.optm_lr\",\n",
    "        \"valid_min\": 5e-06,\n",
    "        \"valid_max\": 2e-04\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"train.epoch\",\n",
    "        \"valid_min\": 1,\n",
    "        \"valid_max\": 3\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"train.optm_betas\",\n",
    "        \"valid_min\": [0.9, 0.995],\n",
    "        \"valid_max\": [0.95, 0.999]\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"train.optm_decay_type\",\n",
    "        \"valid_options\": [\"cosine\", \"none\"],\n",
    "        \"option_weights\": [0.7, 0.3]\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"policy.lora.r\",\n",
    "        \"valid_min\": 4,\n",
    "        \"valid_max\": 64\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"policy.lora.lora_alpha\",\n",
    "        \"valid_min\": 128,\n",
    "        \"valid_max\": 1024\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"policy.lora.lora_dropout\",\n",
    "        \"valid_min\": 0.03,\n",
    "        \"valid_max\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"parameter\": \"custom.vision.fps\",\n",
    "        \"valid_min\": 1,\n",
    "        \"valid_max\": 2\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}:update_automl_param_ranges\"\n",
    "\n",
    "data =  {\n",
    "        \"parameter_ranges\": automl_custom_range\n",
    "    }\n",
    "\n",
    "response = requests.patch(endpoint, headers=headers, json=data)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update the experiment metadata with automl parameters to run experiments on <a class=\"anchor\" id=\"head-14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_information = {\n",
    "    \"automl_enabled\": False,\n",
    "    \"automl_algorithm\": automl_algorithm,\n",
    "    \"automl_max_recommendations\": 2,\n",
    "    \"automl_hyperparameters\": str(automl_params)\n",
    "}\n",
    "data = json.dumps({\"metric\":\"kpi\", \"automl_settings\": automl_information})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, headers=headers, data=data)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch Fine-tuning <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "For all **Actions**:\n",
    "1. Get default spec schema and derive the default values\n",
    "2. Modify defaults if needed\n",
    "3. Post spec dictionary to the service\n",
    "4. Run model action\n",
    "5. Monitor job using retrieve\n",
    "6. Download results using job download endpoint (if needed)\n",
    "\n",
    "**Note** Here Actions stand for TAO Apis for: *train/eval/infer/..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Get default spec schema\n",
    "\n",
    "List all the possible configuration needed for finetuning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema \n",
    "train_specs = response.json()[\"default\"]\n",
    "print(json.dumps(train_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Customize train model configuration\n",
    "\n",
    "Override any of the configration for example update the batch size for training or num of gpus etc.\n",
    "- `dp_shard_size` is the number of GPUs to be used for training\n",
    "- For OOM issues\n",
    "  - Try reducing the following\n",
    "    - `dataloader_num_workers`\n",
    "    - `dataloader_prefetch_factor`\n",
    "    - `mini_batch`\n",
    "    - `total_pixels`\n",
    "    - `fps`\n",
    "    - `train_batch_per_replica`\n",
    "    - `model_max_length`\n",
    "  - Disable\n",
    "    - `enable_dataset_cache`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_specs[\"train\"][\"epoch\"] = 10\n",
    "train_specs[\"train\"][\"ckpt\"][\"save_freq_in_epoch\"] = train_specs[\"train\"][\"epoch\"]\n",
    "train_specs[\"validation\"][\"freq_in_epoch\"] = train_specs[\"train\"][\"epoch\"]\n",
    "train_specs[\"validation\"][\"batch_size\"] = 1\n",
    "\n",
    "train_specs[\"train\"][\"ckpt\"][\"save_mode\"] = \"sync\"\n",
    "\n",
    "train_specs[\"train\"][\"train_policy\"] = {\n",
    "    \"dataset\":{\n",
    "        \"name\":\"sdg\",\n",
    "        \"test_size\":1,\n",
    "    },\n",
    "    \"type\":\"sft\",\n",
    "    \"enable_dataset_cache\":True,\n",
    "    \"dataloader_num_workers\":8,\n",
    "    \"dataloader_prefetch_factor\":8,\n",
    "    \"conversation_column_name\":\"conversations\",\n",
    "    \"mini_batch\":1,\n",
    "}\n",
    "\n",
    "if \"max_pixels\" in train_specs[\"custom\"][\"vision\"]:\n",
    "    del train_specs[\"custom\"][\"vision\"][\"max_pixels\"]\n",
    "train_specs[\"custom\"][\"vision\"][\"total_pixels\"] = 3136000\n",
    "train_specs[\"custom\"][\"vision\"][\"fps\"] = 1.0\n",
    "\n",
    "train_specs[\"policy\"][\"parallelism\"][\"dp_shard_size\"] = 2\n",
    "\n",
    "train_specs[\"train\"][\"train_batch_per_replica\"] = 32\n",
    "train_specs[\"policy\"][\"model_max_length\"] = 8192\n",
    "train_specs[\"custom\"][\"system_prompt\"] = \"Answer the questions.\"\n",
    "\n",
    "if finetuning_mode != \"lora\" and \"lora\" in train_specs[\"policy\"]:\n",
    "    del train_specs[\"policy\"][\"lora\"]\n",
    "elif finetuning_mode == \"lora\":\n",
    "    train_specs[\"policy\"][\"lora\"][\"lora_alpha\"] = 16\n",
    "    train_specs[\"policy\"][\"lora\"][\"lora_r\"] = 16\n",
    "    train_specs[\"policy\"][\"lora\"][\"lora_dropout\"] = 0.05\n",
    "\n",
    "print(json.dumps(train_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Run *train* Action \n",
    "\n",
    "Run *train* action with the configurations defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action = \"train\"\n",
    "train_request_body = {\n",
    "    \"parent_job_id\":None,\n",
    "    \"action\":action,\n",
    "    \"specs\":train_specs}\n",
    "data = json.dumps(train_request_body)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"train\"] = response.json()\n",
    "print(job_map)\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Monitor Job Status\n",
    "\n",
    "The cell below will continuously monitor your training job and display real-time progress updates. This monitoring loop will automatically refresh until the job completes, fails, or is manually stopped.\n",
    "\n",
    "#### Expected Training Times\n",
    "\n",
    "- **Baseline**: Each experiment running for **1 epoch** takes approximately **55 minutes on 8x A100 GPUs**\n",
    "- **Scaling**: Training time scales linearly with the number of epochs\n",
    "  - SFT\n",
    "    - 5 epochs ‚âà 4.5 hours\n",
    "    - 10 epochs ‚âà 9 hours\n",
    "    - 20 epochs ‚âà 18 hours\n",
    "  - LORA takes around 65% of the SFT training time\n",
    "\n",
    "#### Job Logs\n",
    "\n",
    "This cell will give you the logs on the current running AutoML experiment in a loop until all automl experiments are completed\n",
    "\n",
    "Preceding the logs, will the AutoML brain info, like how many more epochs in the total AutoML need to be completed, what is the current experiment ID, eta for total AutoML completion etc\n",
    "\n",
    "- **Individual Experiment Log**:\n",
    "  - Unique `job_id` for each AutoML experiment\n",
    "  - Current hyperparameter configuration being tested\n",
    "  - Per-experiment metrics and status\n",
    "  \n",
    "- **AutoML Brain Summary**:\n",
    "  - Number of experiments remaining in the AutoML search\n",
    "  - Estimated Time to Completion (ETA)\n",
    "  - Current best metric value across all experiments\n",
    "  - Recommendation progress and performance trends\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b> To stop the training job at any time, refer to the instructions in the next cell below.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_id = job_map[\"train\"]\n",
    "job_metadata_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    job_metadata_response = requests.get(job_metadata_endpoint, headers=headers)\n",
    "    if \"error_desc\" in job_metadata_response.json().keys() and job_metadata_response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    automl_experiment_index = None\n",
    "    automl_brain_info = job_metadata_response.json().get(\"job_details\", {}).get(job_id, {}).get(\"automl_brain_info\")\n",
    "\n",
    "    if automl_brain_info is not None:\n",
    "        for metric in automl_brain_info:\n",
    "            print(f'{metric[\"metric\"]}: {metric[\"value\"]}')\n",
    "            if metric[\"metric\"] == \"Current experiment id\":\n",
    "                automl_experiment_index = int(metric[\"value\"]) - 1\n",
    "\n",
    "    params = {}\n",
    "    if automl_experiment_index is not None and automl_experiment_index >= 0:\n",
    "        params = {\"automl_experiment_index\":automl_experiment_index}\n",
    "    \n",
    "    job_logs_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "    job_logs_response = requests.get(job_logs_endpoint, headers=headers, params=params)\n",
    "    print(\"\\nLogs of job: \\n\",job_logs_response.text)\n",
    "\n",
    "    if job_metadata_response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or job_metadata_response.status_code not in (200,201):\n",
    "        print(json.dumps(job_metadata_response.json(), sort_keys=True, indent=4))\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can fetch job of individual AutoML experiments by their index one time below\n",
    " - -1 index for AutoML brain logs\n",
    " - For non automl jobs ignore the params field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train\"]\n",
    "# params = {\"automl_experiment_index\":-1}\n",
    "# job_logs_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "# job_logs_response = requests.get(job_logs_endpoint, headers=headers, params=params)\n",
    "# print(\"\\nLogs of job: \\n\",job_logs_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can fetch job metadata one time below\n",
    "  - Unique `job_id` for each AutoML experiment\n",
    "  - Per-experiment metrics and status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train\"]\n",
    "# job_metadata_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "# print(json.dumps(job_metadata_response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### To Stop the finetuning Job\n",
    "1. Stop code cell in step 5.4 (the cell right before this cell) manually\n",
    "2. Uncomment the snippet in the next cell and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train\"]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:pause\"\n",
    "\n",
    "# data = {\"graceful\":False}\n",
    "\n",
    "# response = requests.post(endpoint, headers=headers, json=data)\n",
    "\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Resume Finetuning job\n",
    "\n",
    "Uncomment the below snippet if you want to resume an already stopped finetuning job and then run code cell in step **5.3 Monitor job status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# job_id = job_map[\"train\"]\n",
    "# endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:resume\"\n",
    "\n",
    "# data = json.dumps({\"parent_job_id\":None, \"specs\":train_specs})\n",
    "# response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate <a class=\"anchor\" id=\"head-6\"></a>\n",
    "\n",
    "Once the model is finetuned, we start evaluation. The model used in evaluation will be as per the predefined checkpoint chosen method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "evaluate_specs = response.json()[\"default\"][\"evaluate\"]\n",
    "print(json.dumps(evaluate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_specs[\"vision\"][\"fps\"] = 1.0\n",
    "evaluate_specs[\"vision\"][\"total_pixels\"] = 3136000\n",
    "if finetuning_mode == \"lora\":\n",
    "    evaluate_specs[\"model\"][\"enable_lora\"] = True\n",
    "    evaluate_specs[\"model\"][\"base_model_path\"] = \"hf_model://nvidia/Cosmos-Reason1-7B\"\n",
    "print(json.dumps(evaluate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Run *evaluate* action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parent = job_map[\"train\"]\n",
    "action = \"evaluate\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":evaluate_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"evaluate\"] = response.json()\n",
    "print(job_map)\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'evaluate': '8a50a050-55c1-4fb1-93f5-84315c448a00'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor job status by repeatedly running this cell. Stop the cell when you are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_id = job_map[\"evaluate\"]\n",
    "job_metadata_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "job_logs_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    job_metadata_response = requests.get(job_metadata_endpoint, headers=headers)\n",
    "    \n",
    "    job_logs_response = requests.get(job_logs_endpoint, headers=headers)\n",
    "    print(job_logs_response.text)\n",
    "    \n",
    "    if job_metadata_response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or job_metadata_response.status_code not in (200,201):\n",
    "        print(json.dumps(job_metadata_response.json(), indent=4))\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quantization <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "- Run FP8 quantization of the LLM modules of the Cosmos-Reason finetuned model created at Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Customize quantization action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/quantize/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "quantize_specs = response.json()[\"default\"][\"quantize\"]\n",
    "print(json.dumps(quantize_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to the specs dictionary if necessary\n",
    "if finetuning_mode == \"lora\":\n",
    "    quantize_specs[\"model\"][\"enable_lora\"] = True\n",
    "    quantize_specs[\"model\"][\"base_model_path\"] = \"hf_model://nvidia/Cosmos-Reason1-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Run *quantize* Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = job_map[\"train\"]\n",
    "action = \"quantize\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":quantize_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"quantize\"] = response.json()\n",
    "print(job_map)\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor job status by repeatedly running this cell. Stop the cell when you are done checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map['quantize']\n",
    "job_metadata_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "job_logs_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    job_metadata_response = requests.get(job_metadata_endpoint, headers=headers)\n",
    "\n",
    "    job_logs_response = requests.get(job_logs_endpoint, headers=headers)\n",
    "    print(job_logs_response.text)\n",
    "\n",
    "    if job_metadata_response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or job_metadata_response.status_code not in (200,201):\n",
    "        print(json.dumps(job_metadata_response.json(), indent=4))\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "- Run inference on a set of images using the quantized model created at Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Inference as a one-time Job <a class=\"anchor\" id=\"head-7\"></a>\n",
    "If you just want to run inference on one set of inputs, run section 8.1, if you wanna try multiple times with different settings and inputs, jump to inference microservices in step 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Customize inference action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "inference_specs = response.json()[\"default\"][\"inference\"]\n",
    "print(json.dumps(inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to the specs dictionary if necessary\n",
    "inference_specs[\"prompt\"] = \"When does something happen in the video?\"\n",
    "inference_specs[\"media\"] = \"aws://nvcf-storage-handling/data/vlm_inference/videos/test_video.mp4\" # Format: cloud_type://bucket_name/video_file_path\n",
    "# # Uncomment this if parent job is train instead of quantize\n",
    "# if finetuning_mode == \"lora\":\n",
    "#     inference_specs[\"enable_lora\"] = True\n",
    "#     inference_specs[\"base_model_path\"] = \"hf_model://nvidia/Cosmos-Reason1-7B\"\n",
    "print(json.dumps(inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Run *inference* Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = job_map[\"quantize\"]\n",
    "action = \"inference\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":inference_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"inference\"] = response.json()\n",
    "print(job_map)\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor job status by repeatedly running this cell. Stop the cell when you are done checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map['inference']\n",
    "job_metadata_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "job_logs_endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    job_metadata_response = requests.get(job_metadata_endpoint, headers=headers)\n",
    "\n",
    "    job_logs_response = requests.get(job_logs_endpoint, headers=headers)\n",
    "    print(job_logs_response.text)\n",
    "\n",
    "    if job_metadata_response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or job_metadata_response.status_code not in (200,201):\n",
    "        print(json.dumps(job_metadata_response.json(), indent=4))\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Inference Microservice <a class=\"anchor\" id=\"head-7\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start inference microservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}/inference_microservice/start\"\n",
    "data = {\n",
    "    \"parent_id\": job_map[\"quantize\"],\n",
    "    # \"model_path\": \"\" # By default, we use the finetuned model. If you want to use any other model other than the finetuned model, you can specify the model path in the cloud here\n",
    "    # Uncomment the below if parent job is train instead of quantize\n",
    "    # \"enable_lora\": True,\n",
    "    # \"base_model_path\": \"hf_model://nvidia/Cosmos-Reason1-7B\"\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=data, headers=headers)\n",
    "job_map[\"inference_microservice\"] = response.json()[\"job_id\"]\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the status of inference microservice, wait until the status is ready before proceeding to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map['inference_microservice']\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/inference_microservice/status\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint,headers=headers)\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        print(\"Inference microservice is not errored\")\n",
    "        break\n",
    "    if response.json().get(\"model_loaded\") == True:\n",
    "        print(\"Inference microservice is ready\")\n",
    "        break\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    time.sleep(15)\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1 Customize inference action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(response)\n",
    "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
    "inference_specs = response.json()[\"default\"][\"inference\"]\n",
    "print(json.dumps(inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to the specs dictionary if necessary\n",
    "inference_specs[\"prompt\"] = \"When does something happen in the video?\"\n",
    "inference_specs[\"media\"] = \"aws://nvcf-storage-handling/data/vlm_inference/videos/test_video.mp4\" # Format: cloud_type://bucket_name/video_file_path\n",
    "print(json.dumps(inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 Run inference using *inference-microservice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map['inference_microservice']\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/inference_microservice/inference\"\n",
    "\n",
    "response = requests.post(endpoint, json=inference_specs, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 Stop the microservice after all your inference tryouts are completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_map['inference_microservice']\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}/inference_microservice/stop\"\n",
    "\n",
    "response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Finish Experiment and Cleanup <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Delete dataset\n",
    "#### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Deployment <a class=\"anchor\" id=\"head-8\"></a>\n",
    "\n",
    "To deploy a post-trained checkpoint, refer to the [Model Deployment session in Cosmos Cookbook](https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason1/intelligent-transportation/post_training.html#model-deployment). It has instructions on deploying with NIM and NVIDIA VSS blueprint\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
