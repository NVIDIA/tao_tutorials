{
  "cells": [
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Notebook to demonstrate FTMS DINO AutoML Training with Local Storage and Airgapped Workflow\n",
        "\n",
        "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
        "\n",
        "---\n",
        "\n",
        "### FTMS AutoML with DINO (DEtection TRansformer with Improved deNoising anchOr boxes)\n",
        "\n",
        "This notebook demonstrates how to use DINO object detection model with AutoML capabilities in a local storage environment designed for airgapped workflows.\n",
        "\n",
        "[DINO](https://arxiv.org/abs/2203.03605) is a state of the art transformer-based object detection model. Similar to Deformable DETR, DINO does not use heuristics based methods like NMS or IOU assignment found in convolution-based object detection models like Faster RCNN. Compared to Deformable DETR, DINO uses de-noising during training which can help training to converge faster.\n",
        "\n",
        "\n",
        "![image](https://raw.githubusercontent.com/vpraveen-nv/model_card_images/main/api/automl_workflow.png)\n",
        "\n",
        "---\n",
        "\n",
        "### Sample prediction of a trained RT-DETR model\n",
        "\n",
        "<img align=\"center\" src=\"sample_images/detection_sample.jpg\" width=\"640\">\n",
        "\n",
        "---\n",
        "\n",
        "### Expected outcome\n",
        "The expected output of this notebook is:\n",
        "- A fully trained DINO object detection model optimized through AutoML\n",
        "- Performance metrics comparing different hyperparameter configurations\n",
        "- Exported model files ready for deployment in production environments\n",
        "- Complete workflow documentation for airgapped deployments\n",
        "\n",
        "Detailed documentation about DINO object detection is available in the TAO [documentation](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/dino.html)\n",
        "\n",
        "---\n",
        "\n",
        "### The workflow in a nutshell\n",
        "This notebook demonstrates how to train DINO object detection models using AutoML in a completely local environment suitable for airgapped deployments:\n",
        "\n",
        "1. **Setup Local Environment** - Configure local storage paths and verify prerequisites\n",
        "2. **Connect to Local TAO API** - Connect to locally deployed TAO API service\n",
        "3. **Create Local Workspace** - Setup local workspace without cloud dependencies\n",
        "4. **Prepare Local Dataset** - Load and register datasets from local storage\n",
        "5. **Configure DINO Model** - Setup DINO architecture and pretrained models\n",
        "6. **Setup AutoML** - Configure automated hyperparameter optimization\n",
        "7. **Train with AutoML** - Execute automated training with multiple trials\n",
        "8. **Model Evaluation** - Evaluate best performing model\n",
        "9. **Model Export** - Export optimized model for deployment\n",
        "10. **Local Inference** - Run inference using local model files\n"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Requirements\n",
        "Prior to running this notebook you must have:\n",
        "1. A locally deployed TAO API server [(Local Setup Guide)](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#local-deployment)\n",
        "2. Local object detection dataset in KITTI or COCO format stored in `~/data/` directory\n",
        "3. Pretrained DINO models downloaded locally at ~/airgapped_models\n",
        "4. Set the `<>` enclosed variables with values in the Configuration section\n",
        "5. Sufficient local storage space (>50GB recommended for datasets and models)\n",
        "\n",
        "### Airgapped Environment Setup\n",
        "For completely airgapped environments, ensure the following are pre-downloaded:\n",
        "- TAO API Docker images\n",
        "- DINO pretrained model weights\n",
        "- Sample datasets for testing\n"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Debugging Finetuning Microservice and Jobs\n",
        "\n",
        "When working with the TAO API, you may encounter issues at different stages. Use the following guidance to debug effectively:\n",
        "\n",
        "#### 1. Dataset, Experiment, or Workspace CRUD Operation Errors\n",
        "\n",
        "If you encounter errors related to creating, reading, updating, or deleting datasets, experiments, or workspaces **and the error messages are not clear**, check the logs of the TAO API service pods:\n",
        "\n",
        "```bash\n",
        "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
        "```\n",
        "\n",
        "#### 2. Errors During Job Launch\n",
        "\n",
        "For issues that occur **while launching a job**, check both the app and workflow pods:\n",
        "\n",
        "```bash\n",
        "kubectl logs -f <pod name starting with tao-api-app-pod>\n",
        "kubectl logs -f <pod name starting with tao-api-workflow-pod>\n",
        "```\n",
        "\n",
        "#### 3. Errors After Job Launch\n",
        "\n",
        "If errors occur **after a job has been launched**, inspect the job pod logs:\n",
        "\n",
        "```bash\n",
        "kubectl logs -f tao-api-sts-<job_id>-0\n",
        "```\n",
        "\n",
        "> **Note:**  \n",
        "> Run these `kubectl` commands on the machine where your Kubernetes service is deployed.\n",
        "\n",
        "#### Additional Debugging Tips\n",
        "\n",
        "- **Job logs are automatically uploaded** to your cloud workspace at:  \n",
        "  `/results/<job_id>/microservices_log.txt`\n",
        "\n",
        "- **You can also view logs via the Jobs API endpoint:**  \n",
        "  ```\n",
        "  /api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs\n",
        "  ```\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "| Error Type                | Where to Check Logs                                      |\n",
        "|-------------------------- |---------------------------------------------------------|\n",
        "| CRUD operation errors     | `tao-api-app-pod`                                       |\n",
        "| Job launch errors         | `tao-api-app-pod`, `tao-api-workflow-pod`               |\n",
        "| Post-launch job errors    | `tao-api-sts-<job_id>-0`                                |\n",
        "| All job logs (cloud)      | `/results/<job_id>/microservices_log.txt`               |\n",
        "| All job logs (API)        | `/api/v1/orgs/<org_name>/<experiments|datasets>/<experiment_id|dataset_id>/jobs/<job_id>/logs` |"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "---\n",
        "### Performance Benchmarks\n",
        "\n",
        "#### Execution Time Breakdown\n",
        "\n",
        "The following table shows the approximate time required for each stage of the DINO AutoML FTMS workflow:\n",
        "\n",
        "| **Stage** | **Duration** | **Description** |\n",
        "|-----------|--------------|-----------------|\n",
        "| Train Dataset Pull | **5s** | Train dataset verification and preprocessing |\n",
        "| Val/Test Dataset Pull | **5s** | Validation and test dataset verification |\n",
        "| AutoML Model Training | **1.5hrs** | 10 minutes per experiment |\n",
        "| ONNX + TensorRT Export | **3min + 7min** | Model optimization and engine generation |\n",
        "| TensorRT Inference | **20min** | High-performance inference testing |\n",
        "| **Total Time** | **9hr 45min** | **Complete end-to-end workflow** |\n",
        "\n",
        "#### Test Environment Specifications\n",
        "\n",
        "| **Component** | **Specification** |\n",
        "|---------------|-------------------|\n",
        "| **GPU** | 1x NVIDIA A40 |\n",
        "| **Training Dataset** | 50 images (5 MB total) |\n",
        "| **Validation Dataset** | 50 images (5 MB total) |\n",
        "| **Training Epochs** | 10 epochs per experiment |\n",
        "| **Model Architecture** | Dino with Fan Tiny |\n",
        "| **Task** | Warehouse object detection (4 classes) |\n",
        "\n",
        "#### Performance Factors\n",
        "\n",
        "> **Important Note:**  \n",
        "> Actual execution times may vary significantly based on:\n",
        "> \n",
        "> - **Hardware Configuration**: GPU type, memory, and compute capability\n",
        "> - **Storage Performance**: Local disk I/O speed and cloud storage latency\n",
        "> - **Network Conditions**: Bandwidth and latency to cloud storage\n",
        "> - **System Load**: Other concurrent processes and resource utilization\n",
        "> - **Dataset Size**: Number of images and total data volume\n",
        "> - **Batch Size**: Larger batch sizes can improve training stability and speed\n",
        "> - **Model Configuration**: Backbone architecture, epochs, AutoML configurations, and hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Transfer Datasets and Pre-trained models into local S3 bucket"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Local Dataset Structure\n",
        "To see the dataset folder structure required for DINO object detection in local storage, ensure your data follows this pattern:\n",
        "\n",
        "```\n",
        "~/data/dino_train_dataset/\n",
        "├── images.tar.gz\n",
        "├── annotations.json\n",
        "├── label_map.txt\n",
        "\n",
        "~/data/dino_val_dataset/\n",
        "├── images.tar.gz\n",
        "├── annotations.json\n",
        "├── label_map.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### Setup AWS credentials as environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# For minikube environment, uncomment this section until before docker compose section\n",
        "# import subprocess\n",
        "\n",
        "# # Get the Minikube IP using subprocess\n",
        "# minikube_ip = subprocess.check_output([\"minikube\", \"ip\"]).decode(\"utf-8\").strip()\n",
        "\n",
        "# # Set environment variables\n",
        "# os.environ[\"CLUSTER_IP\"] = minikube_ip\n",
        "# os.environ[\"SEAWEED_ENDPOINT\"] = f\"http://{os.environ[\"CLUSTER_IP\"]}:32333\"\n",
        "\n",
        "# For docker compose environment, uncomment this section\n",
        "# %env CLUSTER_IP=<MACHINE IP>\n",
        "# %env SEAWEED_ENDPOINT=http://$CLUSTER_IP:8333\n",
        "# os.environ[\"SEAWEED_ENDPOINT\"] = f\"http://{os.environ[\"CLUSTER_IP\"]}:8333\"\n",
        "\n",
        "\n",
        "# Common for both\n",
        "# Set AWS CLI credentials for SeaweedFS\n",
        "%env AWS_ACCESS_KEY_ID=seaweedfs\n",
        "%env AWS_SECRET_ACCESS_KEY=seaweedfs123\n",
        "%env AWS_DEFAULT_REGION=us-east-1\n"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### Create a bucket if not present already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install awscli\n",
        "\n",
        "# Create the main storage bucket if not already created\n",
        "if ! aws s3 ls --endpoint-url \"$SEAWEED_ENDPOINT\" | grep -q \"tao-storage\"; then\n",
        "    aws s3 mb --endpoint-url \"$SEAWEED_ENDPOINT\" s3://tao-storage\n",
        "else\n",
        "    echo \"Bucket already exists, skipping creation.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### Copy data from local disk to local S3 bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "aws s3 cp --endpoint-url $SEAWEED_ENDPOINT ~/data/dino_train_dataset s3://tao-storage/data/dino_train_dataset --recursive\n",
        "aws s3 cp --endpoint-url $SEAWEED_ENDPOINT ~/data/dino_val_dataset s3://tao-storage/data/dino_val_dataset --recursive\n",
        "\n",
        "# This directory that is being used on S3 to copied to is the one that should be present on LOCAL_MODEL_REGISTRY in values.yaml\n",
        "aws s3 cp --endpoint-url $SEAWEED_ENDPOINT ~/airgapped-models/ s3://tao-storage/shared-storage/models/ --recursive"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### Verify if the uploads were successfull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "aws s3 ls --endpoint-url $SEAWEED_ENDPOINT s3://tao-storage/shared-storage/models/\n",
        "aws s3 ls --endpoint-url $SEAWEED_ENDPOINT s3://tao-storage/data/\n"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Configuration \n",
        "\n",
        "Fill in all `<>` enclosed variables with relevant values under this section. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import time\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### TAO FTMS Host & Credentials \n",
        "**Kubernetes Environment**\n",
        "\n",
        "* **IP Address**: Machine’s IP address\n",
        "* **Port**: `32080`\n",
        "\n",
        "**Docker Compose Environment**\n",
        "\n",
        "* **IP Address**: `localhost`\n",
        "* **Port**: Value set in `config.env` (`8090` by default)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Configure for your TAO FTMS server \n",
        "ngc_key = \"<NGC_PERSONAL_KEY>\"\n",
        "ngc_org_name = \"ea-tlt\"\n",
        "host_url = f\"http://<ip_address>:<port_number>/api/v1/orgs/{ngc_org_name}\"\n",
        "\n",
        "docker_env_vars = {}\n",
        "\n",
        "# If you're using a PTM from a private organization like NVAIE, uncomment the following line and add your legacy NGC API Key.\n",
        "# docker_env_vars['TAO_API_KEY'] = '<NGC_LEGACY_API_KEY>' #Set to NGC Legacy API Key "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Cloud Storage Setup & Credentials "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Cloud bucket details to access datasets and store experiment results\n",
        "cloud_metadata = {\n",
        "    \"name\": \"tao_workspace\",\n",
        "    \"cloud_type\": \"seaweedfs\",\n",
        "    \"cloud_specific_details\": {\n",
        "        \"cloud_region\": \"us-east-1\",\n",
        "        \"cloud_bucket_name\": \"tao-storage\",\n",
        "        \"access_key\": \"seaweedfs\",\n",
        "        \"secret_key\": \"seaweedfs123\",\n",
        "        \"endpoint_url\": \"http://seaweedfs-s3:8333\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Dataset Paths in Cloud Storage "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset_path =  \"/data/dino_train_dataset\"\n",
        "eval_dataset_path = \"/data/dino_val_dataset\""
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# DINO Model Configuration\n",
        "# Documentation: https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/dino.html\n",
        "model_name = \"dino\"  # Fixed for this tutorial"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### Configure AutoML Parameters\n",
        "[AutoML documentation](https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#getting-started)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# AutoML Configuration\n",
        "automl_algorithm = \"bayesian\"\n",
        "automl_max_recommendations = 2  # Number of AutoML experiments to run"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Create cloud workspace\n",
        "This workspace will be the place where your datasets reside and results of TAO FTMS jobs will be pushed to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Create cloud workspace\n",
        "data = json.dumps(cloud_metadata)\n",
        "\n",
        "endpoint = f\"{host_url}/workspaces\"\n",
        "\n",
        "response = requests.post(endpoint,data=data)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"id\" in response.json().keys()\n",
        "\n",
        "workspace_id = response.json()[\"id\"]"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Load Base experiments from local storage into your FTMS Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Local workspace configuration\n",
        "# 3. Load base experiments into DB from below endpoint\n",
        "endpoint = f\"{host_url}/experiments:load_airgapped\"\n",
        "data = {\n",
        "    \"workspace_id\": workspace_id,\n",
        "}\n",
        "response = requests.post(endpoint, json=data)\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "assert response.status_code in (200, 201)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Register datasets with FTMS "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "TAO FTMS requires datasets in your cloud storage to be registered to produce a unique ID that can be attached to training jobs. This step only needs to be done once and then you can use the dataset across any experiments that support the dataset format."
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### List Registered Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = f\"{host_url}/datasets\"\n",
        "\n",
        "response = requests.get(endpoint)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "datasets = response.json()[\"datasets\"]\n",
        "for rsp in datasets:\n",
        "    rsp_keys = rsp.keys()\n",
        "    assert \"id\" in rsp_keys\n",
        "    assert \"type\" in rsp_keys\n",
        "    assert \"format\" in rsp_keys\n",
        "    assert \"name\" in rsp_keys\n",
        "\n",
        "print(response)\n",
        "# print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose list output\n",
        "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
        "for rsp in datasets:\n",
        "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "If you have already registered your datasets, then you can directly set their IDs in the following cell to avoid creating duplicate datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset_id = None \n",
        "eval_dataset_id = None \n",
        "test_dataset_id = None "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Train Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Create train dataset\n",
        "if train_dataset_id is None: \n",
        "    train_dataset_metadata = {\"type\": \"object_detection\",\n",
        "                              \"format\": \"coco\",\n",
        "                              \"workspace\":workspace_id,\n",
        "                              \"cloud_file_path\": train_dataset_path,\n",
        "                              \"use_for\": [\"training\"],\n",
        "                              \"name\": \"hardhat_detection_train\"\n",
        "                              }\n",
        "    data = json.dumps(train_dataset_metadata)\n",
        "    \n",
        "    endpoint = f\"{host_url}/datasets\"\n",
        "    \n",
        "    response = requests.post(endpoint,data=data)\n",
        "    assert response.status_code in (200, 201)\n",
        "    assert \"id\" in response.json().keys()\n",
        "    \n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    train_dataset_id = response.json()[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Check progress\n",
        "endpoint = f\"{host_url}/datasets/{train_dataset_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    assert response.status_code in (200, 201)\n",
        "\n",
        "    if response.json().get(\"status\") == \"invalid_pull\":\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "    if response.json().get(\"status\") == \"pull_complete\":\n",
        "        break\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Create eval dataset\n",
        "if eval_dataset_id is None: \n",
        "    eval_dataset_metadata = {\"type\": \"object_detection\",\n",
        "                             \"format\": \"coco\",\n",
        "                             \"workspace\":workspace_id,\n",
        "                             \"cloud_file_path\": eval_dataset_path,\n",
        "                             \"use_for\": [\"evaluation\"],\n",
        "                             \"name\" : \"hardhat_detection_val\" \n",
        "                             }\n",
        "    data = json.dumps(eval_dataset_metadata)\n",
        "    \n",
        "    endpoint = f\"{host_url}/datasets\"\n",
        "    \n",
        "    response = requests.post(endpoint,data=data)\n",
        "    assert response.status_code in (200, 201)\n",
        "    assert \"id\" in response.json().keys()\n",
        "    \n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    eval_dataset_id = response.json()[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Check progress\n",
        "endpoint = f\"{host_url}/datasets/{eval_dataset_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    assert response.status_code in (200, 201)\n",
        "\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    if response.json().get(\"status\") == \"invalid_pull\":\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "    if response.json().get(\"status\") == \"pull_complete\":\n",
        "        break\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Create Experiment "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "Before we can run any jobs such as training, evaluation, or inference, we must create an experiment to setup the network architecture and associated datsets. Then we can chain several jobs together to create our trained models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_choose_method = \"best_model\"\n",
        "data = json.dumps({\"network_arch\":model_name,\n",
        "                   \"checkpoint_choose_method\":checkpoint_choose_method,\n",
        "                   \"workspace\": workspace_id,\n",
        "                   \"train_datasets\":[train_dataset_id],\n",
        "                   \"eval_dataset\":eval_dataset_id,\n",
        "                   \"inference_dataset\":eval_dataset_id,\n",
        "                   \"calibration_dataset\":train_dataset_id})\n",
        "\n",
        "endpoint = f\"{host_url}/experiments\"\n",
        "\n",
        "response = requests.post(endpoint,data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"id\" in response.json()\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "experiment_id = response.json()[\"id\"]"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "When a job is submitted, we will receive a unique ID to reference back to it. We will store these IDs in the following ```job_map``` variable. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "job_map = {}"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-14\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Get default spec schema\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/specs/train/schema\"\n",
        "while True:\n",
        "    response = requests.get(endpoint)\n",
        "    if response.status_code == 404:\n",
        "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
        "            print(\"Base experiment spec file is being downloaded\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"automl_default_parameters\" in response.json().keys()\n",
        "automl_params = response.json()[\"automl_default_parameters\"]\n",
        "print(json.dumps(automl_params, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "#### Update the experiment with automl parameters to run experiments on <a class=\"anchor\" id=\"head-14\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "automl_information = {\n",
        "    \"automl_enabled\": True,\n",
        "    \"automl_algorithm\": automl_algorithm,\n",
        "    \"automl_max_recommendations\": automl_max_recommendations,\n",
        "    \"automl_hyperparameters\": str(automl_params)\n",
        "}\n",
        "data = json.dumps({\"metric\":\"kpi\", \"automl_settings\": automl_information})\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}\"\n",
        "\n",
        "response = requests.patch(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Assign Pretrained Model\n",
        "\n",
        "To help bootstrap the model, we can start the model with pre-trained weights that have already seen a large number of images. This will help reduce the data and time required to finetune your model. Several pretrained models are available from NGC. TAO FTMS will automatically pull PTMs available to use. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# List all pretrained models for the chosen network architecture\n",
        "endpoint = f\"{host_url}/experiments:base\"\n",
        "params = {\"network_arch\": model_name}\n",
        "response = requests.get(endpoint, params=params)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "response_json = response.json()[\"experiments\"]\n",
        "\n",
        "for rsp in response_json:\n",
        "    rsp_keys = rsp.keys()\n",
        "    if \"encryption_key\" not in rsp.keys():\n",
        "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys\n",
        "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_map = {\"dino\" : \"pretrained_dino_imagenet:fan_hybrid_small\"}   \n",
        "endpoint = f\"{host_url}/experiments:base\"\n",
        "params = {\"network_arch\": model_name}\n",
        "response = requests.get(endpoint, params=params)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "response_json = response.json()[\"experiments\"]\n",
        "\n",
        "# Search for ptm with given ngc path\n",
        "ptm = []\n",
        "for rsp in response_json:\n",
        "    rsp_keys = rsp.keys()\n",
        "    assert \"ngc_path\" in rsp_keys\n",
        "    if rsp[\"ngc_path\"].endswith(pretrained_map[model_name]):\n",
        "        assert \"id\" in rsp_keys\n",
        "        ptm_id = rsp[\"id\"]\n",
        "        ptm = [ptm_id]\n",
        "        print(\"Metadata for model with requested NGC Path\")\n",
        "        print(rsp)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "ptm_information = {\"base_experiment\":ptm}\n",
        "data = json.dumps(ptm_information)\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}\"\n",
        "\n",
        "response = requests.patch(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Start AutoML"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Retrieve default training spec "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "Before launching a training a job, we can retrieve the default training spec for our network architecture to use as a starting point to configure the training parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Get default spec schema\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/specs/train/schema\"\n",
        "\n",
        "while True:\n",
        "    response = requests.get(endpoint)\n",
        "    if response.status_code == 404:\n",
        "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
        "            print(\"Base experiment spec file is being downloaded\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"default\" in response.json().keys()\n",
        "\n",
        "print(response)\n",
        "# full_schema = response.json()\n",
        "# print(json.dumps(full_schema, indent=4)) ## Uncomment for verbose schema\n",
        "specs = response.json()[\"default\"]\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Configure Training Spec \n",
        "\n",
        "Now we can customize the json spec object and set our dataset, training and model parameters. No changes are needed unless you are using a custom dataset or model architecture.\n",
        "\n",
        "#### Understanding the Spec Schema Structure\n",
        "\n",
        "Before customizing the training spec, it's helpful to understand the available configuration options and their valid values. The `get_bounds_of_field` utility function helps you explore the schema structure and find valid parameter values if they are defined on the backend (for some fields it might not be defined).\n",
        "\n",
        "**How to use the schema exploration utility:**\n",
        "\n",
        "```python\n",
        "from get_bounds import get_bounds_of_field\n",
        "\n",
        "# Example: Get valid values for backbone type\n",
        "# Note the conversion from dictionary access notation to list notation:\n",
        "# From: [\"model\"][\"backbone\"]\n",
        "# To:   [\"model\", \"backbone\"]\n",
        "print(get_bounds_of_field(full_schema, [\"model\", \"backbone\"]))\n",
        "```\n",
        "\n",
        "**Key differences in notation:**\n",
        "- **Dictionary access format**: `[\"model\"][\"backbone\"]` - This is how you would access nested values in Python\n",
        "- **Schema path format**: `[\"model\", \"backbone\"]` - This is the format expected by the `get_bounds_of_field` function\n",
        "\n",
        "---\n",
        "\n",
        "#### Schema Navigation Tips\n",
        "\n",
        "1. **Nested Structure**: The schema follows a hierarchical structure where each level represents a configuration category\n",
        "2. **Path Format**: Always use a list of strings `[\"level1\", \"level2\", \"level3\"]` when calling `get_bounds_of_field`\n",
        "3. **Validation**: This helps ensure you're using valid parameter values before submitting your training job\n",
        "4. **Documentation**: Use these explorations to understand what options are available for your specific model architecture\n",
        "\n",
        "\n",
        "No changes to the default spec are needed unless you are using a custom dataset format or want to modify the model architecture and training parameters.\n",
        "For more details about the configurable parameters, refer to the documentation on [RT-DETR](https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/object_detection/rt_detr.html#creating-an-experiment-spec-file) in TAO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Customize train model specs\n",
        "specs[\"train\"][\"num_gpus\"] = 1\n",
        "specs[\"dataset\"][\"num_classes\"] = 5\n",
        "\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Submit Training Job  "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "With our training spec configured, we can now submit a training job. All jobs follow the same flow of retreiving the default spec, customizing it then submitting the job. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Run action\n",
        "action = \"train\"\n",
        "data = json.dumps({\"parent_job_id\":None,\"action\":action,\"specs\":specs,\n",
        "                   })\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs\"\n",
        "\n",
        "response = requests.post(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "assert response.json()\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "job_map[\"train_\" + model_name] = response.json()\n",
        "print(job_map)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "After submitting the training job, an ID is returned that we can use to monitor the job progress. The following cell will continuously print the latest status until the job is complete. This notebook will track all of the job IDs in the ```job_map``` variable. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "job_id = job_map[\"train_\" + model_name]\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
        "        print(\"Job is being created\")\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
        "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
        "        break\n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "If you need to cancel the job for any reason, you can uncomment and run the following cell. You can also configure the endpoint to end with ```:pause``` or ```:resume``` instead of ```:cancel``` to temporarily stop and start the job. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# job_id = job_map[\"train_\" + model_name]\n",
        "# endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
        "\n",
        "# response = requests.post(endpoint)\n",
        "# print(response)\n",
        "# print(json.dumps(response.json(), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "If the job runs into any errors or if you want to check the job logs, you can uncomment and run the following cell to view the job logs. Alternatively, you can view the job logs in your cloud workspace under the path /results/<job_id>/microservices_log.txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# job_id = job_map[\"train_\" + model_name]\n",
        "# endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}/logs\"\n",
        "\n",
        "# response = requests.get(endpoint)\n",
        "# print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Evaluate AutoML Model "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "Once our AutoML model has been trained, we can evaluate it on the test dataset to get detection KPIs. "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Receive default evaluation spec "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Get default spec schema\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
        "\n",
        "while True:\n",
        "    response = requests.get(endpoint)\n",
        "    if response.status_code == 404:\n",
        "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
        "            print(\"Base experiment spec file is being downloaded\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"default\" in response.json().keys()\n",
        "\n",
        "print(response)\n",
        "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
        "specs = response.json()[\"default\"]\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Customize Evaluation Spec "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "specs[\"dataset\"][\"num_classes\"] = 5\n",
        "\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Submit Evaluation Job "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "Note that for this job we will set the ```parent_job_id``` parameter in the body of the request to the completed training job. This is required to pass the trained model from the training job into our evaluation job. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Run action\n",
        "parent = job_map[\"train_\" + model_name]\n",
        "action = \"evaluate\"\n",
        "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
        "                   })\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs\"\n",
        "\n",
        "response = requests.post(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "job_map[\"evaluate_\" + model_name] = response.json()\n",
        "print(job_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status by running this cell\n",
        "job_id = job_map[\"evaluate_\" + model_name]\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    assert response.status_code in (200, 201)\n",
        "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
        "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
        "        break\n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## TRT Engine Generation "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "Now that we have a automl trained model, it can be exported to ONNX format then turned into an optimzed TensorRT engine for deployment. "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Export Model to ONNX "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Get default spec schema\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/specs/export/schema\"\n",
        "\n",
        "while True:\n",
        "    response = requests.get(endpoint)\n",
        "    if response.status_code == 404:\n",
        "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
        "            print(\"Base experiment spec file is being downloaded\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"default\" in response.json().keys()\n",
        "\n",
        "print(response)\n",
        "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
        "specs = response.json()[\"default\"]\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "specs[\"dataset\"][\"num_classes\"] = 5\n",
        "\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Run action\n",
        "parent = job_map[\"train_\" + model_name] #parent is trained model\n",
        "action = \"export\"\n",
        "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
        "                   })\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs\"\n",
        "\n",
        "response = requests.post(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "job_map[\"export_\" + model_name] = response.json()\n",
        "print(job_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status by repeatedly running this cell\n",
        "job_id = job_map[\"export_\" + model_name]\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    assert response.status_code in (200, 201)\n",
        "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
        "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
        "        break\n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Convert ONNX to TRT Engine "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Get default spec schema\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/specs/gen_trt_engine/schema\"\n",
        "\n",
        "while True:\n",
        "    response = requests.get(endpoint)\n",
        "    if response.status_code == 404:\n",
        "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
        "            print(\"Base experiment spec file is being downloaded\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"default\" in response.json().keys()\n",
        "\n",
        "print(response)\n",
        "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
        "specs = response.json()[\"default\"]\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "specs[\"dataset\"][\"num_classes\"] = 5\n",
        "specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"FP16\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Run action\n",
        "parent = job_map[\"export_\" + model_name]\n",
        "action = \"gen_trt_engine\"\n",
        "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
        "                   })\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs\"\n",
        "\n",
        "response = requests.post(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "job_map[\"model_gen_trt_engine_\" + model_name] = response.json()\n",
        "print(job_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status by repeatedly running this cell\n",
        "job_id = job_map['model_gen_trt_engine_' + model_name]\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    assert response.status_code in (200, 201)\n",
        "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
        "        break\n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Inference TRT Engine "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "Finally we can use our optimized automl model to inference on our test set and receive the annotated results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Get default spec schema\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
        "\n",
        "while True:\n",
        "    response = requests.get(endpoint)\n",
        "    if response.status_code == 404:\n",
        "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
        "            print(\"Base experiment spec file is being downloaded\")\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        break\n",
        "assert response.status_code in (200, 201)\n",
        "assert \"default\" in response.json().keys()\n",
        "\n",
        "print(response)\n",
        "#print(json.dumps(response.json(), indent=4)) ## Uncomment for verbose schema\n",
        "specs = response.json()[\"default\"]\n",
        "print(json.dumps(specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "specs[\"dataset\"][\"num_classes\"] = 5\n",
        "specs[\"dataset\"][\"batch_size\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Run action\n",
        "parent = job_map[\"model_gen_trt_engine_\" + model_name]\n",
        "action = \"inference\"\n",
        "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":specs,\n",
        "                   })\n",
        "\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs\"\n",
        "\n",
        "response = requests.post(endpoint, data=data)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "job_map[\"inference_trt_\" + model_name] = response.json()\n",
        "print(job_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job status by repeatedly running this cell\n",
        "job_id = job_map[\"inference_trt_\" + model_name]\n",
        "endpoint = f\"{host_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    response = requests.get(endpoint)\n",
        "    print(response)\n",
        "    print(json.dumps(response.json(), indent=4))\n",
        "    assert response.status_code in (200, 201)\n",
        "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
        "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
        "        break\n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "## Clean Up "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "You can optionally run this section to delete the datasets and experiment results. "
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Delete experiment <a class=\"anchor\" id=\"head-23\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = f\"{host_url}/experiments/{experiment_id}\"\n",
        "\n",
        "response = requests.delete(endpoint)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "assert response.status_code in (200, 201)"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Delete train dataset <a class=\"anchor\" id=\"head-24\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = f\"{host_url}/datasets/{train_dataset_id}\"\n",
        "\n",
        "response = requests.delete(endpoint)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
   "metadata": {},
      "source": [
        "### Delete val dataset <a class=\"anchor\" id=\"head-24\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
   "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = f\"{host_url}/datasets/{eval_dataset_id}\"\n",
        "\n",
        "response = requests.delete(endpoint)\n",
        "assert response.status_code in (200, 201)\n",
        "\n",
        "print(response)\n",
        "print(json.dumps(response.json(), indent=4))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
