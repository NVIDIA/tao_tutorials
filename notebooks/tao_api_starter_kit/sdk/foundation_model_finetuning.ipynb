{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Notebook to demonstrate Self-supervised Finetuning Workflow\n",
        "\n",
        "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
        "\n",
        "![image](https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
        "\n",
        "### TAO Architecture\n",
        "\n",
        "This notebook uses the **TAO SDK** for all API interactions. Key architectural features:\n",
        "\n",
        "\n",
        "- **Job-Centric Architecture**: Operations are organized around jobs\n",
        "- **SDK-Based**: Takes advantage of `TaoClient` SDK for faster development\n",
        "- **Persistent Credentials**: Authentication uses `TAO_BASE_URL`, `TAO_ORG`, `TAO_TOKEN` environment variables or same in ~/.tao/config\n",
        "\n",
        "### The workflow in a nutshell\n",
        "\n",
        "- Set up TAO SDK authentication\n",
        "- Pull datasets from cloud storage\n",
        "- Select Pre-trained Model (PTM) from NGC\n",
        "- **Job Actions**:\n",
        "    - Train (Normal/AutoML)\n",
        "    - Export\n",
        "    - Inference\n",
        "    - Delete jobs/datasets\n",
        "\n",
        "### Table of contents\n",
        "\n",
        "1. [FIXME's](#head-1)\n",
        "1. [SDK Authentication](#head-2)\n",
        "1. [Create a cloud workspace](#head-2)\n",
        "1. [Set dataset formats](#head-3)\n",
        "1. [Create and pull train dataset](#head-4)\n",
        "1. [Create and pull test dataset](#head-5)\n",
        "1. [List the created datasets](#head-6)\n",
        "1. [List base experiments (PTMs)](#head-10)\n",
        "1. [Assign datasets](#head-11)\n",
        "1. [Assign PTM](#head-12)\n",
        "1. [Prepare AutoML Configuration](#head-13)\n",
        "1. [Create Training Job](#head-14)\n",
        "1. [Monitor Job Status](#head-15)\n",
        "1. [Create Export Job](#head-18)\n",
        "1. [Create Inference Job](#head-20)\n",
        "1. [Delete jobs](#head-22)\n",
        "1. [Delete datasets](#head-23)\n",
        "\n",
        "### Requirements\n",
        "- TAO API Fine-Tuning Microservce (FTMS) deployment\n",
        "- TAO SDK: `pip install nvidia-tao-client`\n",
        "- NGC API key and organization access\n",
        "- Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Import TAO SDK\n",
        "from tao_sdk.client import TaoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore variable in case of jupyter session restart and resume execution where it left off\n",
        "%store -r model_name\n",
        "%store -r automl_enabled\n",
        "%store -r automl_algorithm\n",
        "%store -r tao_client\n",
        "%store -r workspace_id\n",
        "%store -r train_dataset_id\n",
        "%store -r test_dataset_id\n",
        "%store -r job_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To see the dataset folder structure required for the models supported in this notebook, visit the notebooks under dataset_prepare like for [this notebook](../dataset_prepare/foundational_model_finetuning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FIXME's <a class=\"anchor\" id=\"head-1\"></a>\n",
        "\n",
        "1. Assign a model_name in FIXME 1\n",
        "1. (Optional) Enable AutoML if needed in FIXME 2\n",
        "1. (Optional) Choose between bayesian and hyperband automl_algorithm in FIXME 3 (If automl was enabled in FIXME2)\n",
        "1. Set environment variables for TAO API in FIXME 4 (TAO_BASE_URL, TAO_ORG, TAO_TOKEN)\n",
        "1. Assign the ngc_key variable in FIXME 5 (for login)\n",
        "1. Assign the ngc_org_name variable in FIXME 6 (for login)\n",
        "1. Set cloud storage details in FIXME 7\n",
        "1. Assign path of datasets relative to the bucket in FIXME 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Choose a SSL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# FIXME 1: Define model_name workspaces and other variables\n",
        "# Available models:\n",
        "# 1. nvdinov2 - https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/nvdinov2.html\n",
        "\n",
        "os.environ[\"TAO_MODEL_NAME\"] = model_name = os.environ.get(\"TAO_MODEL_NAME\", \"nvdinov2\")  # Pick the model name from the above mentioned list\n",
        "%store model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Toggle AutoML params\n",
        "[AutoML documentation](https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#getting-started)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# FIXME 2: Set to True if you want to run automl for the model chosen in the previous cell\n",
        "automl_enabled = os.environ.get(\"TAO_AUTOML_ENABLED\", \"False\").lower() == \"true\"\n",
        "os.environ[\"TAO_AUTOML_ENABLED\"] = str(automl_enabled)\n",
        "# One of bayesian/hyperband\n",
        "os.environ[\"TAO_AUTOML_ALGORITHM\"] = automl_algorithm = os.environ.get(\"TAO_AUTOML_ALGORITHM\", \"bayesian\")\n",
        "\n",
        "%store automl_enabled\n",
        "%store automl_algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set TAO API Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 4: Set TAO API environment variables\n",
        "\n",
        "# Set to your TAO API endpoint\n",
        "os.environ[\"TAO_BASE_URL\"] = os.environ.get(\"TAO_BASE_URL\", \"https://your_tao_ip_address:port/api/v2\")\n",
        "\n",
        "# These will be set after login\n",
        "os.environ[\"TAO_ORG\"] = os.environ.get(\"TAO_ORG\", \"your_ngc_org_name\")\n",
        "os.environ[\"TAO_TOKEN\"] = os.environ.get(\"TAO_TOKEN\", \"your_token\" )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set NGC Personal key for authentication and NGC org to access API services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 5: Your NGC personal key\n",
        "os.environ[\"NGC_KEY\"] = ngc_key = os.environ.get(\"NGC_KEY\", \"your_ngc_key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 6: Your NGC ORG name\n",
        "os.environ[\"NGC_ORG\"] = ngc_org_name = os.environ.get(\"NGC_ORG\", \"nvstaging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Login <a class=\"anchor\" id=\"head-2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TAO Client and login using SDK\n",
        "tao_client = TaoClient()\n",
        "\n",
        "# Login using TAO SDK - this will automatically save credentials to environment variables\n",
        "login_response = tao_client.login(\n",
        "    ngc_key=ngc_key,\n",
        "    ngc_org_name=ngc_org_name,\n",
        "    enable_telemetry=True\n",
        ")\n",
        "\n",
        "print(\" Login successful!\")\n",
        "print(\"JWT Token:\", tao_client.token)\n",
        "print(\"API Base URL:\", tao_client.base_url)\n",
        "print(\"Organization:\", tao_client.org_name)\n",
        "\n",
        "%store tao_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get GPU details <a class=\"anchor\" id=\"head-2\"></a>\n",
        "\n",
        " One of the keys of the response json are to be used as platform_id when you run each job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Get available GPU types using TAO SDK\n",
        "# try:\n",
        "#     gpu_types = tao_client.get_gpu_types()\n",
        "#     print(\" Available GPU types:\")\n",
        "#     print(json.dumps(gpu_types, indent=4))\n",
        "# except Exception as e:\n",
        "#     print(\" Could not fetch GPU types (may not be available on this deployment):\", str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create cloud workspace\n",
        "This workspace will be the place where your datasets reside and your results of TAO API jobs will be pushed to.\n",
        "\n",
        "If you want to have different workspaces for dataset and experiment, duplocate the workspace creation part and adjust the metadata accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 7: Dataset Cloud bucket details to download dataset or push job artifacts for jobs\n",
        "\n",
        "cloud_metadata = {}\n",
        "\n",
        "# A Representative name for this cloud info\n",
        "os.environ[\"TAO_WORKSPACE_NAME\"] = cloud_metadata[\"name\"] = os.environ.get(\"TAO_WORKSPACE_NAME\", \"AWS workspace info\")\n",
        "\n",
        "# Cloud specific details. Below is assuming AWS.\n",
        "cloud_metadata[\"cloud_specific_details\"] = {}\n",
        "\n",
        " # Whether it is AWS, HuggingFace or Azure\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_TYPE\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_TYPE\", \"aws\")\n",
        "\n",
        "# Bucket region\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_REGION\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_REGION\", \"us-west-1\")\n",
        "\n",
        "# Bucket name\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_BUCKET_NAME\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_BUCKET_NAME\", \"bucket_name\")\n",
        "\n",
        "# Access and Secret keys\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_ACCESS_KEY\"] = cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_ACCESS_KEY\", \"access_key\")\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_SECRET_KEY\"] = cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_SECRET_KEY\", \"secret_key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cloud workspace using TAO SDK\n",
        "workspace_id = tao_client.create_workspace(\n",
        "    name=cloud_metadata[\"name\"],\n",
        "    cloud_type=cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"],\n",
        "    cloud_specific_details=cloud_metadata[\"cloud_specific_details\"]\n",
        ")\n",
        "\n",
        "print(\"Workspace created successfully!\")\n",
        "print(f\"Workspace ID: {workspace_id}\")\n",
        "\n",
        "# Get workspace details to confirm creation\n",
        "workspace_details = tao_client.get_workspace_metadata(workspace_id)\n",
        "print(\"Workspace details:\")\n",
        "print(json.dumps(workspace_details, indent=4))\n",
        "\n",
        "%store workspace_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Restore Database from backup file\n",
        "#### This is optional and can be skipped if you don't want to restore the database from backup file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Restore workspace from backup using TAO SDK\n",
        "# try:    \n",
        "#     # Construct workspace metadata that matches WorkspaceReq schema requirements\n",
        "#     restore_workspace_metadata = {\n",
        "#         \"cloud_type\": cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"],\n",
        "#         \"cloud_specific_details\": cloud_metadata[\"cloud_specific_details\"]\n",
        "#     }\n",
        "        \n",
        "#     restore_result = tao_client.restore_workspace(\n",
        "#         backup_file_name=\"mongodb_backup.gz\",\n",
        "#         workspace_metadata=restore_workspace_metadata\n",
        "#     )\n",
        "#     print(\" Workspace restored successfully!\")\n",
        "#     print(json.dumps(restore_result, indent=4))\n",
        "# except Exception as e:\n",
        "#     print(\" Workspace restore failed:\", str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set dataset path (path within cloud bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 8 : Set paths relative to cloud bucket\n",
        "os.environ[\"TAO_TRAIN_DATASET_PATH\"] = train_dataset_path =  os.environ.get(\"TAO_TRAIN_DATASET_PATH\", f\"/data/classification_train\")\n",
        "os.environ[\"TAO_TEST_DATASET_PATH\"] = test_dataset_path = os.environ.get(\"TAO_TEST_DATASET_PATH\", f\"/data/classification_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set dataset formats <a class=\"anchor\" id=\"head-3\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create train dataset\n",
        "ds_type = \"image_classification\"\n",
        "ds_format = \"ssl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and pull train dataset <a class=\"anchor\" id=\"head-4\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create train dataset using TAO SDK\n",
        "train_dataset_id = tao_client.create_dataset(\n",
        "    dataset_type=ds_type,\n",
        "    dataset_format=ds_format,\n",
        "    workspace_id=workspace_id,\n",
        "    cloud_file_path=train_dataset_path,\n",
        "    use_for=[\"training\"]\n",
        ")\n",
        "\n",
        "print(\" Train dataset created successfully!\")\n",
        "print(f\"Train Dataset ID: {train_dataset_id}\")\n",
        "\n",
        "%store train_dataset_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check train dataset progress using TAO SDK\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    dataset_details = tao_client.get_dataset_metadata(train_dataset_id)\n",
        "    \n",
        "    print(f\" Train Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
        "    print(f\"Dataset ID: {train_dataset_id}\")\n",
        "    \n",
        "    if dataset_details.get(\"status\") == \"invalid_pull\":\n",
        "        print(\"Dataset pull failed!\")\n",
        "        validation_details = dataset_details.get(\"validation_details\", {})\n",
        "        if validation_details:\n",
        "            print(\"Validation details:\")\n",
        "            print(json.dumps(validation_details, indent=4))\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "        \n",
        "    if dataset_details.get(\"status\") == \"pull_complete\":\n",
        "        print(\"Train dataset pull completed successfully!\")\n",
        "        print(\"Dataset details:\")\n",
        "        print(json.dumps(dataset_details, indent=4))\n",
        "        break\n",
        "        \n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Update dataset path if needed using TAO SDK\n",
        "# # Note: Dataset updates may require re-creation depending on the implementation\n",
        "# try:\n",
        "#     updated_dataset = tao_client.patch_dataset_metadata(train_dataset_id, {\"cloud_file_path\": \"/data/classification_train_1\"})\n",
        "#     print(\" Dataset updated successfully!\")\n",
        "#     print(json.dumps(updated_dataset, indent=4))\n",
        "# except Exception as e:\n",
        "#     print(\" Dataset update not supported or failed:\", str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Check dataset progress using TAO SDK (alternative approach)\n",
        "# # This is an alternative way to monitor dataset status if needed\n",
        "\n",
        "# while True:\n",
        "#     clear_output(wait=True)\n",
        "#     \n",
        "#     try:\n",
        "#         dataset_details = tao_client.get_dataset_metadata(train_dataset_id)\n",
        "#         \n",
        "#         print(f\" Train Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
        "#         print(f\"Dataset ID: {train_dataset_id}\")\n",
        "#         print(\"Full dataset details:\")\n",
        "#         print(json.dumps(dataset_details, indent=4))\n",
        "#         \n",
        "#         if dataset_details.get(\"status\") == \"invalid_pull\":\n",
        "#             print(\" Dataset pull failed!\")\n",
        "#             validation_details = dataset_details.get(\"validation_details\", {})\n",
        "#             if validation_details:\n",
        "#                 print(\"Validation details:\")\n",
        "#                 print(json.dumps(validation_details, indent=4))\n",
        "#             raise ValueError(\"Dataset pull failed\")\n",
        "#             \n",
        "#         if dataset_details.get(\"status\") == \"pull_complete\":\n",
        "#             print(\" Dataset pull completed successfully!\")\n",
        "#             break\n",
        "#             \n",
        "#     except Exception as e:\n",
        "#         print(f\" Error fetching dataset status: {str(e)}\")\n",
        "#         \n",
        "#     time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Uncomment if you want to remove corrupted images in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This packages data-services experiments create and running the job of removing corrupted images\n",
        "# from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "# # try:\n",
        "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "#     train_dataset_id = remove_corrupted_images_workflow(base_url, headers, workspace_id, train_dataset_id)\n",
        "#     %store train_dataset_id\n",
        "# except Exception as e:\n",
        "#     raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and pull test dataset <a class=\"anchor\" id=\"head-5\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create test dataset using TAO SDK\n",
        "test_dataset_id = tao_client.create_dataset(\n",
        "    dataset_type=ds_type,\n",
        "    dataset_format=ds_format,\n",
        "    workspace_id=workspace_id,\n",
        "    cloud_file_path=test_dataset_path,\n",
        "    use_for=[\"testing\"]\n",
        ")\n",
        "\n",
        "print(\"Test dataset created successfully!\")\n",
        "print(f\"Test Dataset ID: {test_dataset_id}\")\n",
        "\n",
        "%store test_dataset_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check test dataset progress using TAO SDK\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    dataset_details = tao_client.get_dataset_metadata(test_dataset_id)\n",
        "    \n",
        "    print(f\" Test Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
        "    print(f\"Dataset ID: {test_dataset_id}\")\n",
        "    \n",
        "    if dataset_details.get(\"status\") == \"invalid_pull\":\n",
        "        print(\" Dataset pull failed!\")\n",
        "        validation_details = dataset_details.get(\"validation_details\", {})\n",
        "        if validation_details:\n",
        "            print(\"Validation details:\")\n",
        "            print(json.dumps(validation_details, indent=4))\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "        \n",
        "    if dataset_details.get(\"status\") == \"pull_complete\":\n",
        "        print(\"Test dataset pull completed successfully!\")\n",
        "        print(\"Dataset details:\")\n",
        "        print(json.dumps(dataset_details, indent=4))\n",
        "        break\n",
        "        \n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Uncomment if you want to remove corrupted images in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This packages data-services experiments create and running the job of removing corrupted images\n",
        "# from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "# # try:\n",
        "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "#     test_dataset_id = remove_corrupted_images_workflow(base_url, headers, workspace_id, test_dataset_id)\n",
        "#     %store test_dataset_id\n",
        "# except Exception as e:\n",
        "#     raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List the created datasets <a class=\"anchor\" id=\"head-6\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# List datasets using TAO SDK\n",
        "datasets = tao_client.list_datasets()\n",
        "\n",
        "print(\" Available datasets:\")\n",
        "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for dataset in datasets:\n",
        "    dataset_id = dataset.get(\"id\", \"N/A\")\n",
        "    dataset_type = dataset.get(\"type\", \"N/A\")\n",
        "    dataset_format = dataset.get(\"format\", \"N/A\")\n",
        "    dataset_name = dataset.get(\"name\", \"N/A\")\n",
        "    print(f\"{dataset_id}\\t{dataset_type}\\t{dataset_format}\\t\\t{dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_map = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment Details <a class=\"anchor\" id=\"head-9\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Experiment metadata\n",
        "# These parameters will be used when creating the actual job:\n",
        "\n",
        "encode_key = \"nvidia_tao\"\n",
        "checkpoint_choose_method = \"best_model\"\n",
        "\n",
        "# These will be used later in job creation\n",
        "experiment_config = {\n",
        "    \"network_arch\": model_name,\n",
        "    \"encryption_key\": encode_key,\n",
        "    \"workspace\": workspace_id\n",
        "}\n",
        "\n",
        "print(\" Experiment configuration prepared for job creation:\")\n",
        "print(json.dumps(experiment_config, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Base Experiments (PTMs) <a class=\"anchor\" id=\"head-10\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# List base experiments (PTMs) using TAO SDK  \n",
        "# These are the pre-trained models available for the selected network architecture\n",
        "base_experiments = tao_client.list_base_experiments(filter_params={\"network_arch\": model_name})\n",
        "\n",
        "print(f\" Available base experiments (PTMs) for {model_name}:\")\n",
        "print(\"name\\t\\t\\t     model id\\t\\t\\t     network architecture\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for exp in base_experiments:\n",
        "    exp_name = exp.get(\"name\", \"N/A\")\n",
        "    exp_id = exp.get(\"id\", \"N/A\")\n",
        "    exp_arch = exp.get(\"network_arch\", \"N/A\")\n",
        "    print(f\"{exp_name}\\t{exp_id}\\t{exp_arch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Assignment <a class=\"anchor\" id=\"head-11\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assign PTM <a class=\"anchor\" id=\"head-12\"></a>\n",
        "\n",
        "Search for the PTM on NGC for the SSL model chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List all pretrained models for the chosen network architecture using TAO SDK\n",
        "base_experiments_detailed = tao_client.list_base_experiments(filter_params={\"network_arch\": model_name})\n",
        "\n",
        "print(f\" Available pre-trained models for {model_name}:\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for exp in base_experiments_detailed:\n",
        "    if \"encryption_key\" not in exp:  # Skip user-created experiments, show only PTMs\n",
        "        ptm_name = exp.get(\"name\", \"N/A\")\n",
        "        ptm_version = exp.get(\"version\", \"N/A\")\n",
        "        ngc_path = exp.get(\"ngc_path\", \"N/A\")\n",
        "        print(f'PTM Name: {ptm_name}; PTM version: {ptm_version}; NGC PATH: {ngc_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Assigning pretrained models to different networks\n",
        "# From the output of previous cell make the appropriate changes to this map if you want to change the default PTM backbone.\n",
        "# Changing the default backbone here requires changing default spec/config during train/infer etc like for example\n",
        "# If you are changing the ptm to resnet34, then you have to modify the config key num_layers if it exists to 34 manually\n",
        "pretrained_map = {\"nvdinov2\" : \"nvaie/nv_dinov2_classification_model:trainable_v1.1\",\n",
        "                  }\n",
        "no_ptm_models = set([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Get pretrained model using TAO SDK\n",
        "selected_ptm_id = None\n",
        "if model_name not in no_ptm_models:\n",
        "    base_experiments_detailed = tao_client.list_base_experiments(filter_params={\"network_arch\": model_name})\n",
        "    \n",
        "    # Search for PTM with given NGC path\n",
        "    for exp in base_experiments_detailed:\n",
        "        ngc_path = exp.get(\"ngc_path\", \"\")\n",
        "        if ngc_path.endswith(pretrained_map[model_name]):\n",
        "            selected_ptm_id = exp.get(\"id\")\n",
        "            print(\" Selected PTM metadata:\")\n",
        "            print(json.dumps(exp, indent=4))\n",
        "            break\n",
        "    \n",
        "    if not selected_ptm_id:\n",
        "        print(f\" PTM with NGC path ending in '{pretrained_map[model_name]}' not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#  TAO: PTM assignment happens during job creation\n",
        "# The selected PTM ID will be used in the job creation step\n",
        "if model_name not in no_ptm_models and selected_ptm_id:\n",
        "    print(f\" PTM ID {selected_ptm_id} will be used as base_experiment_id in job creation\")\n",
        "else:\n",
        "    print(\" No PTM will be used (training from scratch)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Actions <a class=\"anchor\" id=\"head-14\"></a>\n",
        "\n",
        "For all actions:\n",
        "1. Get default spec schema and derive the default values\n",
        "2. Modify defaults if needed\n",
        "3. Post spec dictionary to the service\n",
        "4. Run model action\n",
        "5. Monitor job using retrieve\n",
        "6. Download results using job download (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train <a class=\"anchor\" id=\"head-15\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-13\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Initialize automl_params (needed for AutoML configuration)\n",
        "automl_params = {}\n",
        "\n",
        "if automl_enabled:\n",
        "    # Get default AutoML parameters using TAO SDK\n",
        "    # This is retrieved using the base experiment ID (PTM)\n",
        "    automl_params = tao_client.get_automl_defaults(network_arch=model_name, action=\"train\")\n",
        "    print(\"Default AutoML parameters:\")\n",
        "    print(json.dumps(automl_params, sort_keys=True, indent=4))\n",
        "else:\n",
        "    print(\"AutoML is disabled - automl_params initialized as empty dict\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare AutoML Configuration <a class=\"anchor\" id=\"head-15.1\"></a>\n",
        "\n",
        "Refer to these hyper-links to see the parameters supported by each network and add more parameters if necessary in addition to the default automl enabled parameters: \n",
        "\n",
        "[NVDinoV2](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/nvdinov2/nvdinov2%20-%20train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#  TAO: Prepare AutoML configuration for job creation\n",
        "automl_information = None\n",
        "\n",
        "if automl_enabled:\n",
        "    # Choose any metric that is present in the kpi dictionary present in the model's status.json. \n",
        "    # Example status.json for each model can be found in the respective section in NVIDIA TAO DOCS here: https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/index.html\n",
        "    metric = \"kpi\"\n",
        "\n",
        "    # Refer to parameter list mentioned in the above links and add/remove any extra parameter in addition to the default enabled ones in automl_specs\n",
        "    automl_information = {\n",
        "        \"automl_enabled\": True,\n",
        "        \"automl_algorithm\": automl_algorithm,\n",
        "        \"automl_max_recommendations\": 20,  # Only for bayesian\n",
        "        \"automl_R\": 27,  # Only for hyperband\n",
        "        \"automl_nu\": 3,  # Only for hyperband\n",
        "        \"epoch_multiplier\": 1,  # Only for hyperband\n",
        "        # Warning: The parameters that are disabled are not tested by TAO, so there might be unexpected behaviour in overriding this\n",
        "        \"override_automl_disabled_params\": False,\n",
        "        \"automl_hyperparameters\": str(automl_params),\n",
        "        \"metric\": metric\n",
        "    }\n",
        "    \n",
        "    print(\" AutoML configuration prepared for job creation:\")\n",
        "    print(json.dumps(automl_information, sort_keys=True, indent=4))\n",
        "    print(\" This will be included in the job_run_experiment call\")\n",
        "else:\n",
        "    print(\" AutoML is disabled - training will use standard approach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get default train specs using TAO SDK\n",
        "train_spec_response = tao_client.get_job_schema(action=\"train\", network_arch=model_name)\n",
        "train_specs = train_spec_response.get(\"default\", {})\n",
        "print(\" Default train specifications:\")\n",
        "print(json.dumps(train_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Override any of the parameters listed in the previous cell as required\n",
        "train_specs[\"train\"][\"num_gpus\"] = 1\n",
        "train_specs[\"train\"][\"num_epochs\"] = 10\n",
        "train_specs[\"train\"][\"checkpoint_interval\"] = 10\n",
        "\n",
        "print(\"Training specs:\")\n",
        "print(json.dumps(train_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create experiment job using SDK\n",
        "\n",
        "job_name = f\"{model_name}_training_job\"\n",
        "\n",
        "# Prepare job creation parameters\n",
        "job_params = {\n",
        "    \"kind\": \"experiment\",\n",
        "    \"name\": job_name,\n",
        "    \"network_arch\": model_name,\n",
        "    \"encryption_key\": encode_key,\n",
        "    \"workspace\": workspace_id,\n",
        "    \"action\": \"train\",\n",
        "    \"specs\": train_specs,  # Pass as dict, not JSON string\n",
        "    \"base_experiment_ids\": [selected_ptm_id] if selected_ptm_id else None,\n",
        "    \"train_datasets\": [train_dataset_id] if train_dataset_id else None,\n",
        "    \"eval_dataset\": test_dataset_id,\n",
        "    \"inference_dataset\": test_dataset_id,\n",
        "    \"calibration_dataset\": train_dataset_id,\n",
        "    \"automl_settings\": automl_information if automl_information else None,\n",
        "    # \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        "}\n",
        "\n",
        "# Create experiment job using TAO SDK interface\n",
        "job_id = tao_client.create_job(**job_params)\n",
        "\n",
        "print(\" Experiment job created successfully!\")\n",
        "print(f\"Job ID: {job_id}\")\n",
        "print(f\"Job Name: {job_name}\")\n",
        "print(f\"Network Architecture: {model_name}\")\n",
        "print(f\"Action: train\")\n",
        "if automl_information:\n",
        "    print(f\"AutoML: {automl_information.get('automl_algorithm', 'N/A')} algorithm\")\n",
        "\n",
        "job_map[\"train_\" + model_name] = job_id\n",
        "print(\"\\nJob Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Monitor job status using TAO SDK\n",
        "# For automl: Training times for different models benchmarked on 1 GPU V100 machine can be found here: https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#results-of-automl-experiments\n",
        "\n",
        "train_job_id = job_map[\"train_\" + model_name]\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(train_job_id)\n",
        "        \n",
        "        print(f\"Training Job Status\")\n",
        "        print(f\"Job ID: {train_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            print(\" Job failed!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\" Job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\" Job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\" Error fetching job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## To Stop an AutoML JOB\n",
        "#    1. Stop the 'Monitor job status by repeatedly running this cell' cell (the cell right before this cell) manually\n",
        "#    2. Uncomment the snippet in the next cell and run the cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# # Pause AutoML job using TAO SDK\n",
        "# if automl_enabled:\n",
        "#     train_job_id = job_map[\"train_\" + model_name]\n",
        "#     try:\n",
        "#         pause_result = tao_client.pause_job(train_job_id)\n",
        "#         print(\" Job paused successfully!\")\n",
        "#         print(json.dumps(pause_result, indent=4))\n",
        "#     except Exception as e:\n",
        "#         print(f\" Failed to pause job: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## Resume AutoML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# # Resume AutoML job using TAO SDK\n",
        "# # Uncomment the below snippet if you want to resume an already stopped AutoML job and then run the 'Monitor job status' cell above\n",
        "# if automl_enabled:\n",
        "#     train_job_id = job_map[\"train_\" + model_name]\n",
        "#     try:\n",
        "#         resume_result = tao_client.resume_job(\n",
        "#             job_id=train_job_id,\n",
        "#             parent_job_id=None,\n",
        "#             specs=json.dumps(train_specs)\n",
        "#         )\n",
        "#         print(\" Job resumed successfully!\")\n",
        "#         print(json.dumps(resume_result, indent=4))\n",
        "#     except Exception as e:\n",
        "#         print(f\" Failed to resume job: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publish model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Edit the method of choosing checkpoint from list of train checkpoint files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model handler parameters are managed differently\n",
        "# Checkpoint selection is handled during job creation rather than experiment-level settings\n",
        "# For now, we'll use the default checkpoint selection method\n",
        "print(\" In TAO, checkpoint selection is managed per-job rather than per-experiment\")\n",
        "print(\"Using default checkpoint selection method: best_model\")\n",
        "\n",
        "update_checkpoint_choosing = {\n",
        "    \"checkpoint_choose_method\": \"best_model\",\n",
        "    \"checkpoint_epoch_number\": {}\n",
        "}\n",
        "print(\"Current checkpoint choosing configuration:\")\n",
        "print(json.dumps(update_checkpoint_choosing, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checkpoint method configuration\n",
        "# Checkpoint selection is handled per-job, not per-experiment\n",
        "# You can configure this when creating export/inference jobs if needed\n",
        "\n",
        "# Example: Change checkpoint selection method for future jobs\n",
        "update_checkpoint_choosing[\"checkpoint_choose_method\"] = \"latest_model\"  # Choose between best_model/latest_model/from_epoch_number\n",
        "# Note: If from_epoch_number is chosen, you would specify the epoch in job creation specs\n",
        "\n",
        "print(\"Checkpoint selection configuration updated:\")\n",
        "print(f\"Method: {update_checkpoint_choosing['checkpoint_choose_method']}\")\n",
        "print(\"This will be applied to future job creations\")\n",
        "print(json.dumps(update_checkpoint_choosing, sort_keys=True, indent=4))\n",
        "\n",
        "updated_job = tao_client.update_job(job_id=job_map[\"train_\" + model_name], update_data=update_checkpoint_choosing)\n",
        "print(json.dumps(updated_job, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Push model to private ngc team registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Publish model using TAO SDK\n",
        "train_job_id = job_map[\"train_\" + model_name]\n",
        "\n",
        "try:\n",
        "    publish_result = tao_client.publish_model(\n",
        "        job_id=train_job_id,\n",
        "        display_name=f\"TAO {model_name}\",\n",
        "        description=f\"Trained {model_name} model\",\n",
        "        team_name=\"tao\"\n",
        "    )\n",
        "    \n",
        "    print(\" Model published successfully to NGC!\")\n",
        "    print(f\"Job ID: {train_job_id}\")\n",
        "    print(f\"Display Name: TAO {model_name}\")\n",
        "    print(f\"Team: tao\")\n",
        "    print(\"\\nPublish Response:\")\n",
        "    print(json.dumps(publish_result, indent=4))\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Failed to publish model: {str(e)}\")\n",
        "    print(\"Make sure the job completed successfully and you have appropriate permissions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Remove model from private ngc team registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Remove published model using TAO SDK\n",
        "# train_job_id = job_map[\"train_\" + model_name]\n",
        "# try:\n",
        "#     remove_result = tao_client.remove_published_model(\n",
        "#         job_id=train_job_id,\n",
        "#         team=\"tao\"\n",
        "#     )\n",
        "#     print(\" Published model removed successfully!\")\n",
        "#     print(json.dumps(remove_result, indent=4))\n",
        "# except Exception as e:\n",
        "#     print(f\" Failed to remove published model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export <a class=\"anchor\" id=\"head-18\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get default export specs using TAO SDK\n",
        "export_spec_response = tao_client.get_job_schema(action=\"export\", network_arch=model_name)\n",
        "export_specs = export_spec_response.get(\"default\", {})\n",
        "print(\" Default export specifications:\")\n",
        "print(json.dumps(export_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Apply changes\n",
        "print(json.dumps(export_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create export job using TAO SDK\n",
        "# Create a new experiment job with export action and parent job reference\n",
        "parent_job_id = job_map[\"train_\" + model_name]\n",
        "export_job_name = f\"{model_name}_export_job\"\n",
        "\n",
        "# Create export experiment job using TAO SDK interface\n",
        "export_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=export_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    action=\"export\",\n",
        "    specs=export_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\" Export job created successfully!\")\n",
        "print(f\"Export Job ID: {export_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: export\")\n",
        "\n",
        "job_map[\"export_\" + model_name] = export_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Monitor export job status using TAO SDK\n",
        "export_job_id = job_map[\"export_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(export_job_id)\n",
        "        \n",
        "        print(f\" Export Job Status\")\n",
        "        print(f\"Job ID: {export_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            print(\" Export job failed!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\" Export job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\" Export job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\" Error fetching export job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TAO inference <a class=\"anchor\" id=\"head-20\"></a>\n",
        "\n",
        "- Run inference on a set of images using the .tlt model created at train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Get default inference specs using TAO SDK\n",
        "inference_spec_response = tao_client.get_job_schema(action=\"inference\", network_arch=model_name)\n",
        "tao_inference_specs = inference_spec_response.get(\"default\", {})\n",
        "print(\" Default inference specifications:\")\n",
        "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Apply changes to specs if necessary\n",
        "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create inference job using TAO SDK\n",
        "# Create a new experiment job with inference action and parent job reference\n",
        "parent_job_id = job_map[\"train_\" + model_name]\n",
        "inference_job_name = f\"{model_name}_inference_job\"\n",
        "\n",
        "# Create inference experiment job using TAO SDK interface\n",
        "inference_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=inference_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    action=\"inference\",\n",
        "    specs=tao_inference_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\" Inference job created successfully!\")\n",
        "print(f\"Inference Job ID: {inference_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: inference\")\n",
        "\n",
        "job_map[\"inference_tao_\" + model_name] = inference_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Monitor inference job status using TAO SDK\n",
        "inference_job_id = job_map[\"inference_tao_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(inference_job_id)\n",
        "        \n",
        "        print(f\" Inference Job Status\")\n",
        "        print(f\"Job ID: {inference_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            print(\" Inference job failed!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\" Inference job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\" Inference job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\" Error fetching inference job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Jobs<a class=\"anchor\" id=\"head-22\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete jobs instead of experiments\n",
        "# Delete all created jobs using TAO SDK\n",
        "\n",
        "print(\" Deleting all created jobs...\")\n",
        "\n",
        "jobs_to_delete = []\n",
        "for job_key, job_id in job_map.items():\n",
        "    try:\n",
        "        delete_result = tao_client.delete_job(job_id)\n",
        "        print(f\" Deleted job: {job_key} (ID: {job_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to delete job {job_key} (ID: {job_id}): {str(e)}\")\n",
        "\n",
        "print(f\"\\n Job cleanup completed! Processed {len(jobs_to_delete)} jobs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete dataset <a class=\"anchor\" id=\"head-23\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete train dataset using TAO SDK\n",
        "try:\n",
        "    delete_result = tao_client.delete_dataset(train_dataset_id)\n",
        "    print(\" Train dataset deleted successfully!\")\n",
        "    print(f\"Dataset ID: {train_dataset_id}\")\n",
        "    if delete_result:\n",
        "        print(\"Delete Response:\")\n",
        "        print(json.dumps(delete_result, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\" Failed to delete train dataset {train_dataset_id}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete test dataset using TAO SDK\n",
        "try:\n",
        "    delete_result = tao_client.delete_dataset(test_dataset_id)\n",
        "    print(\" Test dataset deleted successfully!\")\n",
        "    print(f\"Dataset ID: {test_dataset_id}\")\n",
        "    if delete_result:\n",
        "        print(\"Delete Response:\")\n",
        "        print(json.dumps(delete_result, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\" Failed to delete test dataset {test_dataset_id}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create backup file of Database to cloud for quicker turnaround time when re-deploying the API service\n",
        "##### This is optional and can be skipped if you don't want to create a backup file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create workspace backup using TAO SDK\n",
        "try:\n",
        "    backup_workspace_metadata = {\n",
        "        \"cloud_type\": cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"],\n",
        "        \"cloud_specific_details\": cloud_metadata[\"cloud_specific_details\"]\n",
        "    }\n",
        "    \n",
        "    backup_result = tao_client.backup_workspace(\n",
        "        backup_file_name=\"mongodb_backup.gz\",\n",
        "        workspace_metadata=backup_workspace_metadata\n",
        "    )\n",
        "    print(\" Workspace backup created successfully!\")\n",
        "    print(f\"Backup file: mongodb_backup.gz\")\n",
        "    print(\"Backup Response:\")\n",
        "    print(json.dumps(backup_result, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\" Failed to create workspace backup: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
