{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook to demonstrate Image Classification workflow\n",
        "\n",
        "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
        "\n",
        "![image](https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
        "\n",
        "### Sample prediction for an Image Classification model\n",
        "<img align=\"center\" src=\"../example_images/sample_image_classification.jpg\">\n",
        "\n",
        "### The workflow in a nutshell\n",
        "\n",
        "- Pulling datasets from cloud\n",
        "- Getting a PTM from NGC\n",
        "- Model Actions\n",
        "    - Train (Normal/AutoML)\n",
        "    - Evaluate\n",
        "    - Prune, retrain\n",
        "    - Export\n",
        "    - TAO-Deploy\n",
        "    - Inference on TAO, TRT\n",
        "    - Delete experiments/dataset\n",
        "\n",
        "### Table of contents\n",
        "\n",
        "1. [FIXME's](#head-1)\n",
        "1. [Login](#head-2)\n",
        "1. [Create a cloud workspace](#head-2)\n",
        "1. [Set dataset formats](#head-3)\n",
        "1. [Create and pull train dataset](#head-4)\n",
        "1. [Create and pull val dataset](#head-5)\n",
        "1. [Create and pull test dataset](#head-6)\n",
        "1. [List the created datasets](#head-7)\n",
        "1. [Create an experiment](#head-8)\n",
        "1. [List experiments](#head-9)\n",
        "1. [Assign train, eval datasets](#head-10)\n",
        "1. [Assign PTM](#head-11)\n",
        "1. [Set AutoML related configurations](#head-12)\n",
        "1. [Actions](#head-14)\n",
        "1. [Train](#head-14)\n",
        "1. [View hyperparameters that are enabled by default](#head-14.1)\n",
        "1. [Evaluate](#head-15)\n",
        "1. [Optimize: Prune, retrain and evaluate](#head-16)\n",
        "1. [Export](#head-17)\n",
        "1. [TRT Engine generation using TAO-Deploy](#head-18)\n",
        "1. [TAO inference](#head-19)\n",
        "1. [TRT inference](#head-20)\n",
        "1. [Delete experiment](#head-21)\n",
        "1. [Delete dataset](#head-22)\n",
        "\n",
        "### Requirements\n",
        "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Import TAO SDK\n",
        "from tao_sdk.client import TaoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore variable in case of jupyter session restart and resume execution where it left off\n",
        "%store -r model_name\n",
        "%store -r automl_enabled\n",
        "%store -r automl_algorithm\n",
        "%store -r base_url\n",
        "%store -r headers\n",
        "%store -r workspace_id\n",
        "%store -r train_dataset_id\n",
        "%store -r eval_dataset_id\n",
        "%store -r test_dataset_id\n",
        "%store -r experiment_id\n",
        "%store -r job_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To see the dataset folder structure required for the models supported in this notebook, visit the notebooks under dataset_prepare like for [this notebook](../dataset_prepare/classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FIXME's <a class=\"anchor\" id=\"head-1\"></a>\n",
        "\n",
        "1. Assign a model_name in FIXME 1\n",
        "1. (Optional) Enable AutoML if needed in FIXME 2\n",
        "1. (Optional) Choose between bayesian and hyperband automl_algorithm in FIXME 3 (If automl was enabled in FIXME2)\n",
        "1. Assign the ip_address and port_number in FIXME 4 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
        "1. Assign the ngc_key variable in FIXME 5\n",
        "1. Assign the ngc_org_name variable in FIXME 6\n",
        "1. Set cloud storage details in FIXME 7\n",
        "1. Assign path of datasets relative to the bucket in FIXME 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Choose a classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# FIXME 1: Define model_name workspaces and other variables\n",
        "# Available models:\n",
        "# 1. classification_pyt - https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/classification_pyt.html\n",
        "\n",
        "os.environ[\"TAO_MODEL_NAME\"] = model_name = os.environ.get(\"TAO_MODEL_NAME\", \"classification_pyt\")  # Pick the model name from the above mentioned list\n",
        "%store model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Toggle AutoML params\n",
        "[AutoML documentation](https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#getting-started)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# FIXME 2: Set to True if you want to run automl for the model chosen in the previous cell\n",
        "automl_enabled = os.environ.get(\"TAO_AUTOML_ENABLED\", \"False\").lower() == \"true\"\n",
        "os.environ[\"TAO_AUTOML_ENABLED\"] = str(automl_enabled)\n",
        "# One of bayesian/hyperband\n",
        "os.environ[\"TAO_AUTOML_ALGORITHM\"] = automl_algorithm = os.environ.get(\"TAO_AUTOML_ALGORITHM\", \"bayesian\")\n",
        "\n",
        "%store automl_enabled\n",
        "%store automl_algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set API service's host information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 4: Set TAO API environment variables\n",
        "\n",
        "# Set to your TAO API endpoint\n",
        "os.environ[\"TAO_BASE_URL\"] = os.environ.get(\"TAO_BASE_URL\", \"https://your_tao_ip_address:port/api/v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set NGC Personal key for authentication and NGC org to access API services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 5: Your NGC personal key\n",
        "os.environ[\"NGC_KEY\"] = ngc_key = os.environ.get(\"NGC_KEY\", \"your_ngc_key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 6: Your NGC ORG name\n",
        "os.environ[\"NGC_ORG\"] = ngc_org_name = os.environ.get(\"NGC_ORG\", \"nvstaging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Login <a class=\"anchor\" id=\"head-2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize TAO Client and login using SDK\n",
        "tao_client = TaoClient()\n",
        "\n",
        "# Login using TAO SDK - this will automatically save credentials to environment variables\n",
        "login_response = tao_client.login(\n",
        "    ngc_key=ngc_key,\n",
        "    ngc_org_name=ngc_org_name,\n",
        "    enable_telemetry=True\n",
        ")\n",
        "\n",
        "print(\"Login successful!\")\n",
        "print(\"JWT Token:\", tao_client.token)\n",
        "print(\"API Base URL:\", tao_client.base_url)\n",
        "print(\"Organization:\", tao_client.org_name)\n",
        "\n",
        "%store tao_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get NVCF gpu details <a class=\"anchor\" id=\"head-2\"></a>\n",
        "\n",
        " One of the keys of the response json are to be used as platform_id when you run each job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Valid only for NVCF backend during TAO-API helm deployment currently\n",
        "# # Get available GPU types using TAO SDK\n",
        "# try:\n",
        "#     gpu_types = tao_client.get_gpu_types()\n",
        "#     print(\"Available GPU types:\")\n",
        "#     print(json.dumps(gpu_types, indent=4))\n",
        "# except Exception as e:\n",
        "#     print(\"Could not fetch GPU types (may not be available on this deployment):\", str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create cloud workspace\n",
        "This workspace will be the place where your datasets reside and your results of TAO API jobs will be pushed to.\n",
        "\n",
        "If you want to have different workspaces for dataset and experiment, duplocate the workspace creation part and adjust the metadata accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 7: Dataset Cloud bucket details to download dataset or push job artifacts for jobs\n",
        "\n",
        "cloud_metadata = {}\n",
        "\n",
        "# A Representative name for this cloud info\n",
        "os.environ[\"TAO_WORKSPACE_NAME\"] = cloud_metadata[\"name\"] = os.environ.get(\"TAO_WORKSPACE_NAME\", \"AWS workspace info\")\n",
        "\n",
        "# Cloud specific details. Below is assuming AWS.\n",
        "cloud_metadata[\"cloud_specific_details\"] = {}\n",
        "\n",
        " # Whether it is AWS, HuggingFace or Azure\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_TYPE\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_TYPE\", \"aws\")\n",
        "\n",
        "# Bucket region\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_REGION\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_REGION\", \"us-west-1\")\n",
        "\n",
        "# Bucket name\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_BUCKET_NAME\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_BUCKET_NAME\", \"bucket_name\")\n",
        "\n",
        "# Access and Secret keys\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_ACCESS_KEY\"] = cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_ACCESS_KEY\", \"access_key\")\n",
        "os.environ[\"TAO_WORKSPACE_CLOUD_SECRET_KEY\"] = cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_SECRET_KEY\", \"secret_key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cloud workspace using TAO SDK\n",
        "workspace_id = tao_client.create_workspace(\n",
        "    name=cloud_metadata[\"name\"],\n",
        "    cloud_type=cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"],\n",
        "    cloud_specific_details=cloud_metadata[\"cloud_specific_details\"]\n",
        ")\n",
        "\n",
        "print(\"Workspace created successfully!\")\n",
        "print(f\"Workspace ID: {workspace_id}\")\n",
        "\n",
        "# Get workspace details to confirm creation\n",
        "workspace_details = tao_client.get_workspace_metadata(workspace_id)\n",
        "print(\"Workspace details:\")\n",
        "print(json.dumps(workspace_details, indent=4))\n",
        "\n",
        "%store workspace_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set dataset path (path within cloud bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME 8 : Set paths relative to cloud bucket\n",
        "os.environ[\"TAO_TRAIN_DATASET_PATH\"] = train_dataset_path =  os.environ.get(\"TAO_TRAIN_DATASET_PATH\", f\"/data/classification_train\")\n",
        "os.environ[\"TAO_EVAL_DATASET_PATH\"] = eval_dataset_path = os.environ.get(\"TAO_EVAL_DATASET_PATH\", f\"/data/classification_val\")\n",
        "os.environ[\"TAO_TEST_DATASET_PATH\"] = test_dataset_path = os.environ.get(\"TAO_TEST_DATASET_PATH\", f\"/data/classification_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set dataset formats <a class=\"anchor\" id=\"head-3\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train dataset\n",
        "ds_type = \"image_classification\"\n",
        "ds_format = model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and pull train dataset <a class=\"anchor\" id=\"head-4\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create train dataset using TAO SDK\n",
        "train_dataset_id = tao_client.create_dataset(\n",
        "    dataset_type=ds_type,\n",
        "    dataset_format=ds_format,\n",
        "    workspace_id=workspace_id,\n",
        "    cloud_file_path=train_dataset_path,\n",
        "    use_for=[\"training\"]\n",
        ")\n",
        "\n",
        "print(\"Train dataset created successfully!\")\n",
        "print(f\"Train Dataset ID: {train_dataset_id}\")\n",
        "\n",
        "%store train_dataset_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check train dataset progress using TAO SDK\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    dataset_details = tao_client.get_dataset_metadata(train_dataset_id)\n",
        "    \n",
        "    print(f\" Train Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
        "    print(f\"Dataset ID: {train_dataset_id}\")\n",
        "    \n",
        "    if dataset_details.get(\"status\") == \"invalid_pull\":\n",
        "        print(\"Dataset pull failed!\")\n",
        "        validation_details = dataset_details.get(\"validation_details\", {})\n",
        "        if validation_details:\n",
        "            print(\"Validation details:\")\n",
        "            print(json.dumps(validation_details, indent=4))\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "        \n",
        "    if dataset_details.get(\"status\") == \"pull_complete\":\n",
        "        print(\"Train dataset pull completed successfully!\")\n",
        "        print(\"Dataset details:\")\n",
        "        print(json.dumps(dataset_details, indent=4))\n",
        "        break\n",
        "        \n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Uncomment if you want to remove corrupted images in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This packages data-services experiments create and running the job of removing corrupted images\n",
        "# try:\n",
        "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "#     train_dataset_id = remove_corrupted_images_workflow(base_url, headers, workspace_id, train_dataset_id)\n",
        "#     %store train_dataset_id\n",
        "# except Exception as e:\n",
        "#     raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and pull val dataset <a class=\"anchor\" id=\"head-5\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create eval dataset using TAO SDK\n",
        "eval_dataset_id = tao_client.create_dataset(\n",
        "    dataset_type=ds_type,\n",
        "    dataset_format=ds_format,\n",
        "    workspace_id=workspace_id,\n",
        "    cloud_file_path=eval_dataset_path,\n",
        "    use_for=[\"evaluation\"]\n",
        ")\n",
        "\n",
        "print(\"Eval dataset created successfully!\")\n",
        "print(f\"Eval Dataset ID: {eval_dataset_id}\")\n",
        "\n",
        "%store eval_dataset_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check eval dataset progress using TAO SDK\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    dataset_details = tao_client.get_dataset_metadata(eval_dataset_id)\n",
        "    \n",
        "    print(f\" Eval Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
        "    print(f\"Dataset ID: {eval_dataset_id}\")\n",
        "    \n",
        "    if dataset_details.get(\"status\") == \"invalid_pull\":\n",
        "        print(\"Dataset pull failed!\")\n",
        "        validation_details = dataset_details.get(\"validation_details\", {})\n",
        "        if validation_details:\n",
        "            print(\"Validation details:\")\n",
        "            print(json.dumps(validation_details, indent=4))\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "        \n",
        "    if dataset_details.get(\"status\") == \"pull_complete\":\n",
        "        print(\"Eval dataset pull completed successfully!\")\n",
        "        print(\"Dataset details:\")\n",
        "        print(json.dumps(dataset_details, indent=4))\n",
        "        break\n",
        "        \n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Uncomment if you want to remove corrupted images in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This packages data-services experiments create and running the job of removing corrupted images\n",
        "# try:\n",
        "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "#     eval_dataset_id = remove_corrupted_images_workflow(base_url, headers, workspace_id, eval_dataset_id)\n",
        "#     %store eval_dataset_id\n",
        "# except Exception as e:\n",
        "#     raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and pull test dataset <a class=\"anchor\" id=\"head-6\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create test dataset using TAO SDK\n",
        "test_dataset_id = tao_client.create_dataset(\n",
        "    dataset_type=ds_type,\n",
        "    dataset_format=ds_format,\n",
        "    workspace_id=workspace_id,\n",
        "    cloud_file_path=test_dataset_path,\n",
        "    use_for=[\"testing\"]\n",
        ")\n",
        "\n",
        "print(\"Test dataset created successfully!\")\n",
        "print(f\"Test Dataset ID: {test_dataset_id}\")\n",
        "\n",
        "%store test_dataset_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check test dataset progress using TAO SDK\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    dataset_details = tao_client.get_dataset_metadata(test_dataset_id)\n",
        "    \n",
        "    print(f\"Test Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
        "    print(f\"Dataset ID: {test_dataset_id}\")\n",
        "    \n",
        "    if dataset_details.get(\"status\") == \"invalid_pull\":\n",
        "        print(\"Dataset pull failed!\")\n",
        "        validation_details = dataset_details.get(\"validation_details\", {})\n",
        "        if validation_details:\n",
        "            print(\"Validation details:\")\n",
        "            print(json.dumps(validation_details, indent=4))\n",
        "        raise ValueError(\"Dataset pull failed\")\n",
        "        \n",
        "    if dataset_details.get(\"status\") == \"pull_complete\":\n",
        "        print(\"Test dataset pull completed successfully!\")\n",
        "        print(\"Dataset details:\")\n",
        "        print(json.dumps(dataset_details, indent=4))\n",
        "        break\n",
        "        \n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Uncomment if you want to remove corrupted images in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This packages data-services experiments create and running the job of removing corrupted images\n",
        "# try:\n",
        "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
        "#     test_dataset_id = remove_corrupted_images_workflow(base_url, headers, workspace_id, test_dataset_id)\n",
        "#     %store test_dataset_id\n",
        "# except Exception as e:\n",
        "#     raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List the created datasets <a class=\"anchor\" id=\"head-7\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List datasets using TAO SDK\n",
        "datasets = tao_client.list_datasets()\n",
        "\n",
        "print(\"Available datasets:\")\n",
        "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for dataset in datasets:\n",
        "    dataset_id = dataset.get(\"id\", \"N/A\")\n",
        "    dataset_type = dataset.get(\"type\", \"N/A\")\n",
        "    dataset_format = dataset.get(\"format\", \"N/A\")\n",
        "    dataset_name = dataset.get(\"name\", \"N/A\")\n",
        "    print(f\"{dataset_id}\\t{dataset_type}\\t{dataset_format}\\t\\t{dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_map = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set common params across all jobs <a class=\"anchor\" id=\"head-8\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# These parameters are common to all jobs and will be used when creating the actual job:\n",
        "encode_key = \"nvidia_tao\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assign PTM <a class=\"anchor\" id=\"head-11\"></a>\n",
        "\n",
        "Search for the PTM on NGC for the Classification model chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List base experiments (PTMs) using TAO SDK  \n",
        "# These are the pre-trained models available for the selected network architecture\n",
        "base_experiments = tao_client.list_base_experiments(filter_params={\"network_arch\": model_name})\n",
        "\n",
        "print(f\" Available base experiments (PTMs) for {model_name}:\")\n",
        "print(\"name\\t\\t\\t     model id\\t\\t\\t     network architecture\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for exp in base_experiments:\n",
        "    exp_name = exp.get(\"name\", \"N/A\")\n",
        "    exp_id = exp.get(\"id\", \"N/A\")\n",
        "    exp_arch = exp.get(\"network_arch\", \"N/A\")\n",
        "    print(f\"{exp_name}\\t{exp_id}\\t{exp_arch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Assigning pretrained models to different classification models\n",
        "# From the output of previous cell make the appropriate changes to this map if you want to change the default PTM backbone.\n",
        "# Changing the default backbone here requires changing default spec/config during train/eval etc like for example\n",
        "# If you are changing the ptm to resnet34, then you have to modify the config key num_layers if it exists to 34 manually\n",
        "pretrained_map = {\"classification_pyt\" : \"nvidia/tao/pretrained_fan_classification_imagenet:fan_hybrid_small\",\n",
        "                  }\n",
        "no_ptm_models = set([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get pretrained model using TAO SDK\n",
        "selected_ptm_id = None\n",
        "if model_name not in no_ptm_models:\n",
        "    base_experiments_detailed = tao_client.list_base_experiments(filter_params={\"network_arch\": model_name})\n",
        "    \n",
        "    # Search for PTM with given NGC path\n",
        "    for exp in base_experiments_detailed:\n",
        "        ngc_path = exp.get(\"ngc_path\", \"\")\n",
        "        if ngc_path.endswith(pretrained_map[model_name]):\n",
        "            selected_ptm_id = exp.get(\"id\")\n",
        "            print(\"Selected PTM metadata:\")\n",
        "            print(json.dumps(exp, indent=4))\n",
        "            break\n",
        "    \n",
        "    if not selected_ptm_id:\n",
        "        print(f\" PTM with NGC path ending in '{pretrained_map[model_name]}' not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "#  TAO: PTM assignment happens during job creation\n",
        "# The selected PTM ID will be used in the job creation step\n",
        "if model_name not in no_ptm_models and selected_ptm_id:\n",
        "    print(f\" PTM ID {selected_ptm_id} will be used as base_experiment_id in job creation\")\n",
        "else:\n",
        "    print(\"No PTM will be used (training from scratch)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Actions <a class=\"anchor\" id=\"head-13\"></a>\n",
        "\n",
        "For all actions:\n",
        "1. Get default spec schema and derive the default values\n",
        "2. Modify defaults if needed\n",
        "3. Post spec dictionary to the service\n",
        "4. Run model action\n",
        "5. Monitor job using retrieve\n",
        "6. Download results using job download endpoint (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_map = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train <a class=\"anchor\" id=\"head-14\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-12\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize automl_params (needed for AutoML configuration)\n",
        "automl_params = {}\n",
        "\n",
        "if automl_enabled:\n",
        "    # Get default AutoML parameters using TAO SDK\n",
        "    # This is retrieved using the base experiment ID (PTM)\n",
        "    automl_params = tao_client.get_automl_defaults(network_arch=model_name, action=\"train\")\n",
        "    print(\"Default AutoML parameters:\")\n",
        "    print(json.dumps(automl_params, sort_keys=True, indent=4))\n",
        "else:\n",
        "    print(\"AutoML is disabled - automl_params initialized as empty dict\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set AutoML related configurations <a class=\"anchor\" id=\"head-14.1\"></a>\n",
        "Refer to these hyper-links to see the parameters supported by each network and add more parameters if necessary in addition to the default automl enabled parameters:\n",
        "\n",
        "[Classification Pytorch](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/classification_pyt/classification_pyt%20-%20train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "#  TAO: Prepare AutoML configuration for job creation\n",
        "automl_information = None\n",
        "\n",
        "if automl_enabled:\n",
        "    # Choose any metric that is present in the kpi dictionary present in the model's status.json. \n",
        "    # Example status.json for each model can be found in the respective section in NVIDIA TAO DOCS here: https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/index.html\n",
        "    metric = \"kpi\"\n",
        "\n",
        "    # Refer to parameter list mentioned in the above links and add/remove any extra parameter in addition to the default enabled ones in automl_specs\n",
        "    automl_information = {\n",
        "        \"automl_enabled\": True,\n",
        "        \"automl_algorithm\": automl_algorithm,\n",
        "        \"automl_max_recommendations\": 20,  # Only for bayesian\n",
        "        \"automl_R\": 27,  # Only for hyperband\n",
        "        \"automl_nu\": 3,  # Only for hyperband\n",
        "        \"epoch_multiplier\": 1,  # Only for hyperband\n",
        "        # Warning: The parameters that are disabled are not tested by TAO, so there might be unexpected behaviour in overriding this\n",
        "        \"override_automl_disabled_params\": False,\n",
        "        \"automl_hyperparameters\": str(automl_params),\n",
        "        \"metric\": metric\n",
        "    }\n",
        "    \n",
        "    print(\"AutoML configuration prepared for job creation:\")\n",
        "    print(json.dumps(automl_information, sort_keys=True, indent=4))\n",
        "    print(\"This will be included in the job_run_experiment call\")\n",
        "else:\n",
        "    print(\"AutoML is disabled - training will use standard approach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get default train specs using TAO SDK\n",
        "train_spec_response = tao_client.get_job_schema(action=\"train\", network_arch=model_name)\n",
        "train_specs = train_spec_response.get(\"default\", {})\n",
        "print(\"Default train specifications:\")\n",
        "print(json.dumps(train_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Override any of the parameters listed in the previous cell as required\n",
        "# Example for classification_pyt\n",
        "train_specs[\"train\"][\"num_epochs\"] = 2\n",
        "train_specs[\"train\"][\"validation_interval\"] = 2\n",
        "train_specs[\"train\"][\"checkpoint_interval\"] = 2\n",
        "train_specs[\"train\"][\"num_gpus\"] = 1\n",
        "\n",
        "print(\"Training specs:\")\n",
        "print(json.dumps(train_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create experiment job using SDK\n",
        "\n",
        "job_name = f\"{model_name}_training_job\"\n",
        "\n",
        "# Prepare job creation parameters\n",
        "job_params = {\n",
        "    \"kind\": \"experiment\",\n",
        "    \"name\": job_name,\n",
        "    \"network_arch\": model_name,\n",
        "    \"encryption_key\": encode_key,\n",
        "    \"workspace\": workspace_id,\n",
        "    \"action\": \"train\",\n",
        "    \"specs\": train_specs,  # Pass as dict, not JSON string\n",
        "    \"base_experiment_ids\": [selected_ptm_id] if selected_ptm_id else None,\n",
        "    \"train_datasets\": [train_dataset_id] if train_dataset_id else None,\n",
        "    \"eval_dataset\": eval_dataset_id,\n",
        "    \"inference_dataset\": test_dataset_id,\n",
        "    \"automl_settings\": automl_information if automl_information else None,\n",
        "    # \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        "}\n",
        "\n",
        "# Create experiment job using TAO SDK interface\n",
        "job_id = tao_client.create_job(**job_params)\n",
        "\n",
        "print(\"Train job created successfully!\")\n",
        "print(f\"Job ID: {job_id}\")\n",
        "print(f\"Job Name: {job_name}\")\n",
        "print(f\"Network Architecture: {model_name}\")\n",
        "print(f\"Action: train\")\n",
        "if automl_information:\n",
        "    print(f\"AutoML: {automl_information.get('automl_algorithm', 'N/A')} algorithm\")\n",
        "\n",
        "job_map[\"train_\" + model_name] = job_id\n",
        "print(\"\\nJob Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Monitor job status using TAO SDK\n",
        "# For automl: Training times for different models benchmarked on 1 GPU V100 machine can be found here: https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#results-of-automl-experiments\n",
        "\n",
        "train_job_id = job_map[\"train_\" + model_name]\n",
        "\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(train_job_id)\n",
        "        \n",
        "        print(f\"Training Job Status\")\n",
        "        print(f\"Job ID: {train_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Training job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\" Job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## To Stop an AutoML JOB\n",
        "#    1. Stop the 'Monitor job status by repeatedly running this cell' cell (the cell right before this cell) manually\n",
        "#    2. Uncomment the snippet in the next cell and run the cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Pause AutoML job using TAO SDK\n",
        "# if automl_enabled:\n",
        "#     train_job_id = job_map[\"train_\" + model_name]\n",
        "#     try:\n",
        "#         pause_result = tao_client.pause_job(train_job_id)\n",
        "#         print(\"Job paused successfully!\")\n",
        "#         print(json.dumps(pause_result, indent=4))\n",
        "#     except Exception as e:\n",
        "#         print(f\" Failed to pause job: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Resume AutoML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# # Resume AutoML job using TAO SDK\n",
        "# # Uncomment the below snippet if you want to resume an already stopped AutoML job and then run the 'Monitor job status' cell above\n",
        "# if automl_enabled:\n",
        "#     train_job_id = job_map[\"train_\" + model_name]\n",
        "#     try:\n",
        "#         resume_result = tao_client.resume_job(\n",
        "#             job_id=train_job_id,\n",
        "#             parent_job_id=None,\n",
        "#             specs=json.dumps(train_specs)\n",
        "#         )\n",
        "#         print(\"Job resumed successfully!\")\n",
        "#         print(json.dumps(resume_result, indent=4))\n",
        "#     except Exception as e:\n",
        "#         print(f\" Failed to resume job: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publish model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Edit the method of choosing checkpoint from list of train checkpoint files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Model handler parameters are managed differently\n",
        "# Checkpoint selection is handled during job creation rather than experiment-level settings\n",
        "# For now, we'll use the default checkpoint selection method\n",
        "print(\"In TAO, checkpoint selection is managed per-job rather than per-experiment\")\n",
        "print(\"Using default checkpoint selection method: best_model\")\n",
        "\n",
        "update_checkpoint_choosing = {\n",
        "    \"checkpoint_choose_method\": \"best_model\",\n",
        "    \"checkpoint_epoch_number\": {}\n",
        "}\n",
        "print(\"Current checkpoint choosing configuration:\")\n",
        "print(json.dumps(update_checkpoint_choosing, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Checkpoint method configuration\n",
        "# Checkpoint selection is handled per-job, not per-experiment\n",
        "# You can configure this when creating export/inference jobs if needed\n",
        "\n",
        "# Example: Change checkpoint selection method for future jobs\n",
        "update_checkpoint_choosing[\"checkpoint_choose_method\"] = \"latest_model\"  # Choose between best_model/latest_model/from_epoch_number\n",
        "# Note: If from_epoch_number is chosen, you would specify the epoch in job creation specs\n",
        "\n",
        "print(\"Checkpoint selection configuration updated:\")\n",
        "print(f\"Method: {update_checkpoint_choosing['checkpoint_choose_method']}\")\n",
        "print(\"This will be applied to future job creations\")\n",
        "print(json.dumps(update_checkpoint_choosing, sort_keys=True, indent=4))\n",
        "\n",
        "updated_job = tao_client.update_job(job_id=job_map[\"train_\" + model_name], update_data=update_checkpoint_choosing)\n",
        "print(json.dumps(updated_job, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Push model to private ngc team registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publish model using TAO SDK\n",
        "train_job_id = job_map[\"train_\" + model_name]\n",
        "\n",
        "try:\n",
        "    publish_result = tao_client.publish_model(\n",
        "        job_id=train_job_id,\n",
        "        display_name=f\"TAO {model_name}\",\n",
        "        description=f\"Trained {model_name} model\",\n",
        "        team_name=\"tao\"\n",
        "    )\n",
        "    \n",
        "    print(\"Model published successfully to NGC!\")\n",
        "    print(f\"Job ID: {train_job_id}\")\n",
        "    print(f\"Display Name: TAO {model_name}\")\n",
        "    print(f\"Team: tao\")\n",
        "    print(\"\\nPublish Response:\")\n",
        "    print(json.dumps(publish_result, indent=4))\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Failed to publish model: {str(e)}\")\n",
        "    print(\"Make sure the job completed successfully and you have appropriate permissions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Remove model from private ngc team registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Remove published model using TAO SDK\n",
        "# train_job_id = job_map[\"train_\" + model_name]\n",
        "# try:\n",
        "#     remove_result = tao_client.remove_published_model(\n",
        "#         job_id=train_job_id,\n",
        "#         team=\"tao\"\n",
        "#     )\n",
        "#     print(\"Published model removed successfully!\")\n",
        "#     print(json.dumps(remove_result, indent=4))\n",
        "# except Exception as e:\n",
        "#     print(f\" Failed to remove published model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate <a class=\"anchor\" id=\"head-15\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get default eval specs using TAO SDK\n",
        "eval_spec_response = tao_client.get_job_schema(action=\"evaluate\", network_arch=model_name)\n",
        "eval_specs = eval_spec_response.get(\"default\", {})\n",
        "print(\"Default evaluate specifications:\")\n",
        "print(json.dumps(eval_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modify specs dictionary to change any config parameters\n",
        "print(json.dumps(eval_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluate job using TAO SDK\n",
        "parent_job_id = job_map[\"train_\" + model_name]\n",
        "eval_job_name = f\"{model_name}_eval_job\"\n",
        "\n",
        "eval_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=eval_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    eval_dataset=eval_dataset_id,\n",
        "    action=\"evaluate\",\n",
        "    specs=eval_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"Evaluate job created successfully!\")\n",
        "print(f\"Evaluate Job ID: {eval_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: evaluate\")\n",
        "\n",
        "job_map[\"evaluate_\" + model_name] = eval_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor evaluate job status using TAO SDK\n",
        "eval_job_id = job_map[\"evaluate_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(eval_job_id)\n",
        "        \n",
        "        print(f\"Evaluate Job Status\")\n",
        "        print(f\"Job ID: {eval_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Evaluate job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Evaluate job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\"Evaluate job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching inference job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export <a class=\"anchor\" id=\"head-17\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get default export specs using TAO SDK\n",
        "export_spec_response = tao_client.get_job_schema(action=\"export\", network_arch=model_name)\n",
        "export_specs = export_spec_response.get(\"default\", {})\n",
        "print(\"Default export specifications:\")\n",
        "print(json.dumps(export_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply changes to spec dictionary if required\n",
        "export_specs[\"export\"][\"input_height\"] = 224\n",
        "export_specs[\"export\"][\"input_width\"] = 224\n",
        "print(json.dumps(export_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create export job using TAO SDK\n",
        "parent_job_id = job_map[\"train_\" + model_name]\n",
        "export_job_name = f\"{model_name}_export_job\"\n",
        "\n",
        "export_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=export_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    train_datasets=[train_dataset_id],\n",
        "    eval_dataset=eval_dataset_id,\n",
        "    inference_dataset=test_dataset_id,\n",
        "    action=\"export\",\n",
        "    specs=export_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"Export job created successfully!\")\n",
        "print(f\"Export Job ID: {export_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: export\")\n",
        "\n",
        "job_map[\"export_\" + model_name] = export_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor export job status using TAO SDK\n",
        "export_job_id = job_map[\"export_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(export_job_id)\n",
        "        \n",
        "        print(f\"Export Job Status\")\n",
        "        print(f\"Job ID: {export_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Export job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Export job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\" Export job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching export job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Post-training quantization of the model <a class=\"anchor\" id=\"head-15\"></a>\n",
        "- Quantization can be carried out on either the trainined model or on the ONNX exported model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get default inference specs using TAO SDK\n",
        "quantize_on_train_spec_response = tao_client.get_job_schema(action=\"quantize\", network_arch=model_name)\n",
        "quantize_specs = quantize_on_train_spec_response.get(\"default\", {})\n",
        "print(\"Default inference specifications:\")\n",
        "print(json.dumps(quantize_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quantization on the trained model <a class=\"anchor\" id=\"head-15.1\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Modify specs dictionary to change any config parameters\n",
        "import copy\n",
        "quantize_on_train_specs = copy.deepcopy(quantize_specs)\n",
        "quantize_on_train_specs[\"quantize\"][\"layers\"] = [\n",
        "    {\n",
        "        \"module_name\": \"*\",\n",
        "        \"weights\": {\n",
        "            \"dtype\": \"float8_e4m3fn\"\n",
        "        },\n",
        "        \"activations\": {\n",
        "            \"dtype\": \"float8_e4m3fn\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(json.dumps(quantize_on_train_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create quantize on train job using TAO SDK\n",
        "parent_job_id = job_map[\"train_\" + model_name]\n",
        "quantize_on_train_job_name = f\"{model_name}_quantize_on_train_job\"\n",
        "\n",
        "quantize_on_train_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=quantize_on_train_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    train_datasets=[train_dataset_id],\n",
        "    calibration_dataset=train_dataset_id,\n",
        "    action=\"quantize\",\n",
        "    specs=quantize_on_train_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"Quantize on train job created successfully!\")\n",
        "print(f\"Quantize on train Job ID: {quantize_on_train_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: quantize\")\n",
        "\n",
        "job_map[\"quantize_on_train_\" + model_name] = quantize_on_train_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor quantize on train job status using TAO SDK\n",
        "quantize_on_train_job_id = job_map[\"quantize_on_train_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(quantize_on_train_job_id)\n",
        "        \n",
        "        print(f\"Quantize on train Job Status\")\n",
        "        print(f\"Job ID: {quantize_on_train_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Quantize on train job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Quantize on train job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\"Quantize on train job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching quantize on train job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quantization on the exported model <a class=\"anchor\" id=\"head-15.2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modify specs dictionary to change any config parameters\n",
        "quantize_on_export_specs = copy.deepcopy(quantize_specs)\n",
        "quantize_on_export_specs[\"quantize\"][\"backend\"] = \"modelopt.onnx\"\n",
        "quantize_on_export_specs[\"quantize\"][\"mode\"] = \"static_ptq\"\n",
        "quantize_on_export_specs[\"quantize\"][\"algorithm\"] = \"max\"\n",
        "quantize_on_export_specs[\"quantize\"][\"layers\"] = [    \n",
        "    {\n",
        "        \"module_name\": \"*\",\n",
        "        \"weights\": {\n",
        "            \"dtype\": \"int8\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "quantize_on_export_specs[\"quantize\"][\"backend_kwargs\"] = {\n",
        "    \"log_level\": \"DEBUG\",\n",
        "    \"calibration_eps\": [\"cpu\",\"cuda:0\", \"trt\"]\n",
        "}\n",
        "print(json.dumps(quantize_on_export_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create quantize on export job using TAO SDK\n",
        "parent_job_id = job_map[\"export_\" + model_name]\n",
        "quantize_on_export_job_name = f\"{model_name}_quantize_on_export_job\"\n",
        "\n",
        "# Create quantize on export experiment job using TAO SDK interface\n",
        "quantize_on_export_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=quantize_on_export_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    train_datasets=[train_dataset_id],\n",
        "    calibration_dataset=train_dataset_id,\n",
        "    action=\"quantize\",\n",
        "    specs=quantize_on_export_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"Quantize on export job created successfully!\")\n",
        "print(f\"Quantize on export Job ID: {quantize_on_export_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: quantize\")\n",
        "\n",
        "job_map[\"quantize_on_export_\" + model_name] = quantize_on_export_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor quantize on export job status using TAO SDK\n",
        "quantize_on_export_job_id = job_map[\"quantize_on_export_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(quantize_on_export_job_id)\n",
        "        \n",
        "        print(f\" Quantize on export Job Status\")\n",
        "        print(f\"Job ID: {quantize_on_export_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Quantize on export job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Quantize on export job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\"Quantize on export job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching quantize on export job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRT Engine generation using TAO-Deploy <a class=\"anchor\" id=\"head-18\"></a>\n",
        "\n",
        "- Here, we use the exported model to generate trt engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get default generate trt engine specs using TAO SDK\n",
        "tao_deploy_spec_response = tao_client.get_job_schema(action=\"gen_trt_engine\", network_arch=model_name)\n",
        "tao_deploy_specs = tao_deploy_spec_response.get(\"default\", {})\n",
        "print(\"Default trt engine specifications:\")\n",
        "print(json.dumps(tao_deploy_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply changes\n",
        "tao_deploy_specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"fp16\"\n",
        "print(json.dumps(tao_deploy_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create generate trt engine job using TAO SDK\n",
        "parent_job_id = job_map[\"quantize_on_export_\" + model_name]\n",
        "gen_trt_engine_job_name = f\"{model_name}_gen_trt_engine_job\"\n",
        "\n",
        "gen_trt_engine_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=gen_trt_engine_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    action=\"gen_trt_engine\",\n",
        "    specs=tao_deploy_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"Generate trt engine job created successfully!\")\n",
        "print(f\"Generate trt engine Job ID: {gen_trt_engine_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: gen_trt_engine\")\n",
        "\n",
        "job_map[\"gen_trt_engine_\" + model_name] = gen_trt_engine_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor gen_trt_engine job status using TAO SDK\n",
        "gen_trt_engine_job_id = job_map[\"gen_trt_engine_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(gen_trt_engine_job_id)\n",
        "        \n",
        "        print(f\"Generate trt engine Job Status\")\n",
        "        print(f\"Job ID: {gen_trt_engine_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Generate trt engine job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Generate trt engine job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\"Generate trt engine job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\"Error fetching gen_trt_engine job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TAO inference <a class=\"anchor\" id=\"head-19\"></a>\n",
        "\n",
        "- Run inference on a set of images using the model created from quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get default inference specs using TAO SDK\n",
        "inference_spec_response = tao_client.get_job_schema(action=\"inference\", network_arch=model_name)\n",
        "tao_inference_specs = inference_spec_response.get(\"default\", {})\n",
        "print(\"Default inference specifications:\")\n",
        "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply changes to the specs you want to modify\n",
        "tao_inference_specs[\"inference\"][\"is_quantized\"] = True\n",
        "tao_inference_specs[\"quantize\"][\"layers\"] = [    \n",
        "    {\n",
        "        \"module_name\": \"*\",\n",
        "        \"weights\": {\n",
        "            \"dtype\": \"float8_e4m3fn\"\n",
        "        },\n",
        "        \"activations\": {\n",
        "            \"dtype\": \"float8_e4m3fn\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create inference job using TAO SDK\n",
        "parent_job_id = job_map[\"quantize_on_train_\" + model_name]\n",
        "inference_job_name = f\"{model_name}_inference_job\"\n",
        "\n",
        "inference_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=inference_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    inference_dataset=test_dataset_id,\n",
        "    action=\"inference\",\n",
        "    specs=tao_inference_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"Inference job created successfully!\")\n",
        "print(f\"Inference Job ID: {inference_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: inference\")\n",
        "\n",
        "job_map[\"inference_tao_\" + model_name] = inference_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor inference job status using TAO SDK\n",
        "inference_job_id = job_map[\"inference_tao_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(inference_job_id)\n",
        "        \n",
        "        print(f\" Inference Job Status\")\n",
        "        print(f\"Job ID: {inference_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"Inference job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"Inference job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\" Inference job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching inference job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRT inference <a class=\"anchor\" id=\"head-20\"></a>\n",
        "\n",
        "- no need to change the specs since we already uploaded it at the tlt inference step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get default trt inference specs using TAO SDK\n",
        "trt_inference_spec_response = tao_client.get_job_schema(action=\"inference\", network_arch=model_name)\n",
        "trt_inference_specs = trt_inference_spec_response.get(\"default\", {})\n",
        "print(\"Default trt inference specifications:\")\n",
        "print(json.dumps(trt_inference_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply changes to the specs you want to modify\n",
        "trt_inference_specs[\"dataset\"][\"batch_size\"] = 1\n",
        "print(json.dumps(trt_inference_specs, sort_keys=True, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897359ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trt inference job using TAO SDK\n",
        "parent_job_id = job_map[\"gen_trt_engine_\" + model_name]\n",
        "trt_inference_job_name = f\"{model_name}_trt_inference_job\"\n",
        "\n",
        "trt_inference_job_id = tao_client.create_job(\n",
        "    kind=\"experiment\",\n",
        "    name=trt_inference_job_name,\n",
        "    network_arch=model_name,\n",
        "    encryption_key=encode_key,\n",
        "    workspace=workspace_id,\n",
        "    eval_dataset=eval_dataset_id,\n",
        "    inference_dataset=test_dataset_id,\n",
        "    action=\"inference\",\n",
        "    specs=trt_inference_specs,  # Pass as dict, not JSON string\n",
        "    parent_job_id=parent_job_id,\n",
        "    base_experiment_ids=[selected_ptm_id] if selected_ptm_id else None,\n",
        "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
        ")\n",
        "\n",
        "print(\"TRT Inference job created successfully!\")\n",
        "print(f\"TRT Inference Job ID: {trt_inference_job_id}\")\n",
        "print(f\"Parent Job ID: {parent_job_id}\")\n",
        "print(f\"Action: inference\")\n",
        "\n",
        "job_map[\"trt_inference_\" + model_name] = trt_inference_job_id\n",
        "print(\"\\nUpdated Job Map:\")\n",
        "print(json.dumps(job_map, indent=4))\n",
        "%store job_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Monitor trt inference job status using TAO SDK\n",
        "trt_inference_job_id = job_map[\"trt_inference_\" + model_name]\n",
        "\n",
        "while True:    \n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    try:\n",
        "        job_status = tao_client.get_job_metadata(trt_inference_job_id)\n",
        "        \n",
        "        print(f\"TRT Inference Job Status\")\n",
        "        print(f\"Job ID: {trt_inference_job_id}\")\n",
        "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
        "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
        "        \n",
        "        # Show detailed status information\n",
        "        print(\"\\nDetailed Status:\")\n",
        "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
        "        \n",
        "        current_status = job_status.get(\"status\", \"Unknown\")\n",
        "        \n",
        "        if current_status == \"Error\":\n",
        "            raise Exception(\"TRT Inference job failed!\")\n",
        "            \n",
        "        if current_status in [\"Done\", \"Completed\"]:\n",
        "            print(\"TRT Inference job completed successfully!\")\n",
        "            break\n",
        "            \n",
        "        if current_status in [\"Canceled\", \"Paused\"]:\n",
        "            print(f\"TRT Inference job {current_status}\")\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        if \"failed\" in str(e).lower():\n",
        "            raise\n",
        "        print(f\" Error fetching  job status: {str(e)}\")\n",
        "        print(\"Job might still be starting up...\")\n",
        "        \n",
        "    time.sleep(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Jobs<a class=\"anchor\" id=\"head-22\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete jobs instead of experiments\n",
        "# Delete all created jobs using TAO SDK\n",
        "\n",
        "print(\" Deleting all created jobs...\")\n",
        "\n",
        "jobs_to_delete = []\n",
        "for job_key, job_id in job_map.items():\n",
        "    try:\n",
        "        delete_result = tao_client.delete_job(job_id)\n",
        "        print(f\" Deleted job: {job_key} (ID: {job_id})\")\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to delete job {job_key} (ID: {job_id}): {str(e)}\")\n",
        "\n",
        "print(f\"\\n Job cleanup completed! Processed {len(jobs_to_delete)} jobs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete dataset <a class=\"anchor\" id=\"head-22\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete train dataset using TAO SDK\n",
        "try:\n",
        "    delete_result = tao_client.delete_dataset(train_dataset_id)\n",
        "    print(\"Train dataset deleted successfully!\")\n",
        "    print(f\"Dataset ID: {train_dataset_id}\")\n",
        "    if delete_result:\n",
        "        print(\"Delete Response:\")\n",
        "        print(json.dumps(delete_result, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\" Failed to delete train dataset {train_dataset_id}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete eval dataset using TAO SDK\n",
        "try:\n",
        "    delete_result = tao_client.delete_dataset(eval_dataset_id)\n",
        "    print(\"Eval dataset deleted successfully!\")\n",
        "    print(f\"Dataset ID: {eval_dataset_id}\")\n",
        "    if delete_result:\n",
        "        print(\"Delete Response:\")\n",
        "        print(json.dumps(delete_result, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\" Failed to delete eval dataset {eval_dataset_id}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete test dataset using TAO SDK\n",
        "try:\n",
        "    delete_result = tao_client.delete_dataset(test_dataset_id)\n",
        "    print(\"Test dataset deleted successfully!\")\n",
        "    print(f\"Dataset ID: {test_dataset_id}\")\n",
        "    if delete_result:\n",
        "        print(\"Delete Response:\")\n",
        "        print(json.dumps(delete_result, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\" Failed to delete test dataset {test_dataset_id}: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
