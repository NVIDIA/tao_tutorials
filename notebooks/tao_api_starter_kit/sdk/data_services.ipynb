{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to demonstrate Data-Services workflow\n",
    "### The workflow in a nutshell\n",
    "TAO Data Services include 4 key pipelines:\n",
    "1. Offline data augmentation using DALI\n",
    "2. Auto labeling using TAO Mask Auto-labeler (MAL)\n",
    "3. Annotation conversion\n",
    "4. Groundtruth analytics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Convert KITTI dataset to COCO format\n",
    "* Run auto-labeling to generate pseudo masks for KITTI bounding boxes\n",
    "* Apply data augmentation to the KITTI dataset with bounding boxe refinement\n",
    "* Run data analytics to collect useful statistics on the original and augmented KITTI dataset\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Create a cloud workspace](#head-2)\n",
    "1. [Convert KITTI data to COCO format](#head-1)\n",
    "1. [Generate pseudo-masks with the auto-labeler](#head-2)\n",
    "1. [Apply data augmentation](#head-3)\n",
    "1. [Perform data analytics](#head-4)\n",
    "1. [Perform data validation](#head-5)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import TAO SDK\n",
    "from tao_sdk.client import TaoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore variable in case of jupyter session restart and resume execution where it left off\n",
    "%store -r base_url\n",
    "%store -r headers\n",
    "%store -r workspace_id\n",
    "%store -r kitti_dataset_id\n",
    "%store -r coco_dataset_id\n",
    "%store -r coco_mask_dataset_id\n",
    "%store -r coco_mask_augmented_dataset_id\n",
    "%store -r job_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME's <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "1. Assign the ip_address and port_number in FIXME 1 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "1. Assign the ngc_key variable in FIXME 2\n",
    "1. Assign the ngc_org_name variable in FIXME 3\n",
    "1. Set cloud storage details in FIXME 4\n",
    "1. Assign path of kitti dataset relative to the bucket in FIXME 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set API service's host information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME 4: Set TAO API environment variables\n",
    "\n",
    "# Set to your TAO API endpoint\n",
    "os.environ[\"TAO_BASE_URL\"] = os.environ.get(\"TAO_BASE_URL\", \"https://your_tao_ip_address:port/api/v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set NGC Personal key for authentication and NGC org to access API services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME 5: Your NGC personal key\n",
    "os.environ[\"NGC_KEY\"] = ngc_key = os.environ.get(\"NGC_KEY\", \"your_ngc_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME 6: Your NGC ORG name\n",
    "os.environ[\"NGC_ORG\"] = ngc_org_name = os.environ.get(\"NGC_ORG\", \"nvstaging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TAO Client and login using SDK\n",
    "tao_client = TaoClient()\n",
    "\n",
    "# Login using TAO SDK - this will automatically save credentials to environment variables\n",
    "login_response = tao_client.login(\n",
    "    ngc_key=ngc_key,\n",
    "    ngc_org_name=ngc_org_name,\n",
    "    enable_telemetry=True\n",
    ")\n",
    "\n",
    "print(\"Login successful!\")\n",
    "print(\"JWT Token:\", tao_client.token)\n",
    "print(\"API Base URL:\", tao_client.base_url)\n",
    "print(\"Organization:\", tao_client.org_name)\n",
    "\n",
    "%store tao_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NVCF gpu details <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    " One of the keys of the response json are to be used as platform_id when you run each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Valid only for NVCF backend during TAO-API helm deployment currently\n",
    "# # Get available GPU types using TAO SDK\n",
    "# try:\n",
    "#     gpu_types = tao_client.get_gpu_types()\n",
    "#     print(\"Available GPU types:\")\n",
    "#     print(json.dumps(gpu_types, indent=4))\n",
    "# except Exception as e:\n",
    "#     print(\"Could not fetch GPU types (may not be available on this deployment):\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cloud workspace\n",
    "This workspace will be the place where your datasets reside and your results of TAO API jobs will be pushed to.\n",
    "\n",
    "If you want to have different workspaces for dataset and experiment, duplocate the workspace creation part and adjust the metadata accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME 7: Dataset Cloud bucket details to download dataset or push job artifacts for jobs\n",
    "\n",
    "cloud_metadata = {}\n",
    "\n",
    "# A Representative name for this cloud info\n",
    "os.environ[\"TAO_WORKSPACE_NAME\"] = cloud_metadata[\"name\"] = os.environ.get(\"TAO_WORKSPACE_NAME\", \"AWS workspace info\")\n",
    "\n",
    "# Cloud specific details. Below is assuming AWS.\n",
    "cloud_metadata[\"cloud_specific_details\"] = {}\n",
    "\n",
    " # Whether it is AWS, HuggingFace or Azure\n",
    "os.environ[\"TAO_WORKSPACE_CLOUD_TYPE\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_TYPE\", \"aws\")\n",
    "\n",
    "# Bucket region\n",
    "os.environ[\"TAO_WORKSPACE_CLOUD_REGION\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_REGION\", \"us-west-1\")\n",
    "\n",
    "# Bucket name\n",
    "os.environ[\"TAO_WORKSPACE_CLOUD_BUCKET_NAME\"] = cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_BUCKET_NAME\", \"bucket_name\")\n",
    "\n",
    "# Access and Secret keys\n",
    "os.environ[\"TAO_WORKSPACE_CLOUD_ACCESS_KEY\"] = cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_ACCESS_KEY\", \"access_key\")\n",
    "os.environ[\"TAO_WORKSPACE_CLOUD_SECRET_KEY\"] = cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = os.environ.get(\"TAO_WORKSPACE_CLOUD_SECRET_KEY\", \"secret_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloud workspace using TAO SDK\n",
    "workspace_id = tao_client.create_workspace(\n",
    "    name=cloud_metadata[\"name\"],\n",
    "    cloud_type=cloud_metadata[\"cloud_specific_details\"][\"cloud_type\"],\n",
    "    cloud_specific_details=cloud_metadata[\"cloud_specific_details\"]\n",
    ")\n",
    "\n",
    "print(\"Workspace created successfully!\")\n",
    "print(f\"Workspace ID: {workspace_id}\")\n",
    "\n",
    "# Get workspace details to confirm creation\n",
    "workspace_details = tao_client.get_workspace_metadata(workspace_id)\n",
    "print(\"Workspace details:\")\n",
    "print(json.dumps(workspace_details, indent=4))\n",
    "\n",
    "%store workspace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert KITTI data to COCO format <a class=\"anchor\" id=\"head-1\"></a>\n",
    "We would first convert the dataset from KITTI to COCO formats.\n",
    "\n",
    "### Create the dataset\n",
    "We support both KITTI and COCO data formats\n",
    "\n",
    "KITTI dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    " images\n",
    "   image_name_1.jpg\n",
    "   image_name_2.jpg\n",
    "| ...\n",
    " labels\n",
    " image_name_1.txt\n",
    " image_name_2.txt\n",
    " ...\n",
    "```\n",
    "\n",
    "And COCO dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    " images\n",
    "   image_name_1.jpg\n",
    "   image_name_2.jpg\n",
    "| ...\n",
    " annotations.json\n",
    "```\n",
    "For this notebook, we will be using the kitti object detection dataset for this example. To find more details, please visit [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME5 : Set path relative to cloud bucket\n",
    "os.environ[\"TAO_KITTI_DATASET_PATH\"] = kitti_dataset_path = os.environ.get(\"TAO_KITTI_DATASET_PATH\", \"/data/tao_od_synthetic_subset_train_convert_cleaned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset using TAO SDK\n",
    "kitti_dataset_id = tao_client.create_dataset(\n",
    "    dataset_type=\"object_detection\",\n",
    "    dataset_format=\"kitti\",\n",
    "    workspace_id=workspace_id,\n",
    "    cloud_file_path=kitti_dataset_path,\n",
    "    use_for=[\"testing\"]\n",
    ")\n",
    "\n",
    "print(\"Kitti dataset created successfully!\")\n",
    "print(f\"Kitti Dataset ID: {kitti_dataset_id}\")\n",
    "\n",
    "%store kitti_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check kitti dataset progress using TAO SDK\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    dataset_details = tao_client.get_dataset_metadata(kitti_dataset_id)\n",
    "    \n",
    "    print(f\"Kitti Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
    "    print(f\"Dataset ID: {kitti_dataset_id}\")\n",
    "    \n",
    "    if dataset_details.get(\"status\") == \"invalid_pull\":\n",
    "        print(\"Dataset pull failed!\")\n",
    "        validation_details = dataset_details.get(\"validation_details\", {})\n",
    "        if validation_details:\n",
    "            print(\"Validation details:\")\n",
    "            print(json.dumps(validation_details, indent=4))\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "        \n",
    "    if dataset_details.get(\"status\") == \"pull_complete\":\n",
    "        print(\"Kitti dataset pull completed successfully!\")\n",
    "        print(\"Dataset details:\")\n",
    "        print(json.dumps(dataset_details, indent=4))\n",
    "        break\n",
    "        \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset format conversion action \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default anootations conversion specs using TAO SDK\n",
    "annotations_conversion_spec_response = tao_client.get_job_schema(action=\"annotation_format_convert\", network_arch=\"object_detection\")\n",
    "annotations_conversion_specs = annotations_conversion_spec_response.get(\"default\", {})\n",
    "print(\"Default annotations conversion specifications:\")\n",
    "print(json.dumps(annotations_conversion_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating spec file\n",
    "annotations_conversion_specs[\"data\"][\"input_format\"] = \"KITTI\"\n",
    "annotations_conversion_specs[\"data\"][\"output_format\"] = \"COCO\"\n",
    "print(json.dumps(annotations_conversion_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"annotation_format_convert\"\n",
    "train_ds_convert_job_name = f\"annotations_convert_job\"\n",
    "\n",
    "train_ds_convert_job_id = tao_client.create_job(\n",
    "    kind=\"dataset\",\n",
    "    name=train_ds_convert_job_name,\n",
    "    network_arch=\"object_detection\",\n",
    "    workspace=workspace_id,\n",
    "    dataset_id=kitti_dataset_id,\n",
    "    action=action,\n",
    "    specs=annotations_conversion_specs,  # Pass as dict, not JSON string\n",
    "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
    ")\n",
    "\n",
    "print(\"Train Dataset Convert job created successfully!\")\n",
    "print(f\"Train Dataset Convert Job ID: {train_ds_convert_job_id}\")\n",
    "print(f\"Action: train dataset_convert\")\n",
    "\n",
    "job_map[\"annotations_convert_job\"] = train_ds_convert_job_id\n",
    "print(\"\\nUpdated Job Map:\")\n",
    "print(json.dumps(job_map, indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "coco_dataset_id = kitti_dataset_id\n",
    "%store coco_dataset_id\n",
    "annotations_convert_job_id = job_map[\"annotations_convert_job\"]\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        job_status = tao_client.get_job_metadata(annotations_convert_job_id)\n",
    "        \n",
    "        print(f\"Annotations Convert Job Status\")\n",
    "        print(f\"Job ID: {annotations_convert_job_id}\")\n",
    "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
    "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
    "        \n",
    "        # Show detailed status information\n",
    "        print(\"\\nDetailed Status:\")\n",
    "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
    "        \n",
    "        current_status = job_status.get(\"status\", \"Unknown\")\n",
    "        \n",
    "        if current_status == \"Error\":\n",
    "            raise Exception(\"Annotations Convert job failed!\")\n",
    "            \n",
    "        if current_status in [\"Done\", \"Completed\"]:\n",
    "            print(\"Annotations Convert job completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        if current_status in [\"Canceled\", \"Paused\"]:\n",
    "            print(f\"Annotations Convert job {current_status}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"failed\" in str(e).lower():\n",
    "            raise\n",
    "        print(f\" Error fetching inference job status: {str(e)}\")\n",
    "        print(\"Job might still be starting up...\")\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the action is completed the format of dataset will be converted to coco from kitti\n",
    "coco_dataset_details = tao_client.get_dataset_metadata(coco_dataset_id)\n",
    "print(json.dumps(coco_dataset_details, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate pseudo-masks with the auto-labeler <a class=\"anchor\" id=\"head-2\"></a>\n",
    "Here we will use a pretrained MAL model to generate pseudo-masks for the converted KITTI data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a coco Dataset - If you already have data in coco detection format(without masks) and skipped step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create train dataset using TAO SDK\n",
    "# coco_dataset_id = tao_client.create_dataset(\n",
    "#     dataset_type=\"object_detection\",\n",
    "#     dataset_format=\"coco\",\n",
    "#     workspace_id=workspace_id,\n",
    "#     cloud_file_path=coco_dataset_path,\n",
    "#     use_for=[\"testing\"]\n",
    "# )\n",
    "\n",
    "# print(\"Coco dataset created successfully!\")\n",
    "# print(f\"Coco Dataset ID: {coco_dataset_id}\")\n",
    "\n",
    "# %store coco_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check kitti dataset progress using TAO SDK\n",
    "# while True:\n",
    "#     clear_output(wait=True)\n",
    "#     dataset_details = tao_client.get_dataset_metadata(coco_dataset_id)\n",
    "    \n",
    "#     print(f\"Coco Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
    "#     print(f\"Dataset ID: {coco_dataset_id}\")\n",
    "    \n",
    "#     if dataset_details.get(\"status\") == \"invalid_pull\":\n",
    "#         print(\"Dataset pull failed!\")\n",
    "#         validation_details = dataset_details.get(\"validation_details\", {})\n",
    "#         if validation_details:\n",
    "#             print(\"Validation details:\")\n",
    "#             print(json.dumps(validation_details, indent=4))\n",
    "#         raise ValueError(\"Dataset pull failed\")\n",
    "        \n",
    "#     if dataset_details.get(\"status\") == \"pull_complete\":\n",
    "#         print(\"Coco dataset pull completed successfully!\")\n",
    "#         print(\"Dataset details:\")\n",
    "#         print(json.dumps(dataset_details, indent=4))\n",
    "#         break\n",
    "        \n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List base experiments (PTMs) using TAO SDK  \n",
    "# These are the pre-trained models available for the selected network architecture\n",
    "base_experiments = tao_client.list_base_experiments(filter_params={\"network_arch\": \"auto_label\"})\n",
    "\n",
    "print(f\" Available base experiments (PTMs) for auto_label:\")\n",
    "print(\"name\\t\\t\\t     model id\\t\\t\\t     network architecture\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for exp in base_experiments:\n",
    "    exp_name = exp.get(\"name\", \"N/A\")\n",
    "    exp_id = exp.get(\"id\", \"N/A\")\n",
    "    exp_arch = exp.get(\"network_arch\", \"N/A\")\n",
    "    print(f\"{exp_name}\\t{exp_id}\\t{exp_arch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_map = {\"auto_label\" : \"mask_auto_label:trainable_v1.1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get pretrained model using TAO SDK\n",
    "selected_ptm_id = None\n",
    "base_experiments_detailed = tao_client.list_base_experiments(filter_params={\"network_arch\": \"mal\"})\n",
    "\n",
    "# Search for PTM with given NGC path\n",
    "for exp in base_experiments_detailed:\n",
    "    ngc_path = exp.get(\"ngc_path\", \"\")\n",
    "    if ngc_path.endswith(pretrained_map[\"auto_label\"]):\n",
    "        selected_ptm_id = exp.get(\"id\")\n",
    "        print(\"Selected PTM metadata:\")\n",
    "        print(json.dumps(exp, indent=4))\n",
    "        break\n",
    "\n",
    "if not selected_ptm_id:\n",
    "    print(f\" PTM with NGC path ending in '{pretrained_map[\"auto_label\"]}' not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Selected PTM ID: {selected_ptm_id}\")\n",
    "\n",
    "if selected_ptm_id:\n",
    "    update_payload = {\"base_experiment_ids\": [selected_ptm_id]}\n",
    "    \n",
    "    # Assign PTM to dataset using TAO SDK\n",
    "    updated_dataset = tao_client.update_dataset_metadata(\n",
    "        coco_dataset_id, \n",
    "        update_payload\n",
    "    )\n",
    "    print(\"\\nUpdated dataset:\")\n",
    "    print(json.dumps(updated_dataset, indent=4))\n",
    "else:\n",
    "    print(\"ERROR: selected_ptm_id is None! The PTM was not found.\")\n",
    "    print(\"Make sure you ran the cell that searches for the PTM first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto labeling action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default anootations conversion specs using TAO SDK\n",
    "auto_label_spec_response = tao_client.get_job_schema(action=\"auto_label\", network_arch=\"object_detection\")\n",
    "auto_label_generate_specs = auto_label_spec_response.get(\"default\", {})\n",
    "print(\"Default annotations conversion specifications:\")\n",
    "print(json.dumps(auto_label_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Override any of the parameters listed in the previous cell as required\n",
    "auto_label_generate_specs[\"gpu_ids\"] = [0]\n",
    "print(json.dumps(auto_label_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"annotations_convert_job\"]\n",
    "action = \"auto_label\"\n",
    "auto_label_job_name = f\"auto_label_job\"\n",
    "\n",
    "auto_label_job_id = tao_client.create_job(\n",
    "    kind=\"dataset\",\n",
    "    name=auto_label_job_name,\n",
    "    network_arch=\"object_detection\",\n",
    "    workspace=workspace_id,\n",
    "    dataset_id=coco_dataset_id,\n",
    "    action=action,\n",
    "    parent_job_id=parent,\n",
    "    specs=auto_label_generate_specs,  # Pass as dict, not JSON string\n",
    "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
    ")\n",
    "\n",
    "print(\"Auto Labeling job created successfully!\")\n",
    "print(f\"Auto Labeling Job ID: {auto_label_job_id}\")\n",
    "print(f\"Action: auto_label\")\n",
    "\n",
    "job_map[\"auto_labeling\"] = auto_label_job_id\n",
    "print(\"\\nUpdated Job Map:\")\n",
    "print(json.dumps(job_map, indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "coco_mask_dataset_id = kitti_dataset_id\n",
    "%store coco_mask_dataset_id\n",
    "auto_labeling_job_id = job_map[\"auto_labeling\"]\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        job_status = tao_client.get_job_metadata(auto_labeling_job_id)\n",
    "        \n",
    "        print(f\"Auto Labeling Job Status\")\n",
    "        print(f\"Job ID: {auto_labeling_job_id}\")\n",
    "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
    "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
    "        \n",
    "        # Show detailed status information\n",
    "        print(\"\\nDetailed Status:\")\n",
    "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
    "        \n",
    "        current_status = job_status.get(\"status\", \"Unknown\")\n",
    "        \n",
    "        if current_status == \"Error\":\n",
    "            raise Exception(\"Auto Labeling job failed!\")\n",
    "            \n",
    "        if current_status in [\"Done\", \"Completed\"]:\n",
    "            print(\"Auto Labeling job completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        if current_status in [\"Canceled\", \"Paused\"]:\n",
    "            print(f\"Auto Labeling job {current_status}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"failed\" in str(e).lower():\n",
    "            raise\n",
    "        print(f\" Error fetching inference job status: {str(e)}\")\n",
    "        print(\"Job might still be starting up...\")\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply data augmentation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "In this section, we run offline augmentation with the original dataset. During the augmentation process, we can use the pseudo-masks generated from the last step to refine the distorted or rotated bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a coco mask Dataset - If you already have data in coco segmentation format and skipped step 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create train dataset using TAO SDK\n",
    "# coco_mask_dataset_id = tao_client.create_dataset(\n",
    "#     dataset_type=\"object_detection\",\n",
    "#     dataset_format=\"coco\",\n",
    "#     workspace_id=workspace_id,\n",
    "#     cloud_file_path=coco_mask_dataset_path,\n",
    "#     use_for=[\"testing\"]\n",
    "# )\n",
    "\n",
    "# print(\"Coco Mask dataset created successfully!\")\n",
    "# print(f\"Coco Mask Dataset ID: {coco_mask_dataset_id}\")\n",
    "\n",
    "# %store coco_mask_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check coco mask dataset progress using TAO SDK\n",
    "# while True:\n",
    "#     clear_output(wait=True)\n",
    "#     dataset_details = tao_client.get_dataset_metadata(coco_mask_dataset_id)\n",
    "    \n",
    "#     print(f\"Coco Mask Dataset Status: {dataset_details.get('status', 'Unknown')}\")\n",
    "#     print(f\"Dataset ID: {coco_mask_dataset_id}\")\n",
    "    \n",
    "#     if dataset_details.get(\"status\") == \"invalid_pull\":\n",
    "#         print(\"Dataset pull failed!\")\n",
    "#         validation_details = dataset_details.get(\"validation_details\", {})\n",
    "#         if validation_details:\n",
    "#             print(\"Validation details:\")\n",
    "#             print(json.dumps(validation_details, indent=4))\n",
    "#         raise ValueError(\"Dataset pull failed\")\n",
    "        \n",
    "#     if dataset_details.get(\"status\") == \"pull_complete\":\n",
    "#         print(\"Coco Mask dataset pull completed successfully!\")\n",
    "#         print(\"Dataset details:\")\n",
    "#         print(json.dumps(dataset_details, indent=4))\n",
    "#         break\n",
    "        \n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data augmentation action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default anootations conversion specs using TAO SDK\n",
    "augmentation_spec_response = tao_client.get_job_schema(action=\"augment\", network_arch=\"object_detection\")\n",
    "augmentation_generate_specs = augmentation_spec_response.get(\"default\", {})\n",
    "print(\"Default data augmentation specifications:\")\n",
    "print(json.dumps(augmentation_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary\n",
    "print(json.dumps(augmentation_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"auto_labeling\"]\n",
    "action = \"augment\"\n",
    "augment_job_name = f\"augment_job\"\n",
    "\n",
    "augment_job = tao_client.create_job(\n",
    "    kind=\"dataset\",\n",
    "    name=augment_job_name,\n",
    "    network_arch=\"object_detection\",\n",
    "    workspace=workspace_id,\n",
    "    dataset_id=coco_mask_dataset_id,\n",
    "    action=action,\n",
    "    parent_job_id=parent,\n",
    "    specs=auto_label_generate_specs,  # Pass as dict, not JSON string\n",
    "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
    ")\n",
    "\n",
    "print(\"Augmentation job created successfully!\")\n",
    "print(f\"Augmentation Job ID: {augment_job}\")\n",
    "print(f\"Action: augment\")\n",
    "\n",
    "job_map[\"augment\"] = augment_job\n",
    "print(\"\\nUpdated Job Map:\")\n",
    "print(json.dumps(job_map, indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "augment_job_id = job_map[\"augment\"]\n",
    "coco_mask_augmented_dataset_id = augment_job_id\n",
    "%store coco_mask_augmented_dataset_id\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        job_status = tao_client.get_job_metadata(augment_job_id)\n",
    "        \n",
    "        print(f\"Augmentation Job Status\")\n",
    "        print(f\"Job ID: {augment_job_id}\")\n",
    "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
    "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
    "        \n",
    "        # Show detailed status information\n",
    "        print(\"\\nDetailed Status:\")\n",
    "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
    "        \n",
    "        current_status = job_status.get(\"status\", \"Unknown\")\n",
    "        \n",
    "        if current_status == \"Error\":\n",
    "            raise Exception(\"Augmentation job failed!\")\n",
    "            \n",
    "        if current_status in [\"Done\", \"Completed\"]:\n",
    "            print(\"Augmentation job completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        if current_status in [\"Canceled\", \"Paused\"]:\n",
    "            print(f\"Augmentation job {current_status}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"failed\" in str(e).lower():\n",
    "            raise\n",
    "        print(f\" Error fetching inference job status: {str(e)}\")\n",
    "        print(\"Job might still be starting up...\")\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform data analytics  <a class=\"anchor\" id=\"head-4\"></a>\n",
    "Next, we perform analytics with the KITTI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data analytics annotation analytics action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default anootations conversion specs using TAO SDK\n",
    "analytics_spec_response = tao_client.get_job_schema(action=\"analyze\", network_arch=\"object_detection\")\n",
    "analytics_analyze_specs = analytics_spec_response.get(\"default\", {})\n",
    "print(\"Default data analytics specifications:\")\n",
    "print(json.dumps(analytics_analyze_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary\n",
    "analytics_analyze_specs[\"data\"][\"input_format\"] = \"COCO\"\n",
    "print(json.dumps(analytics_analyze_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"annotations_convert_job\"]\n",
    "action = \"analyze\"\n",
    "analytics_job_name = f\"analytics_job\"\n",
    "\n",
    "analytics_job = tao_client.create_job(\n",
    "    kind=\"dataset\",\n",
    "    name=analytics_job_name,\n",
    "    network_arch=\"object_detection\",\n",
    "    workspace=workspace_id,\n",
    "    dataset_id=kitti_dataset_id,\n",
    "    action=action,\n",
    "    parent_job_id=parent,\n",
    "    specs=auto_label_generate_specs,  # Pass as dict, not JSON string\n",
    "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
    ")\n",
    "\n",
    "print(\"Analytics job created successfully!\")\n",
    "print(f\"Analytics Job ID: {analytics_job}\")\n",
    "print(f\"Action: analyze\")\n",
    "\n",
    "job_map[\"analytics\"] = analytics_job\n",
    "print(\"\\nUpdated Job Map:\")\n",
    "print(json.dumps(job_map, indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "analytics_job_id = job_map[\"analytics\"]\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        job_status = tao_client.get_job_metadata(analytics_job_id)\n",
    "        \n",
    "        print(f\"Analytics Job Status\")\n",
    "        print(f\"Job ID: {analytics_job_id}\")\n",
    "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
    "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
    "        \n",
    "        # Show detailed status information\n",
    "        print(\"\\nDetailed Status:\")\n",
    "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
    "        \n",
    "        current_status = job_status.get(\"status\", \"Unknown\")\n",
    "        \n",
    "        if current_status == \"Error\":\n",
    "            raise Exception(\"Analytics job failed!\")\n",
    "            \n",
    "        if current_status in [\"Done\", \"Completed\"]:\n",
    "            print(\"Analytics job completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        if current_status in [\"Canceled\", \"Paused\"]:\n",
    "            print(f\"Analytics job {current_status}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"failed\" in str(e).lower():\n",
    "            raise\n",
    "        print(f\" Error fetching inference job status: {str(e)}\")\n",
    "        print(\"Job might still be starting up...\")\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform data validation  <a class=\"anchor\" id=\"head-5\"></a>\n",
    "Next, we perform validate the annotations and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data annotation validation action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default anootations conversion specs using TAO SDK\n",
    "validate_annotation_spec_response = tao_client.get_job_schema(action=\"validate_annotations\", network_arch=\"object_detection\")\n",
    "validate_annotation_specs = validate_annotation_spec_response.get(\"default\", {})\n",
    "print(\"Default validate annotation specifications:\")\n",
    "print(json.dumps(validate_annotation_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary\n",
    "validate_annotation_specs[\"data\"][\"input_format\"] = \"COCO\"\n",
    "print(json.dumps(validate_annotation_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"annotations_convert_job\"]\n",
    "action = \"validate_annotations\"\n",
    "data_validate_job_name = f\"validate_job\"\n",
    "\n",
    "validate_job = tao_client.create_job(\n",
    "    kind=\"dataset\",\n",
    "    name=data_validate_job_name,\n",
    "    network_arch=\"object_detection\",\n",
    "    workspace=workspace_id,\n",
    "    dataset_id=kitti_dataset_id,\n",
    "    action=action,\n",
    "    parent_job_id=parent,\n",
    "    specs=auto_label_generate_specs,  # Pass as dict, not JSON string\n",
    "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
    ")\n",
    "\n",
    "print(\"Data validation job created successfully!\")\n",
    "print(f\"Data validation Job ID: {validate_job}\")\n",
    "print(f\"Action: validate\")\n",
    "\n",
    "job_map[\"data_validation\"] = validate_job\n",
    "print(\"\\nUpdated Job Map:\")\n",
    "print(json.dumps(job_map, indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "data_validate_job_id = job_map[\"data_validation\"]\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        job_status = tao_client.get_job_metadata(data_validate_job_id)\n",
    "        \n",
    "        print(f\"Data validation Job Status\")\n",
    "        print(f\"Job ID: {data_validate_job_id}\")\n",
    "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
    "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
    "        \n",
    "        # Show detailed status information\n",
    "        print(\"\\nDetailed Status:\")\n",
    "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
    "        \n",
    "        current_status = job_status.get(\"status\", \"Unknown\")\n",
    "        \n",
    "        if current_status == \"Error\":\n",
    "            raise Exception(\"Data validation job failed!\")\n",
    "            \n",
    "        if current_status in [\"Done\", \"Completed\"]:\n",
    "            print(\"Data validation job completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        if current_status in [\"Canceled\", \"Paused\"]:\n",
    "            print(f\"Data validation job {current_status}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"failed\" in str(e).lower():\n",
    "            raise\n",
    "        print(f\" Error fetching inference job status: {str(e)}\")\n",
    "        print(\"Job might still be starting up...\")\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data image validation action - removes corrupted images and creates a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default anootations conversion specs using TAO SDK\n",
    "validate_images_spec_response = tao_client.get_job_schema(action=\"validate_images\", network_arch=\"object_detection\")\n",
    "validate_images_specs = validate_images_spec_response.get(\"default\", {})\n",
    "print(\"Default validate images specifications:\")\n",
    "print(json.dumps(validate_images_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "action = \"validate_images\"\n",
    "validate_images_job_name = f\"validate_images_job\"\n",
    "\n",
    "validate_images_job = tao_client.create_job(\n",
    "    kind=\"dataset\",\n",
    "    name=validate_images_job_name,\n",
    "    network_arch=\"object_detection\",\n",
    "    workspace=workspace_id,\n",
    "    dataset_id=kitti_dataset_id,\n",
    "    action=action,\n",
    "    specs=validate_images_specs,  # Pass as dict, not JSON string\n",
    "    # platform_id=\"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Optional: Pick from get_gpu_types output\n",
    ")\n",
    "\n",
    "print(\"Validate images job created successfully!\")\n",
    "print(f\"Validate images Job ID: {validate_images_job}\")\n",
    "print(f\"Action: validate_images\")\n",
    "\n",
    "job_map[\"validate_images\"] = validate_images_job\n",
    "print(\"\\nUpdated Job Map:\")\n",
    "print(json.dumps(job_map, indent=4))\n",
    "%store job_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "validate_images_job_id = job_map[\"validate_images\"]\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        job_status = tao_client.get_job_metadata(validate_images_job_id)\n",
    "        \n",
    "        print(f\"Validate images Job Status\")\n",
    "        print(f\"Job ID: {validate_images_job_id}\")\n",
    "        print(f\"Status: {job_status.get('status', 'Unknown')}\")\n",
    "        print(f\"Progress: {job_status.get('progress', 'N/A')}\")\n",
    "        \n",
    "        # Show detailed status information\n",
    "        print(\"\\nDetailed Status:\")\n",
    "        print(json.dumps(job_status.get(\"job_details\", {}), sort_keys=True, indent=4))\n",
    "        \n",
    "        current_status = job_status.get(\"status\", \"Unknown\")\n",
    "        \n",
    "        if current_status == \"Error\":\n",
    "            raise Exception(\"Validate images job failed!\")\n",
    "            \n",
    "        if current_status in [\"Done\", \"Completed\"]:\n",
    "            print(\"Validate images job completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        if current_status in [\"Canceled\", \"Paused\"]:\n",
    "            print(f\"Validate images job {current_status}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"failed\" in str(e).lower():\n",
    "            raise\n",
    "        print(f\" Error fetching inference job status: {str(e)}\")\n",
    "        print(\"Job might still be starting up...\")\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Jobs<a class=\"anchor\" id=\"head-22\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete jobs instead of experiments\n",
    "# Delete all created jobs using TAO SDK\n",
    "\n",
    "print(\" Deleting all created jobs...\")\n",
    "\n",
    "jobs_to_delete = []\n",
    "for job_key, job_id in job_map.items():\n",
    "    try:\n",
    "        delete_result = tao_client.delete_job(job_id)\n",
    "        print(f\" Deleted job: {job_key} (ID: {job_id})\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to delete job {job_key} (ID: {job_id}): {str(e)}\")\n",
    "\n",
    "print(f\"\\n Job cleanup completed! Processed {len(jobs_to_delete)} jobs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete original kitti dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete kitti dataset using TAO SDK\n",
    "try:\n",
    "    delete_result = tao_client.delete_dataset(kitti_dataset_id)\n",
    "    print(\"Kitti dataset deleted successfully!\")\n",
    "    print(f\"Dataset ID: {kitti_dataset_id}\")\n",
    "    if delete_result:\n",
    "        print(\"Delete Response:\")\n",
    "        print(json.dumps(delete_result, indent=4))\n",
    "except Exception as e:\n",
    "    print(f\" Failed to delete kitti dataset {kitti_dataset_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete coco augment dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete coco augmented dataset using TAO SDK\n",
    "try:\n",
    "    delete_result = tao_client.delete_dataset(coco_mask_augmented_dataset_id)\n",
    "    print(\"Coco mask dataset deleted successfully!\")\n",
    "    print(f\"Dataset ID: {coco_mask_augmented_dataset_id}\")\n",
    "    if delete_result:\n",
    "        print(\"Delete Response:\")\n",
    "        print(json.dumps(delete_result, indent=4))\n",
    "except Exception as e:\n",
    "    print(f\" Failed to delete train dataset {coco_dataset_id}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
