{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to demonstrate Data-Services workflow\n",
    "### The workflow in a nutshell\n",
    "TAO Data Services include 4 key pipelines:\n",
    "1. Offline data augmentation using DALI\n",
    "2. Auto labeling using TAO Mask Auto-labeler (MAL)\n",
    "3. Annotation conversion\n",
    "4. Groundtruth analytics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Convert KITTI dataset to COCO format\n",
    "* Run auto-labeling to generate pseudo masks for KITTI bounding boxes\n",
    "* Apply data augmentation to the KITTI dataset with bounding boxe refinement\n",
    "* Run data analytics to collect useful statistics on the original and augmented KITTI dataset\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Create a cloud workspace](#head-2)\n",
    "1. [Convert KITTI data to COCO format](#head-1)\n",
    "1. [Generate pseudo-masks with the auto-labeler](#head-2)\n",
    "1. [Apply data augmentation](#head-3)\n",
    "1. [Perform data analytics](#head-4)\n",
    "1. [Perform data validation](#head-5)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME's <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "1. Assign the ip_address and port_number in FIXME 1 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "1. Assign the ngc_key variable in FIXME 2\n",
    "1. Assign the ngc_org_name variable in FIXME 3\n",
    "1. Set cloud storage details in FIXME 4\n",
    "1. Assign path of kitti dataset relative to the bucket in FIXME 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set API service's host information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = \"http://<ip_address>:<port_number>\" # FIXME1 example: https://10.137.149.22:32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service tao-api-ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set NGC Personal key for authentication and NGC org to access API services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = \"<ngc_key>\" # FIXME2 example: (Add NGC Personal key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_org_name = \"ea-tlt\" # FIXME3 your NGC ORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate NGC_PERSONAL_KEY\n",
    "data = json.dumps({\"ngc_org_name\": ngc_org_name,\n",
    "                   \"ngc_key\": ngc_key,\n",
    "                   \"enable_telemetry\": True})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"token\" in response.json().keys()\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/orgs/{ngc_org_name}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NVCF gpu details <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    " One of the keys of the response json are to be used as platform_id when you run each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Valid only for NVCF backend during TAO-API helm deployment currently\n",
    "# endpoint = f\"{base_url}:gpu_types\"\n",
    "# response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "# assert response.ok\n",
    "# print(response)\n",
    "# print((json.dumps(response.json(), indent=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert KITTI data to COCO format <a class=\"anchor\" id=\"head-1\"></a>\n",
    "We would first convert the dataset from KITTI to COCO formats.\n",
    "\n",
    "### Create the dataset\n",
    "We support both KITTI and COCO data formats\n",
    "\n",
    "KITTI dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   ├── ...\n",
    "└── labels\n",
    "    ├── image_name_1.txt\n",
    "    ├── image_name_2.txt\n",
    "    ├── ...\n",
    "```\n",
    "\n",
    "And COCO dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   ├── ...\n",
    "└── annotations.json\n",
    "```\n",
    "For this notebook, we will be using the kitti object detection dataset for this example. To find more details, please visit [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set cloud details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME4 Dataset Cloud bucket details to download dataset for experiments (Can be read only)\n",
    "cloud_metadata = {}\n",
    "cloud_metadata[\"name\"] = \"AWS workspace info\"  # A Representative name for this cloud info\n",
    "cloud_metadata[\"cloud_type\"] = \"aws\"  # If it's AWS, HuggingFace or Azure\n",
    "cloud_metadata[\"cloud_specific_details\"] = {}\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_region\"] = \"us-west-1\"  # Bucket region\n",
    "cloud_metadata[\"cloud_specific_details\"][\"cloud_bucket_name\"] = \"\"  # Bucket name\n",
    "# Access and Secret for AWS\n",
    "cloud_metadata[\"cloud_specific_details\"][\"access_key\"] = \"\"\n",
    "cloud_metadata[\"cloud_specific_details\"][\"secret_key\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloud workspace\n",
    "data = json.dumps(cloud_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/workspaces\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "assert \"id\" in response.json().keys()\n",
    "workspace_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a kitti Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME5 : Set path relative to cloud bucket\n",
    "kitti_dataset_path =  \"/data/tao_od_synthetic_subset_train_convert_cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset_metadata = {\"type\": \"object_detection\",\n",
    "                    \"format\": \"kitti\",\n",
    "                    \"workspace\": workspace_id,\n",
    "                    \"cloud_file_path\": kitti_dataset_path,\n",
    "                    \"use_for\": [\"testing\"]\n",
    "                   }\n",
    "data = json.dumps(dataset_metadata)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"id\" in response.json().keys()\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "kitti_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "    if response.json().get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.json().get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset format conversion action \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/specs/annotation_format_convert/schema\"\n",
    " \n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "annotations_conversion_specs = response.json()[\"default\"]\n",
    "print(json.dumps(annotations_conversion_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updating spec file\n",
    "annotations_conversion_specs[\"data\"][\"input_format\"] = \"KITTI\"\n",
    "annotations_conversion_specs[\"data\"][\"output_format\"] = \"COCO\"\n",
    "print(json.dumps(annotations_conversion_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "data = json.dumps({\"parent_job_id\":parent, \"action\":\"annotation_format_convert\", \"specs\":annotations_conversion_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"convert\"] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "coco_dataset_id = kitti_dataset_id\n",
    "convert_job_id = job_map[\"convert\"]\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs/{convert_job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue        \n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the action is completed the format of dataset will be converted to coco from kitti\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate pseudo-masks with the auto-labeler <a class=\"anchor\" id=\"head-2\"></a>\n",
    "Here we will use a pretrained MAL model to generate pseudo-masks for the converted KITTI data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a coco Dataset - If you already have data in coco detection format(without masks) and skipped step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dataset\n",
    "# dataset_metadata = {\"type\": \"object_detection\",\n",
    "#                     \"format\": \"coco\",\n",
    "#                     \"workspace\": workspace_id,\n",
    "#                     \"cloud_file_path\": coco_dataset_path\n",
    "#                     \"use_for\": [\"testing\"]\n",
    "#                     }\n",
    "# data = json.dumps(dataset_metadata)\n",
    "\n",
    "# endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "# response = requests.post(endpoint, data=data, headers=headers)\n",
    "# assert response.status_code in (200, 201)\n",
    "# assert \"id\" in response.json().keys()\n",
    "\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))\n",
    "# coco_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check progress\n",
    "# endpoint = f\"{base_url}/datasets/{coco_dataset_id}\"\n",
    "\n",
    "# while True:\n",
    "#     clear_output(wait=True)\n",
    "#     response = requests.get(endpoint, headers=headers)\n",
    "#     assert response.status_code in (200, 201)\n",
    "\n",
    "#     print(response)\n",
    "#     print(json.dumps(response.json(), indent=4))\n",
    "#     if response.json().get(\"status\") == \"invalid_pull\":\n",
    "#         raise ValueError(\"Dataset pull failed\")\n",
    "#     if response.json().get(\"status\") == \"pull_complete\":\n",
    "#         break\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List models\n",
    "endpoint = f\"{base_url}/experiments:base\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(\"model id\\t\\t\\t     network architecture\")\n",
    "for rsp in response.json()[\"experiments\"]:\n",
    "    if rsp[\"name\"] == \"Mask Auto Label\":\n",
    "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_map = {\"auto_label\" : \"mask_auto_label:trainable_v1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get pretrained model\n",
    "model_list = f\"{base_url}/experiments:base\"\n",
    "response = requests.get(model_list, headers=headers)\n",
    "\n",
    "response_json = response.json()[\"experiments\"]\n",
    "\n",
    "# Search for ptm with given ngc path\n",
    "ptm = []\n",
    "for rsp in response_json:\n",
    "    if rsp[\"ngc_path\"].endswith(pretrained_map[\"auto_label\"]):\n",
    "        ptm_id = rsp[\"id\"]\n",
    "        ptm = [ptm_id]\n",
    "        print(\"Metadata for model with requested NGC Path\")\n",
    "        print(rsp)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign PTM\n",
    "dataset_information = {\"base_experiment\": ptm}\n",
    "\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto labeling action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}/specs/auto_label/schema\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        if \"Base spec file download state is \" in response.json()[\"error_desc\"]:\n",
    "            print(\"Base experiment spec file is being downloaded\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(response)\n",
    "auto_label_generate_specs = response.json()[\"default\"]\n",
    "print(json.dumps(auto_label_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Override any of the parameters listed in the previous cell as required\n",
    "auto_label_generate_specs[\"gpu_ids\"] = [0]\n",
    "print(json.dumps(auto_label_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = convert_job_id\n",
    "\n",
    "data = json.dumps({\"parent_job_id\": parent, \"action\":\"auto_label\", \"specs\":auto_label_generate_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"auto_labeling\"] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "coco_mask_dataset_id = kitti_dataset_id\n",
    "auto_labeling_job_id = job_map[\"auto_labeling\"]\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}/jobs/{auto_labeling_job_id}\"\n",
    "\n",
    "while True: \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply data augmentation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "In this section, we run offline augmentation with the original dataset. During the augmentation process, we can use the pseudo-masks generated from the last step to refine the distorted or rotated bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a coco mask Dataset - If you already have data in coco segmentation format and skipped step 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dataset\n",
    "# dataset_metadata = {\"type\": \"object_detection\",\n",
    "#                     \"format\": \"coco\",\n",
    "#                     \"workspace\": workspace_id,\n",
    "#                     \"cloud_file_path\": coco_mask_dataset_path\n",
    "#                     \"use_for\": [\"testing\"]\n",
    "#                     }\n",
    "# data = json.dumps(dataset_metadata)\n",
    "\n",
    "# endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "# response = requests.post(endpoint, data=data, headers=headers)\n",
    "# assert response.status_code in (200, 201)\n",
    "# assert \"id\" in response.json().keys()\n",
    "\n",
    "# print(response)\n",
    "# print(json.dumps(response.json(), indent=4))\n",
    "# coco_mask_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check progress\n",
    "# endpoint = f\"{base_url}/datasets/{coco_mask_dataset_id}\"\n",
    "\n",
    "# while True:\n",
    "#     clear_output(wait=True)\n",
    "#     response = requests.get(endpoint, headers=headers)\n",
    "#     assert response.status_code in (200, 201)\n",
    "\n",
    "#     print(response)\n",
    "#     print(json.dumps(response.json(), indent=4))\n",
    "#     if response.json().get(\"status\") == \"invalid_pull\":\n",
    "#         raise ValueError(\"Dataset pull failed\")\n",
    "#     if response.json().get(\"status\") == \"pull_complete\":\n",
    "#         break\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data augmentation action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{coco_mask_dataset_id}/specs/augment/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "augmentation_generate_specs = response.json()[\"default\"]\n",
    "print(json.dumps(augmentation_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary\n",
    "print(json.dumps(augmentation_generate_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = auto_labeling_job_id\n",
    "\n",
    "data = json.dumps({\"parent_job_id\":parent, \"action\":\"augment\", \"specs\":augmentation_generate_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{coco_mask_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"augmentation\"] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "coco_mask_augmented_dataset_id = job_map[\"augmentation\"]\n",
    "endpoint = f\"{base_url}/datasets/{coco_mask_dataset_id}/jobs/{coco_mask_augmented_dataset_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the augment action you'll get a new dataset\n",
    "endpoint = f\"{base_url}/datasets/{coco_mask_augmented_dataset_id}\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform data analytics  <a class=\"anchor\" id=\"head-4\"></a>\n",
    "Next, we perform analytics with the KITTI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data analytics annotation analytics action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}/specs/analyze/schema\"\n",
    " \n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "analytics_analyze_specs = response.json()[\"default\"]\n",
    "print(json.dumps(analytics_analyze_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary\n",
    "analytics_analyze_specs[\"data\"][\"input_format\"] = \"COCO\"\n",
    "print(json.dumps(analytics_analyze_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = convert_job_id\n",
    "\n",
    "data = json.dumps({\"parent_job_id\":parent, \"action\":\"analyze\", \"specs\":analytics_analyze_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"analytics\"] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"analytics\"]\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform data validation  <a class=\"anchor\" id=\"head-5\"></a>\n",
    "Next, we perform validate the annotations and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data annotation validation action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{coco_dataset_id}/specs/validate_annotations/schema\"\n",
    " \n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "validate_annotation_specs = response.json()[\"default\"]\n",
    "print(json.dumps(validate_annotation_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary\n",
    "validate_annotation_specs[\"data\"][\"input_format\"] = \"COCO\"\n",
    "print(json.dumps(validate_annotation_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = convert_job_id\n",
    "\n",
    "data = json.dumps({\"parent_job_id\":parent, \"action\":\"validate_annotations\", \"specs\":validate_annotation_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"validate_annotations\"] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"validate_annotations\"]\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data image validation action - removes corrupted images and creates a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/specs/validate_images/schema\"\n",
    " \n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "validate_images_specs = response.json()[\"default\"]\n",
    "print(json.dumps(validate_images_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "\n",
    "data = json.dumps({\"parent_job_id\":parent, \"action\":\"validate_images\", \"specs\":validate_images_specs,\n",
    "                  #  \"platform_id\": \"9af1aa90-8ea5-5a11-98d9-3879cd0da92c\",  # Pick a platform_from output of {base_url}:gpu_types depending on GPU_type and instance_type\n",
    "                   })\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))\n",
    "\n",
    "job_map[\"validate_images\"] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"validate_images\"]\n",
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\", \"Paused\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete original kitti dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{kitti_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete coco augment dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{coco_mask_augmented_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
