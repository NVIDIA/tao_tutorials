{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAO remote client - Data-Services\n",
    "### The workflow in a nutshell\n",
    "TAO Data Services include 4 key pipelines:\n",
    "1. Offline data augmentation using DALI\n",
    "2. Auto labeling using TAO Mask Auto-labeler (MAL)\n",
    "3. Annotation conversion\n",
    "4. Groundtruth analytics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Convert KITTI dataset to COCO format\n",
    "* Run auto-labeling to generate pseudo masks for KITTI bounding boxes\n",
    "* Apply data augmentation to the KITTI dataset with bounding boxe refinement\n",
    "* Run data analytics to collect useful statistics on the original and augmented KITTI dataset\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Create a cloud workspace](#head-2)\n",
    "1. [Convert KITTI data to COCO format](#head-1)\n",
    "1. [Generate pseudo-masks with the auto-labeler](#head-2)\n",
    "1. [Apply data augmentation](#head-3)\n",
    "1. [Perform data analytics](#head-4)\n",
    "1. [Perform data validation](#head-5)\n",
    "\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install TAO remote client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # SKIP this step IF you have already installed the TAO-Client wheel.\n",
    "! pip3 install nvidia-tao-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # View the version of the TAO-Client\n",
    "! tao-client --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "namespace = 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME's <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "1. Assign a workdir in FIXME 1\n",
    "1. Assign the ip_address and port_number in FIXME 2 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "1. Assign the ngc_key variable in FIXME 3\n",
    "1. Assign the ngc_org_name variable in FIXME 4\n",
    "1. Set cloud storage details in FIXME 5\n",
    "1. Assign path of kitti dataset relative to the bucket in FIXME 6\n",
    "1. Database backup/restore archive filename in FIXME 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"workdir_data_services\" # FIXME1\n",
    "# Creating workdir\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set API service's host information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = \"http://<ip_address>:<port_number>\" # FIXME2 example: https://10.137.149.22:32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service tao-api-ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set NGC Personal key for authentication and NGC org to access API services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = \"<ngc_key>\" # FIXME3 example: (Add NGC Personal key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_org_name = \"ea-tlt\" # FIXME4 your NGC ORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login <a class=\"anchor\" id=\"head-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BASE_URL={host_url}/{namespace}/api/v1\n",
    "\n",
    "# Exchange NGC_API_KEY for JWT\n",
    "identity = json.loads(subprocess.getoutput(f\"tao-client login --ngc-key {ngc_key} --ngc-org-name {ngc_org_name} --enable-telemetry\"))\n",
    "\n",
    "%env USER={identity['user_id']}\n",
    "%env TOKEN={identity['token']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NVCF gpu details <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    " One of the keys of the response json are to be used as platform_id when you run each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Valid only for NVCF backend during TAO-API helm deployment currently\n",
    "# response = json.loads(subprocess.getoutput(f'tao get-gpu-types'))\n",
    "# print((json.dumps(response, indent=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cloud workspace\n",
    "This workspace will be the place where your datasets reside and your results of TAO API jobs will be pushed to.\n",
    "\n",
    "If you want to have different workspaces for dataset and experiment, duplocate the workspace creation part and adjust the metadata accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME5 Dataset Cloud bucket details to download dataset for experiments (Can be read only)\n",
    "workspace_name = \"AWS workspace info\"  # A Representative name for this cloud info\n",
    "cloud_type = \"aws\"  # If it's AWS, HuggingFace or Azure\n",
    "\n",
    "cloud_metadata = {}\n",
    "cloud_metadata[\"cloud_region\"] = \"us-west-1\"  # Bucket region\n",
    "cloud_metadata[\"cloud_bucket_name\"] = \"\"  # Bucket name\n",
    "# Access and Secret for AWS\n",
    "cloud_metadata[\"access_key\"] = \"\"\n",
    "cloud_metadata[\"secret_key\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_id = subprocess.getoutput(f\"tao-client annotations workspace-create --name '{workspace_name}' --cloud_type {cloud_type} --cloud_details '{json.dumps(cloud_metadata)}'\")\n",
    "print(workspace_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Optional: Restore database with a mongodump file saved in workspace dump/archive/{backup_filename}\n",
    "# backup_file_name = \"mongodump.tar.gz\" # FIXME 7\n",
    "# response = subprocess.getoutput(f\"tao-client {model_name} workspace-restore --workspace {workspace_id} --backup_file_name {backup_file_name}\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to parse logs <a class=\"anchor\" id=\"head-1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tail(model_name_cli, id, job_id, job_type, workdir):\n",
    "\tstatus = None\n",
    "\twhile True:\n",
    "\t\ttime.sleep(10)\n",
    "\t\tclear_output(wait=True)\n",
    "\t\tresponse = subprocess.getoutput(f\"tao-client {model_name_cli} get-action-status --job_type {job_type} --id {id} --job {job_id}\")\n",
    "\t\tresponse = json.loads(response)\n",
    "\t\tif response and \"status\" in response.keys() and response.get(\"status\") in (\"Done\", \"Error\", \"Canceled\", \"Paused\"):\n",
    "\t\t\tprint(json.dumps(response, indent=4))\n",
    "\t\t\tstatus = response.get(\"status\")\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tlogs = subprocess.getoutput(f\"tao-client {model_name_cli} get-job-logs --id {id} --job {job_id} --job_type {job_type} --workdir {workdir}\")\n",
    "\t\tif not logs:\n",
    "\t\t\tcontinue\n",
    "\t\tlog_content_lines = logs.split(\"\\n\")        \n",
    "\t\tfor line in log_content_lines:\n",
    "\t\t\tprint(line.strip())\n",
    "\t\t\tif line.strip() == \"Error EOF\":\n",
    "\t\t\t\tstatus = \"Error\"\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif line.strip() == \"Done EOF\":\n",
    "\t\t\t\tstatus = \"Done\"\n",
    "\t\t\t\tbreak\n",
    "\t\tif status is not None:\n",
    "\t\t\tbreak\n",
    "\treturn status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert KITTI data to COCO format <a class=\"anchor\" id=\"head-1\"></a>\n",
    "We would first convert the dataset from KITTI to COCO formats.\n",
    "\n",
    "### Define the task and action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "We support both KITTI and COCO data formats\n",
    "\n",
    "KITTI dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   ├── ...\n",
    "└── labels\n",
    "    ├── image_name_1.txt\n",
    "    ├── image_name_2.txt\n",
    "    ├── ...\n",
    "```\n",
    "\n",
    "And COCO dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   ├── ...\n",
    "└── annotations.json\n",
    "```\n",
    "For this notebook, we will be using the KITTI object detection dataset for this example. To find more details, please visit [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a kitti Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME6 : Set path relative to cloud bucket\n",
    "kitti_dataset_path =  \"/data/tao_od_synthetic_subset_train_convert_cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "model_name = \"annotations\"\n",
    "kitti_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format kitti --workspace {workspace_id} --cloud_file_path {kitti_dataset_path} --use_for '{json.dumps(['testing'])}'\")\n",
    "print(kitti_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {kitti_dataset_id} --job_type dataset\")\n",
    "    response = json.loads(response)\n",
    "    print(json.dumps(response, sort_keys=True, indent=4))\n",
    "    if response.get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset format conversion action \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "annotation_conversion_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action annotation_format_convert --job_type dataset --id {kitti_dataset_id}\")\n",
    "annotation_conversion_specs = json.loads(annotation_conversion_specs)\n",
    "print(json.dumps(annotation_conversion_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "annotation_conversion_specs[\"data\"][\"input_format\"] = \"KITTI\"\n",
    "annotation_conversion_specs[\"data\"][\"output_format\"] = \"COCO\"\n",
    "print(json.dumps(annotation_conversion_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "# Run action\n",
    "coco_dataset_id = kitti_dataset_id\n",
    "convert_job_id = subprocess.getoutput(f\"tao-client {model_name} dataset-run-action --id {kitti_dataset_id} --action annotation_format_convert --specs '{json.dumps(annotation_conversion_specs)}'\")\n",
    "print(convert_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, kitti_dataset_id, convert_job_id, \"dataset\", workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the action is completed the format of dataset will be converted to coco from kitti\n",
    "print(subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {kitti_dataset_id} --job_type dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate pseudo-masks with the auto-labeler <a class=\"anchor\" id=\"head-2\"></a>\n",
    "Here we will use a pretrained MAL model to generate pseudo-masks for the converted KITTI data. \n",
    "\n",
    "### Define the task and action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a coco Dataset - If you already have data in coco detection format(without masks) and skipped step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset\n",
    "# model_name = \"annotations\"\n",
    "# coco_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format coco --workspace {workspace_id} --cloud_file_path {coco_dataset_path} --use_for '{json.dumps(['testing'])}'\")\n",
    "# print(coco_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check progress\n",
    "# while True:\n",
    "#     clear_output(wait=True)\n",
    "#     response = subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {coco_dataset_id} --job_type dataset\")\n",
    "#     response = json.loads(response)\n",
    "#     print(json.dumps(response, sort_keys=True, indent=4))\n",
    "#     if response.get(\"status\") == \"invalid_pull\":\n",
    "#         raise ValueError(\"Dataset pull failed\")\n",
    "#     if response.get(\"status\") == \"pull_complete\":\n",
    "#         break\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "model_name = \"auto_label\"\n",
    "filter_params = {\"network_arch\": \"mal\"}\n",
    "message = subprocess.getoutput(f\"tao-client {model_name} list-base-experiments --filter_params '{json.dumps(filter_params)}'\")\n",
    "message = ast.literal_eval(message)\n",
    "for rsp in message:\n",
    "    rsp_keys = rsp.keys()\n",
    "    if \"encryption_key\" not in rsp.keys():\n",
    "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys\n",
    "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_map = {\"auto_label\" : \"mask_auto_label:trainable_v1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_params = {\"network_arch\": \"mal\"}\n",
    "message = subprocess.getoutput(f\"tao-client {model_name} list-base-experiments --filter_params '{json.dumps(filter_params)}'\")\n",
    "message = ast.literal_eval(message)\n",
    "ptm = []\n",
    "for rsp in message:\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"ngc_path\" in rsp_keys\n",
    "    if rsp[\"ngc_path\"].endswith(pretrained_map[model_name]):\n",
    "        assert \"id\" in rsp_keys\n",
    "        ptm_id = rsp[\"id\"]\n",
    "        ptm = [ptm_id]\n",
    "        print(\"Metadata for model with requested NGC Path\")\n",
    "        print(rsp)\n",
    "        break\n",
    "print(ptm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm_information = {\"base_experiment\":ptm}\n",
    "patched_model = subprocess.getoutput(f\"tao-client {model_name} patch-artifact-metadata --id {coco_dataset_id} --job_type dataset --update_info '{json.dumps(ptm_information)}' \")\n",
    "print(patched_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto labeling action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "auto_label_generate_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action auto_label --job_type dataset --id {coco_dataset_id}\")\n",
    "auto_label_generate_specs = json.loads(auto_label_generate_specs)\n",
    "print(json.dumps(auto_label_generate_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "auto_label_generate_specs[\"gpu_ids\"] = [0]\n",
    "print(json.dumps(auto_label_generate_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "# Run action\n",
    "coco_mask_dataset_id = kitti_dataset_id\n",
    "parent = convert_job_id\n",
    "auto_labeling_job_id = subprocess.getoutput(f\"tao-client {model_name} dataset-run-action --id {coco_dataset_id} --parent_job_id {parent} --action auto_label --specs '{json.dumps(auto_label_generate_specs)}'\")\n",
    "print(auto_labeling_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, coco_dataset_id, auto_labeling_job_id, \"dataset\", workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply data augmentation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "In this section, we run offline augmentation with the original dataset. During the augmentation process, we can use the pseudo-masks generated from the last step to refine the distorted or rotated bounding boxes.\n",
    "\n",
    "### Define the task and action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a coco mask Dataset - If you already have data in coco segmentation format and skipped step 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset\n",
    "# model_name = \"annotations\"\n",
    "# coco_mask_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format coco  --workspace {workspace_id} --cloud_file_path {coco_mask_dataset_path} --use_for '{json.dumps(['testing'])}'\")\n",
    "# print(coco_mask_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check progress\n",
    "# while True:\n",
    "#     clear_output(wait=True)\n",
    "#     response = subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {coco_mask_dataset_id} --job_type dataset\")\n",
    "#     response = json.loads(response)\n",
    "#     print(json.dumps(response, sort_keys=True, indent=4))\n",
    "#     if response.get(\"status\") == \"invalid_pull\":\n",
    "#         raise ValueError(\"Dataset pull failed\")\n",
    "#     if response.get(\"status\") == \"pull_complete\":\n",
    "#         break\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data augmentation action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "augmentation_generate_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action augment --job_type dataset --id {coco_mask_dataset_id}\")\n",
    "augmentation_generate_specs = json.loads(augmentation_generate_specs)\n",
    "print(json.dumps(augmentation_generate_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change any spec key if required\n",
    "print(json.dumps(augmentation_generate_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "# Run action\n",
    "parent = auto_labeling_job_id\n",
    "coco_mask_augmented_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-run-action --id {coco_mask_dataset_id} --parent_job_id {parent} --action augment --specs '{json.dumps(augmentation_generate_specs)}'\")\n",
    "print(coco_mask_augmented_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, coco_mask_dataset_id, coco_mask_augmented_dataset_id, \"dataset\", workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the augment action you'll get a new dataset\n",
    "print(subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {coco_mask_augmented_dataset_id} --job_type dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform data analytics  <a class=\"anchor\" id=\"head-4\"></a>\n",
    "Next, we perform analytics with the KITTI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data analytics annotation analytics action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "model_name = \"analytics\"\n",
    "analytics_analyze_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action analyze --job_type dataset --id {coco_dataset_id}\")\n",
    "analytics_analyze_specs = json.loads(analytics_analyze_specs)\n",
    "print(json.dumps(analytics_analyze_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "analytics_analyze_specs[\"data\"][\"input_format\"] = \"COCO\"\n",
    "print(json.dumps(analytics_analyze_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "# Run action\n",
    "parent = convert_job_id\n",
    "analyze_job_id = subprocess.getoutput(f\"tao-client {model_name} dataset-run-action --id {coco_dataset_id} --action analyze --parent_job_id {parent} --specs '{json.dumps(analytics_analyze_specs)}'\")\n",
    "print(analyze_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, coco_dataset_id, analyze_job_id, \"dataset\", workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform data validation  <a class=\"anchor\" id=\"head-5\"></a>\n",
    "Next, we perform validate the annotations and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data annotation validation action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "model_name = \"analytics\"\n",
    "validate_annotations_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action validate_annotations --job_type dataset --id {coco_dataset_id}\")\n",
    "validate_annotations_specs = json.loads(validate_annotations_specs)\n",
    "print(json.dumps(validate_annotations_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "validate_annotations_specs[\"data\"][\"input_format\"] = \"COCO\"\n",
    "print(json.dumps(validate_annotations_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "# Run action\n",
    "parent = convert_job_id\n",
    "validate_annotations_job_id = subprocess.getoutput(f\"tao-client {model_name} dataset-run-action --id {coco_dataset_id} --action validate_annotations --parent_job_id {parent} --specs '{json.dumps(validate_annotations_specs)}'\")\n",
    "print(validate_annotations_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, coco_dataset_id, validate_annotations_job_id, \"dataset\", workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data image validation action - removes corrupted images and creates a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "model_name = \"image\"\n",
    "validate_images_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action validate_images --job_type dataset --id {kitti_dataset_id}\")\n",
    "validate_images_specs = json.loads(validate_images_specs)\n",
    "print(json.dumps(validate_images_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make changes to the specs if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "# Run action\n",
    "validate_images_job_id = subprocess.getoutput(f\"tao-client {model_name} dataset-run-action --id {kitti_dataset_id} --action validate_images --specs '{json.dumps(validate_images_specs)}'\")\n",
    "print(validate_images_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, kitti_dataset_id, validate_images_job_id, \"dataset\", workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Backup database with a mongodump file saved in workspace dump/archive/{backup_filename}\n",
    "# backup_file_name = \"mongodump.tar.gz\" # FIXME 7\n",
    "# subprocess.getoutput(f\"tao-client {model_name} workspace-backup --workspace {workspace_id} --backup_file_name {backup_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete original kitti dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.getoutput(f\"tao-client {model_name} dataset-delete --id {kitti_dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete coco augment dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.getoutput(f\"tao-client {model_name} dataset-delete --id {coco_mask_augmented_dataset_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
