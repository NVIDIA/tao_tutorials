{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### TAO remote client - Auto Labeling\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![image](https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
    "\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "- Pulling datasets from cloud\n",
    "- Getting a PTM from NGC\n",
    "- Model Actions\n",
    "    - Train (Normal/AutoML)\n",
    "    - Evaluate\n",
    "    - Inference\n",
    "    - Delete experiments/datasets\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Install TAO remote client ](#head-1)\n",
    "1. [FIXME's](#head-2)\n",
    "1. [Login](#head-3)\n",
    "1. [Create a cloud workspace](#head-2)\n",
    "1. [Set dataset formats](#head-4)\n",
    "1. [Create and pull train dataset](#head-5)\n",
    "1. [Create and pull val dataset](#head-6)\n",
    "1. [List the created datasets](#head-7)\n",
    "1. [Create an experiment](#head-8)\n",
    "1. [List experiments](#head-9)\n",
    "1. [Assign train, eval datasets](#head-10)\n",
    "1. [Assign PTM](#head-11)\n",
    "1. [View hyperparameters that are enabled by default](#head-12)\n",
    "1. [Train](#head-13)\n",
    "1. [Set AutoML related configurations](#head-13.1)\n",
    "1. [Evaluate](#head-14)\n",
    "1. [TAO inference](#head-15)\n",
    "1. [Delete experiment](#head-16)\n",
    "1. [Delete dataset](#head-17)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Install TAO remote client <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO-Client wheel.\n",
    "! pip3 install nvidia-tao-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the version of the TAO-Client\n",
    "! tao-client --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages required for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import ast\n",
    "import json\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from remove_corrupted_images import remove_corrupted_images_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "namespace = 'default'\n",
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the dataset folder structure required for the models supported in this notebook, visit the notebooks under dataset_prepare like for [this notebook](../dataset_prepare/auto_labeling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME's <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "1. (Optional) Enable AutoML if needed in FIXME 1\n",
    "1. (Optional) Choose between bayesian and hyperband automl_algorithm in FIXME 2 (If automl was enabled in FIXME1)\n",
    "1. Assign a workdir in FIXME 3 for log file download\n",
    "1. Assign the ip_address and port_number in FIXME 4 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "1. Assign the ngc_key variable in FIXME 5\n",
    "1. Assign the ngc_org_name variable in FIXME 6\n",
    "1. Set cloud storage details in FIXME 7\n",
    "1. Assign path of datasets relative to the bucket in FIXME 8\n",
    "1. Database backup/restore archive filename in FIXME 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toggle AutoML params\n",
    "[AutoML documentation](https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_enabled = False # FIXME1 set to True if you want to run automl for the model chosen in the previous cell\n",
    "automl_algorithm = \"bayesian\" # FIXME2 example: bayesian/hyperband"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toggle downloading jobs onto local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"workdir_auto_labeling\" # FIXME3\n",
    "# Creating workdir\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set API service's host information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = \"http://<ip_address>:<port_number>\" # FIXME4 example: https://10.137.149.22:32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service tao-api-ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set NGC Personal key for authentication and NGC org to access API services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = \"<ngc_key>\" # FIXME5 example: (Add NGC Personal key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_org_name = \"ea-tlt\" # FIXME6 your NGC ORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login <a class=\"anchor\" id=\"head-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env BASE_URL={host_url}/{namespace}/api/v1\n",
    "\n",
    "# Exchange NGC_API_KEY for JWT\n",
    "identity = json.loads(subprocess.getoutput(f'tao login --ngc-key {ngc_key} --ngc-org-name {ngc_org_name} --enable-telemetry'))\n",
    "\n",
    "%env USER={identity['user_id']}\n",
    "%env TOKEN={identity['token']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NVCF gpu details <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    " One of the keys of the response json are to be used as platform_id when you run each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Valid only for NVCF backend during TAO-API helm deployment currently\n",
    "# response = json.loads(subprocess.getoutput(f'tao get-gpu-types'))\n",
    "# print((json.dumps(response, indent=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cloud workspace\n",
    "This workspace will be the place where your datasets reside and your results of TAO API jobs will be pushed to.\n",
    "\n",
    "If you want to have different workspaces for dataset and experiment, duplocate the workspace creation part and adjust the metadata accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME7 Dataset Cloud bucket details to download dataset for experiments (Can be read only)\n",
    "workspace_name = \"AWS workspace info\"  # A Representative name for this cloud info\n",
    "cloud_type = \"aws\"  # If it's AWS, HuggingFace or Azure\n",
    "\n",
    "cloud_metadata = {}\n",
    "cloud_metadata[\"cloud_region\"] = \"us-west-1\"  # Bucket region\n",
    "cloud_metadata[\"cloud_bucket_name\"] = \"\"  # Bucket name\n",
    "# Access and Secret for AWS\n",
    "cloud_metadata[\"access_key\"] = \"\"\n",
    "cloud_metadata[\"secret_key\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_id = subprocess.getoutput(f\"tao-client {model_name} workspace-create --name '{workspace_name}' --cloud_type {cloud_type} --cloud_details '{json.dumps(cloud_metadata)}'\")\n",
    "print(workspace_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Optional: Restore database with a mongodump file saved in workspace dump/archive/{backup_filename}\n",
    "# backup_file_name = \"mongodump.tar.gz\" # FIXME 9\n",
    "# response = subprocess.getoutput(f\"tao-client {model_name} workspace-restore --workspace {workspace_id} --backup_file_name {backup_file_name}\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set dataset path (path within cloud bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME8 : Set paths relative to cloud bucket\n",
    "train_dataset_path = \"/data/auto_label_train\"\n",
    "eval_dataset_path = \"/data/auto_label_val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to parse logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tail(model_name_cli, id, job_id, job_type, workdir):\n",
    "\tstatus = None\n",
    "\twhile True:\n",
    "\t\ttime.sleep(10)\n",
    "\t\tclear_output(wait=True)\n",
    "\t\tresponse = subprocess.getoutput(f\"tao-client {model_name_cli} get-action-status --job_type {job_type} --id {id} --job {job_id}\")\n",
    "\t\tresponse = json.loads(response)\n",
    "\t\tif response and \"status\" in response.keys() and response.get(\"status\") in (\"Done\", \"Error\", \"Canceled\", \"Paused\"):\n",
    "\t\t\tprint(json.dumps(response, indent=4))\n",
    "\t\t\tstatus = response.get(\"status\")\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tlogs = subprocess.getoutput(f\"tao-client {model_name_cli} get-job-logs --id {id} --job {job_id} --job_type {job_type} --workdir {workdir}\")\n",
    "\t\tif not logs:\n",
    "\t\t\tcontinue\n",
    "\t\tlog_content_lines = logs.split(\"\\n\")        \n",
    "\t\tfor line in log_content_lines:\n",
    "\t\t\tprint(line.strip())\n",
    "\t\t\tif line.strip() == \"Error EOF\":\n",
    "\t\t\t\tstatus = \"Error\"\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif line.strip() == \"Done EOF\":\n",
    "\t\t\t\tstatus = \"Done\"\n",
    "\t\t\t\tbreak\n",
    "\t\tif status is not None:\n",
    "\t\t\tbreak\n",
    "\treturn status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset formats <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = \"segmentation\"\n",
    "ds_format = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and pull train dataset <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type {ds_type} --dataset_format {ds_format} --workspace {workspace_id} --cloud_file_path {train_dataset_path} --use_for '{json.dumps(['training'])}'\")\n",
    "print(train_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {train_dataset_id} --job_type dataset\")\n",
    "    response = json.loads(response)\n",
    "    print(json.dumps(response, sort_keys=True, indent=4))\n",
    "    if response.get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment if you want to remove corrupted images in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This packages data-services experiments create and running the job of removing corrupted images\n",
    "# try:\n",
    "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
    "#     train_dataset_id = remove_corrupted_images_workflow(workspace_id, train_dataset_id)\n",
    "# except Exception as e:\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and pull val dataset <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type {ds_type} --dataset_format {ds_format} --workspace {workspace_id} --cloud_file_path {eval_dataset_path} --use_for '{json.dumps(['evaluation'])}'\")\n",
    "print(eval_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check progress\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {eval_dataset_id} --job_type dataset\")\n",
    "    response = json.loads(response)\n",
    "    print(json.dumps(response, sort_keys=True, indent=4))\n",
    "    if response.get(\"status\") == \"invalid_pull\":\n",
    "        raise ValueError(\"Dataset pull failed\")\n",
    "    if response.get(\"status\") == \"pull_complete\":\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment if you want to remove corrupted images in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This packages data-services experiments create and running the job of removing corrupted images\n",
    "# try:\n",
    "#     from remove_corrupted_images import remove_corrupted_images_workflow\n",
    "#     eval_dataset_id = remove_corrupted_images_workflow(workspace_id, eval_dataset_id)\n",
    "# except Exception as e:\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### List datasets <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "message = subprocess.getoutput(f\"tao-client {model_name} list-datasets\")\n",
    "message = ast.literal_eval(message)\n",
    "for rsp in message:\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"id\" in rsp_keys\n",
    "    assert \"type\" in rsp_keys\n",
    "    assert \"format\" in rsp_keys\n",
    "    assert \"name\" in rsp_keys\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Create an experiment <a class=\"anchor\" id=\"head-8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_arch = model_name\n",
    "experiment_id = subprocess.getoutput(f\"tao-client {model_name} experiment-create --network_arch {network_arch} --encryption_key tlt_encode  --workspace {workspace_id}\")\n",
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### List experiments <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all user created experiments for the chosen network architecture\n",
    "filter_params = {\"network_arch\": network_arch}\n",
    "message = subprocess.getoutput(f\"tao-client {model_name} list-experiments --filter_params '{json.dumps(filter_params)}'\")\n",
    "message = ast.literal_eval(message)\n",
    "for rsp in message:\n",
    "    rsp_keys = rsp.keys()\n",
    "    if \"encryption_key\" not in rsp.keys():\n",
    "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys\n",
    "        print(rsp[\"name\"],rsp[\"id\"],rsp[\"network_arch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign train, eval datasets <a class=\"anchor\" id=\"head-10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_env_vars = {} # Update any variables to be included while triggering Docker run-time like MLOPs variables \n",
    "dataset_information = {\"train_datasets\":[train_dataset_id],\n",
    "                       \"eval_dataset\":eval_dataset_id,\n",
    "                       \"inference_dataset\":eval_dataset_id,\n",
    "                       \"docker_env_vars\": docker_env_vars,\n",
    "                       \"metric\": \"train_loss\"}\n",
    "patched_model = subprocess.getoutput(f\"tao-client {model_name} patch-artifact-metadata --id {experiment_id} --job_type experiment --update_info '{json.dumps(dataset_information)}' \")\n",
    "print(patched_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Assign PTM <a class=\"anchor\" id=\"head-11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "filter_params = {\"network_arch\": network_arch}\n",
    "message = subprocess.getoutput(f\"tao-client {model_name} list-base-experiments --filter_params '{json.dumps(filter_params)}'\")\n",
    "message = ast.literal_eval(message)\n",
    "for rsp in message:\n",
    "    rsp_keys = rsp.keys()\n",
    "    if \"encryption_key\" not in rsp.keys():\n",
    "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys\n",
    "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assigning pretrained models\n",
    "# From the output of previous cell make the appropriate changes to this map if you want to change the default PTM backbone.\n",
    "# Changing the default backbone here requires changing default spec/config during train/eval etc like for example\n",
    "# If you are changing the ptm to resnet34, then you have to modify the config key num_layers if it exists to 34 manually\n",
    "pretrained_map = {\"mal\" : \"mask_auto_label:trainable_v1.0\"}\n",
    "no_ptm_models = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if network_arch not in no_ptm_models:\n",
    "    filter_params = {\"network_arch\": network_arch}\n",
    "    message = subprocess.getoutput(f\"tao-client {model_name} list-base-experiments --filter_params '{json.dumps(filter_params)}'\")\n",
    "    message = ast.literal_eval(message)\n",
    "    ptm = []\n",
    "    for rsp in message:\n",
    "        rsp_keys = rsp.keys()\n",
    "        assert \"ngc_path\" in rsp_keys\n",
    "        if rsp[\"ngc_path\"].endswith(pretrained_map[network_arch]):\n",
    "            assert \"id\" in rsp_keys\n",
    "            ptm_id = rsp[\"id\"]\n",
    "            ptm = [ptm_id]\n",
    "            print(\"Metadata for model with requested NGC Path\")\n",
    "            print(rsp)\n",
    "            break\n",
    "    print(ptm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if network_arch not in no_ptm_models:\n",
    "    ptm_information = {\"base_experiment\":ptm}\n",
    "    patched_model = subprocess.getoutput(f\"tao-client {model_name} patch-artifact-metadata --id {experiment_id} --job_type experiment --update_info '{json.dumps(ptm_information)}' \")\n",
    "    print(patched_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train <a class=\"anchor\" id=\"head-13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if automl_enabled:\n",
    "    # View default automl params enabled\n",
    "    automl_params = subprocess.getoutput(f\"tao-client {model_name} model-automl-defaults --id {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "#### Set AutoML related configurations <a class=\"anchor\" id=\"head-13.1\"></a>\n",
    "Refer to these hyper-links to see the parameters supported by each network and add more parameters if necessary in addition to the default automl enabled parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if automl_enabled:\n",
    "    # Choose any metric that is present in the kpi dictionary present in the model's status.json. \n",
    "    # Example status.json for each model can be found in the respective section in NVIDIA TAO DOCS here: https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/index.html\n",
    "    metric=\"kpi\"\n",
    "\n",
    "    #Refer to parameter list mentioned in the above links and add/remove any extra parameter in addition to the default enabled ones in automl_specs\n",
    "\n",
    "    automl_information = {\"automl_enabled\":automl_enabled,\n",
    "                          \"automl_algorithm\":automl_algorithm,\n",
    "                          \"automl_max_recommendations\": 20, # Only for bayesian\n",
    "                          \"automl_R\": 27, # Only for hyperband\n",
    "                          \"automl_nu\": 3, # Only for hyperband\n",
    "                          \"epoch_multiplier\": 1, # Only for hyperband\n",
    "                          # Warning: The parameters that are disabled are not tested by TAO, so there might be unexpected behaviour in overriding this\n",
    "                          \"override_automl_disabled_params\": False,\n",
    "                          \"automl_hyperparameters\":json.loads(automl_params)\n",
    "                         }\n",
    "    patch_metadata = {\"metric\": metric, \"automl_settings\": automl_information}\n",
    "    patched_model = subprocess.getoutput(f\"tao-client {model_name} patch-artifact-metadata --id {experiment_id} --job_type experiment --update_info '{json.dumps(patch_metadata)}' \")\n",
    "    patched_model = json.loads(patched_model)\n",
    "    print(json.dumps(patched_model, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "#### Provide train specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default train model specs\n",
    "train_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action train --job_type experiment --id {experiment_id}\")\n",
    "train_specs = json.loads(train_specs)\n",
    "print(json.dumps(train_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize train model specs\n",
    "train_specs[\"train\"][\"num_gpus\"] = 1\n",
    "train_specs[\"train\"][\"gpu_ids\"] = [0]\n",
    "train_specs[\"train\"][\"num_epochs\"] = 5\n",
    "train_specs[\"train\"][\"checkpoint_interval\"] = 5\n",
    "train_specs[\"train\"][\"validation_interval\"] = 5\n",
    "print(json.dumps(train_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "#### Run train action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "job_id = subprocess.getoutput(f\"tao-client {model_name} experiment-run-action --action train --id {experiment_id} --specs '{json.dumps(train_specs)}'\")\n",
    "job_map[\"train_\" + model_name] = job_id\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status\n",
    "if automl_enabled:\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        response = subprocess.getoutput(f\"tao-client {model_name} get-action-status --job_type experiment --id {experiment_id} --job {job_id}\")\n",
    "        response = json.loads(response)\n",
    "        if \"error_desc\" in response.keys() and response[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "            print(\"Job is being created\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        print(json.dumps(response, sort_keys=True, indent=4))\n",
    "        assert \"status\" in response.keys() and response.get(\"status\") != \"Error\"\n",
    "        if response.get(\"status\") in [\"Done\",\"Error\"]:\n",
    "            break\n",
    "        time.sleep(15)\n",
    "else:\n",
    "    # Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "    status = my_tail(model_name, experiment_id, job_id, \"experiment\", workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Stop an AutoML JOB\n",
    "#    1. Stop the 'Monitor job status' cell (the cell right before this cell) manually\n",
    "#    2. Uncomment the snippet in the next cell and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if automl_enabled:\n",
    "#     job_id = job_map[\"train_\" + model_name]\n",
    "#     job_id = subprocess.getoutput(f\"tao-client {model_name} job-pause --job_type experiment --id {experiment_id} --job {job_id}\")\n",
    "#     job_map[\"canceled_\" + model_name] = job_id\n",
    "#     print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the below snippet if you want to resume an already stopped AutoML job and then run the 'Monitor job status' cell above (4th cell above from this cell)\n",
    "# if automl_enabled:\n",
    "#     job_id = job_map[\"train_\" + model_name]\n",
    "#     job_id = subprocess.getoutput(f\"tao-client {model_name} job-resume --id {experiment_id} --job {job_id}  --parent_job_id {parent}  --specs '{json.dumps(train_specs)}'\")\n",
    "#     job_map[\"resumed_\" + model_name] = job_id\n",
    "#     print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit the method of choosing checkpoint from list of train checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model handler parameters\n",
    "model_parameters = subprocess.getoutput(f\"tao-client {model_name} get-metadata --id {experiment_id} --job_type experiment\")\n",
    "model_parameters = json.loads(model_parameters)\n",
    "update_checkpoint_choosing = {}\n",
    "update_checkpoint_choosing[\"checkpoint_choose_method\"] = model_parameters[\"checkpoint_choose_method\"]\n",
    "update_checkpoint_choosing[\"checkpoint_epoch_number\"] = model_parameters[\"checkpoint_epoch_number\"]\n",
    "print(json.dumps(update_checkpoint_choosing, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the method by which checkpoint from the parent action is chosen, when parent action is a train/retrain action.\n",
    "# Example for evaluate action below, can be applied in the same way for other actions too\n",
    "update_checkpoint_choosing[\"checkpoint_choose_method\"] = \"latest_model\" # Choose between best_model/latest_model/from_epoch_number\n",
    "# If from_epoch_number is chosen then assign the epoch number to the dictionary key in the format 'from_epoch_number{train_job_id}'\n",
    "# update_checkpoint_choosing[\"checkpoint_epoch_number\"][\"from_epoch_number_c2f76eb7-2a75-4197-9a84-c1547f20c17d\"] = 6\n",
    "\n",
    "patched_model = subprocess.getoutput(f\"tao-client {model_name} patch-artifact-metadata --id {experiment_id} --job_type experiment --update_info '{json.dumps(update_checkpoint_choosing)}'\")\n",
    "patched_model = json.loads(patched_model)\n",
    "print(json.dumps(patched_model, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push model to private ngc team registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f\"TAO {model_name}\"  # Display name for the model to be published on the model card\n",
    "description = f\"Train {model_name}\"  # Short description for the model to be published on the model card\n",
    "team = \"tao_ea\"  # Team within org for the model to be published to\n",
    "\n",
    "job_id = job_map[\"train_\" + model_name]\n",
    "message = subprocess.getoutput(f\"tao-client {model_name} publish-model --id {experiment_id} --job {job_id} --job_type experiment --display_name='{display_name}' --description='{description}' --team {team}\")\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove model from private ngc team registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = subprocess.getoutput(f\"tao-client {model_name} remove-published-model --id {experiment_id} --job {job_id} --job_type experiment --team {team}\")\n",
    "# print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate <a class=\"anchor\" id=\"head-14\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide evaluate specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default evaluate model specs\n",
    "eval_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --action evaluate --job_type experiment --id {experiment_id}\")\n",
    "eval_specs = json.loads(eval_specs)\n",
    "print(json.dumps(eval_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize evaluate model specs\n",
    "# Change any spec if you wish\n",
    "print(json.dumps(eval_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "job_id = subprocess.getoutput(f\"tao-client {model_name} experiment-run-action --action evaluate --id {experiment_id} --parent_job_id {parent} --specs '{json.dumps(eval_specs)}'\")\n",
    "job_map[\"eval_\" + model_name] = job_id\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, experiment_id, job_id, \"experiment\", workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAO inference <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide TAO inference specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default inference model specs\n",
    "tao_inference_specs = subprocess.getoutput(f\"tao-client {model_name} get-spec --id {experiment_id} --action inference --job_type experiment\")\n",
    "tao_inference_specs = json.loads(tao_inference_specs)\n",
    "print(json.dumps(tao_inference_specs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Customize TAO inference specs\n",
    "#Apply changes to the specs dictionary here if required\n",
    "print(json.dumps(tao_inference_specs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run TAO inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add --platform_id uuid for NVCF backend, where the uuid is a key from output of tao-client gpu-types\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "job_id = subprocess.getoutput(f\"tao-client {model_name} experiment-run-action --action inference --id {experiment_id} --parent_job_id {parent} --specs '{json.dumps(tao_inference_specs)}'\")\n",
    "job_map[\"tao_inference_\" + model_name] = job_id\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "status = my_tail(model_name, experiment_id, job_id, \"experiment\", workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Backup database with a mongodump file saved in workspace dump/archive/{backup_filename}\n",
    "# backup_file_name = \"mongodump.tar.gz\" # FIXME 9\n",
    "# subprocess.getoutput(f\"tao-client {model_name} workspace-backup --workspace {workspace_id} --backup_file_name {backup_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete experiment <a class=\"anchor\" id=\"head-16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.getoutput(f\"tao-client {model_name} experiment-delete --id {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete dataset <a class=\"anchor\" id=\"head-17\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.getoutput(f\"tao-client {model_name} dataset-delete --id {train_dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.getoutput(f\"tao-client {model_name} dataset-delete --id {eval_dataset_id}\")"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
