{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ppurpose built models dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign a model_name in FIXME 1\n",
    "\n",
    "    1.1 Assign model type for action_recognition/pose_classification in FIXME 1.1\n",
    "\n",
    "    1.2 Assign platform for action_recognition in FIXME 1.2\n",
    "    \n",
    "    1.3 Assign model input type for action_recognition in FIXME 1.3\n",
    "1. Choose between default and custom dataset in FIXME 2\n",
    "1. Assign path of DATA_DIR in FIXME 3\n",
    "1. Assign Cloud credentials in FIXME 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model_name workspaces and other variables\n",
    "# Available models (#FIXME 1):\n",
    "# 1. action_recognition - https://docs.nvidia.com/tao/tao-toolkit/text/action_recognition_net.html\n",
    "# 2. bevfusion - https://docs.nvidia.com/tao/tao-toolkit/text/action_recognition_net.html\n",
    "# 3. ml_recog - https://docs.nvidia.com/tao/tao-toolkit/text/ml_recog/index.html\n",
    "# 4. ocdnet - https://docs.nvidia.com/tao/tao-toolkit/text/ocdnet/index.html\n",
    "# 5. ocrnet - https://docs.nvidia.com/tao/tao-toolkit/text/ocrnet/index.html\n",
    "# 6. optical_inspection - https://docs.nvidia.com/tao/tao-toolkit/text/optical_inspection/index.html\n",
    "# 7. pose_classification - https://docs.nvidia.com/tao/tao-toolkit/text/pose_classification/index.html\n",
    "# 8. pointpillars - https://docs.nvidia.com/tao/tao-toolkit/text/point_cloud/pointpillars.html\n",
    "# 9. re_identification - https://docs.nvidia.com/tao/tao-toolkit/text/re_identification/index.html\n",
    "# 10. centerpose - https://docs.nvidia.com/tao/tao-toolkit/text/centerpose/index.html\n",
    "# 11. visual_changenet_segment - https://docs.nvidia.com/tao/tao-toolkit/text/visual_changenet/index.html\n",
    "# 12. visual_changenet_classify - https://docs.nvidia.com/tao/tao-toolkit/text/visual_changenet/index.html \n",
    "\n",
    "model_name = \"bevfusion\" # FIXME1 (Add the model name from the above mentioned list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name in (\"action_recognition\",\"pose_classification\"):\n",
    "    # FIXME1.1 - model_type - string\n",
    "        # action-recognition: rgb/of/joint;\n",
    "        # pose-classification: kinetics/nvidia\n",
    "    model_type = \"rgb\"\n",
    "\n",
    "    if model_name == \"action_recognition\":\n",
    "        if model_type not in (\"rgb\",\"of\",\"joint\"):\n",
    "            raise Exception(\"Choose one of rgb/of/joint for action recognition model_type\")\n",
    "    elif model_name == \"pose_classification\":\n",
    "        if model_type not in (\"kinetics\",\"nvidia\"):\n",
    "            raise Exception(\"Choose one of kinetics/nvidia for pose classification model_type (preferred nvidia)\")\n",
    "\n",
    "    if model_name == \"action_recognition\":\n",
    "        platform = \"a100\" # FIXME1.2 a100/xavier - valid only for model_type that is not rgb\n",
    "        model_input_type = \"3d\" # FIXME1.3 3d/2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example dataset source and structure <a class=\"anchor\" id=\"head-1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Recognition:** We will be using the HMDB51 [dataset](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/) for the tutorial. (We choose catch/smile for this tutorial):\n",
    "\n",
    "**MLRecogNet** We will be using the `Retail Product Checkout Dataset` for the tutorial. Downdload the datsaet from [here](https://www.kaggle.com/datasets/diyer22/retail-product-checkout-dataset) and place it under $DATA_DIR/metric_learning_recognition\n",
    "\n",
    "**OCDNET**: We will be using the ICDAR2015 dataset for the ocdnet tutorial. Please access the dataset [here](https://rrc.cvc.uab.es/?ch=4&com=tasks) to register and download the data from Task 4.1: Text Localization. Unzip the files to DATA_DIR\n",
    "\n",
    "**OCRNET**: We will be using the ICDAR15 word recognition dataset for the tutorial. To find more details please visit [here](\n",
    "https://rrc.cvc.uab.es/?ch=4&com=tasks). Please download the ICDAR15 word recognition train dataset and test_dataset [here](https://rrc.cvc.uab.es/?ch=4&com=downloads) to DATA_DIR.\n",
    "\n",
    "**Pointpillars:** We will be using the `kitti object detection dataset` for this example. To find more details, please visit [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d)\n",
    "\n",
    "**Pose Classification:** We will be using the Kinetics dataset from [Deepmind](https://deepmind.com/research/open-source/kinetics) or NVIDIA created dataset. For kinetics based dataset set model_type as `kinetics` and for nvidia based dataset set model_type as `nvidia`\n",
    "\n",
    "**Re-Identification:** We will be using the [Market-1501](https://zheng-lab.cecs.anu.edu.au/Project/project_reid.html) dataset. Download the dataset [here](https://drive.google.com/file/d/1TwkgQcIa_EgRjVMPSbyEKtcfljqURrzi/view?usp=sharing) and extract it.\n",
    "\n",
    "**Optical Inspection:** Bring your own dataset according to the format described [here](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#optical-inspection-format). \n",
    "\n",
    "**Visual ChangeNet-Classification:** Bring your own dataset according to the format described [here](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#optical-inspection-format). \n",
    "\n",
    "**Visual ChangeNet-Segmentation:** We will be using the [Market-1501](https://zheng-lab.cecs.anu.edu.au/Project/project_reid.html) dataset. Download the dataset [here](https://www.dropbox.com/s/18fb5jo0npu5evm/LEVIR-CD256.zip) and extract it. \n",
    "\n",
    "**CenterPose:** We will be using [Google Objectron](https://github.com/google-research-datasets/Objectron) dataset. The following script will download and preprocess the dataset the dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_to_be_used = \"default\" #FIXME2 #default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset\n",
    "DATA_DIR = os.path.abspath(model_name) #FIXME3 (set absolute path of the data_directory)\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download and pre-processing <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    if model_name == \"action_recognition\":\n",
    "        !sudo apt-get update -y && sudo apt-get install unrar -y\n",
    "        !wget -P $DATA_DIR --no-check-certificate http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
    "        assert os.path.exists(f\"{DATA_DIR}/hmdb51_org.rar\")\n",
    "        !mkdir -p $DATA_DIR/videos && unrar x -o+ $DATA_DIR/hmdb51_org.rar $DATA_DIR/videos\n",
    "        !mkdir -p $DATA_DIR/raw_data\n",
    "        !unrar x -o+ $DATA_DIR/videos/catch.rar $DATA_DIR/raw_data\n",
    "        !unrar x -o+ $DATA_DIR/videos/smile.rar $DATA_DIR/raw_data\n",
    "        assert os.path.exists(f\"{DATA_DIR}/raw_data/catch\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/raw_data/smile\")\n",
    "    elif model_name == \"ml_recog\":\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset.zip\")\n",
    "        !unzip -uq $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset.zip -d $DATA_DIR/metric_learning_recognition\n",
    "    elif model_name == \"ocdnet\":\n",
    "        assert(os.path.exists(f\"{DATA_DIR}/train/img\"))\n",
    "        assert(os.path.exists(f\"{DATA_DIR}/train/gt\"))\n",
    "        assert(os.path.exists(f\"{DATA_DIR}/test/img\"))\n",
    "        assert(os.path.exists(f\"{DATA_DIR}/test/gt\"))\n",
    "    elif model_name == \"ocrnet\":\n",
    "        !mkdir -p $DATA_DIR/train && rm -rf $DATA_DIR/train/*\n",
    "        !mkdir -p $DATA_DIR/test && rm -rf $DATA_DIR/test/*\n",
    "        !unzip -u $DATA_DIR/ch4_test_word_images_gt.zip -d $DATA_DIR/test\n",
    "        !cp $DATA_DIR/Challenge4_Test_Task3_GT.txt -d $DATA_DIR/test\n",
    "        !unzip -u $DATA_DIR/ch4_training_word_images_gt.zip -d $DATA_DIR/train    \n",
    "        assert os.path.exists(f\"{DATA_DIR}/ch4_test_word_images_gt.zip\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/Challenge4_Test_Task3_GT.txt\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/ch4_training_word_images_gt.zip\")\n",
    "    elif model_name in (\"optical_inspection\", \"visual_changenet_classify\"):\n",
    "        assert os.path.exists(f\"{DATA_DIR}/train/images\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/train/dataset.csv\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/val/images\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/val/dataset.csv\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/test/images\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/test/dataset.csv\")\n",
    "    elif model_name == \"visual_changenet_segment\":\n",
    "        #Download the data\n",
    "        URL_DATASET = \"https://www.dropbox.com/s/18fb5jo0npu5evm/LEVIR-CD256.zip\"\n",
    "        os.environ[\"URL_DATASET\"]=URL_DATASET\n",
    "        !if [ ! -f $DATA_DIR/LEVIR-CD256.zip ]; then wget $URL_DATASET -O $DATA_DIR/LEVIR-CD-256.zip; else echo \"image archive already downloaded\"; fi \n",
    "        # Check the dataset is present\n",
    "        !mkdir -p $DATA_DIR\n",
    "        !if [ ! -f $DATA_DIR/LEVIR-CD-256.zip ]; then echo 'Dataset zip file not found, please download.'; else echo 'Found Dataset zip file.';fi\n",
    "        # unpack \n",
    "        !unzip -u $DATA_DIR/LEVIR-CD-256.zip -d $DATA_DIR\n",
    "        assert os.path.exists(f\"{DATA_DIR}/LEVIR-CD256/A\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/LEVIR-CD256/B\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/LEVIR-CD256/label\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/LEVIR-CD256/list/train.txt\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/LEVIR-CD256/list/val.txt\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/LEVIR-CD256/list/test.txt\")\n",
    "    elif model_name in (\"pointpillars\", \"bevfusion\"):\n",
    "        !unzip -u $DATA_DIR/data_object_image_2.zip -d $DATA_DIR\n",
    "        !unzip -u $DATA_DIR/data_object_label_2.zip -d $DATA_DIR\n",
    "        !unzip -u $DATA_DIR/data_object_velodyne.zip -d $DATA_DIR\n",
    "        !unzip -u $DATA_DIR/data_object_calib.zip -d $DATA_DIR\n",
    "\n",
    "        if model_name == \"bevfusion\":\n",
    "            !mkdir -p $DATA_DIR/ImageSets\n",
    "            !if [ ! -f $DATA_DIR//ImageSets/test.txt ]; then wget https://raw.githubusercontent.com/traveller59/second.pytorch/master/second/data/ImageSets/test.txt --no-check-certificate --content-disposition -O $DATA_DIR//ImageSets/test.txt; else echo \"test.txt archive already downloaded\"; fi \n",
    "            !if [ ! -f $DATA_DIR//ImageSets/train.txt ]; then wget https://raw.githubusercontent.com/traveller59/second.pytorch/master/second/data/ImageSets/train.txt --no-check-certificate --content-disposition -O $DATA_DIR/ImageSets/train.txt; else echo \"train.txt archive already downloaded\"; fi \n",
    "            !if [ ! -f $DATA_DIR//ImageSets/val.txt ]; then wget https://raw.githubusercontent.com/traveller59/second.pytorch/master/second/data/ImageSets/val.txt --no-check-certificate --content-disposition -O $DATA_DIR/ImageSets/val.txt; else echo \"val.txt archive already downloaded\"; fi \n",
    "            !if [ ! -f $DATA_DIR//ImageSets/trainval.txt ]; then wget https://raw.githubusercontent.com/traveller59/second.pytorch/master/second/data/ImageSets/trainval.txt --no-check-certificate --content-disposition -O $DATA_DIR/ImageSets/trainval.txt; else echo \"trainval.txt archive already downloaded\"; fi\n",
    "\n",
    "        assert os.path.exists(f\"{DATA_DIR}/training/image_2\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/training/label_2\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/training/velodyne\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/training/calib\")\n",
    "    elif model_name == \"pose_classification\":\n",
    "        !pip3 install -U gdown\n",
    "        if model_type == \"kinetics\":\n",
    "            !gdown https://drive.google.com/uc?id=1dmzCRQsFXJ18BlXj1G9sbDnsclXIdDdR -O $DATA_DIR/st-gcn-processed-data.zip\n",
    "            !unzip $DATA_DIR/st-gcn-processed-data.zip -d $DATA_DIR\n",
    "            !mv $DATA_DIR/data/Kinetics/kinetics-skeleton $DATA_DIR/kinetics\n",
    "            !rm -r $DATA_DIR/data\n",
    "            !rm $DATA_DIR/st-gcn-processed-data.zip\n",
    "            assert os.path.exists(f\"{DATA_DIR}/kinetics\")\n",
    "        elif model_type == \"nvidia\":\n",
    "            !gdown https://drive.google.com/uc?id=1GhSt53-7MlFfauEZ2YkuzOaZVNIGo_c- -O $DATA_DIR/data_3dbp_nvidia.zip\n",
    "            !mkdir -p $DATA_DIR/nvidia\n",
    "            !unzip $DATA_DIR/data_3dbp_nvidia.zip -d $DATA_DIR/nvidia\n",
    "            !rm $DATA_DIR/data_3dbp_nvidia.zip\n",
    "            assert os.path.exists(f\"{DATA_DIR}/nvidia\")\n",
    "            assert os.path.exists(f\"{DATA_DIR}/{model_type}/train_data.npy\") and os.path.exists(f\"{DATA_DIR}/{model_type}/train_label.pkl\") and os.path.exists(f\"{DATA_DIR}/{model_type}/val_data.npy\") and os.path.exists(f\"{DATA_DIR}/{model_type}/val_label.pkl\")\n",
    "    elif model_name == \"re_identification\":\n",
    "        !pip3 install -U gdown\n",
    "        !gdown https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM -O $DATA_DIR/market1501.zip\n",
    "        !unzip -u $DATA_DIR/market1501.zip -d $DATA_DIR\n",
    "        !rm -rf $DATA_DIR/market1501\n",
    "        !mv $DATA_DIR/Market-1501-v15.09.15 $DATA_DIR/market1501\n",
    "        !rm $DATA_DIR/market1501.zip\n",
    "        assert os.path.exists(f\"{DATA_DIR}/market1501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    if model_name == \"action_recognition\":\n",
    "        !python3 -m pip install opencv-python numpy\n",
    "        # For rgb action recognition\n",
    "        !if [ -d tao_toolkit_recipes ]; then rm -rf tao_toolkit_recipes; fi\n",
    "        !git clone https://github.com/NVIDIA-AI-IOT/tao_toolkit_recipes\n",
    "        assert os.path.exists(\"tao_toolkit_recipes\")\n",
    "        !cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && bash ./preprocess_HMDB_RGB.sh $DATA_DIR/raw_data $DATA_DIR/processed_data \n",
    "\n",
    "        # For optical flow, comment the above 3 lines and uncomment the below (Note: for generating optical flow, a Turing or Ampere above GPU is needed.)\n",
    "        #!echo <passwd> | sudo -S apt install -y libfreeimage-dev\n",
    "        #!cp action_recognition/AppOFCuda tao_toolkit_recipes/tao_action_recognition/data_generation/\n",
    "        #!cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && bash ./preprocess_HMDB.sh $DATA_DIR/raw_data $DATA_DIR/processed_data\n",
    "\n",
    "        # download the split files and unrar\n",
    "        !wget -P $DATA_DIR --no-check-certificate http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar\n",
    "        assert os.path.exists(f\"{DATA_DIR}/test_train_splits.rar\")\n",
    "        !mkdir -p $DATA_DIR/splits && unrar x -o+ $DATA_DIR/test_train_splits.rar $DATA_DIR/splits\n",
    "        assert os.path.exists(f\"{DATA_DIR}/splits\")\n",
    "        # run split_HMDB to generate training split\n",
    "        !if [ -d $DATA_DIR/train ]; then rm -rf $DATA_DIR/train $DATA_DIR/test; fi\n",
    "        !cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && python3 ./split_dataset.py $DATA_DIR/processed_data $DATA_DIR/splits/testTrainMulti_7030_splits $DATA_DIR/train  $DATA_DIR/test\n",
    "        assert os.path.exists(f'{DATA_DIR}/train')\n",
    "        assert os.path.exists(f'{DATA_DIR}/test')\n",
    "\n",
    "        if os.path.exists(\"tao_toolkit_recipes\"):\n",
    "            shutil.rmtree(\"tao_toolkit_recipes\")\n",
    "\n",
    "        assert not os.path.exists(\"tao_toolkit_recipes\")\n",
    "\n",
    "    elif model_name == \"ml_recog\":\n",
    "        # crops images from detection set and form a classification set\n",
    "        # splits to reference/train/val/test set\n",
    "        !sudo apt-get update && sudo apt-get install gcc -y\n",
    "        !python3 -m pip install opencv-python numpy pycocotools tqdm\n",
    "        !python3 metric_learning_recognition/process_retail_product_checkout_dataset.py\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/train\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/train\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/test\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/test\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/val\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/val\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/reference\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/reference\")\n",
    "\n",
    "    elif model_name == \"ocrnet\":\n",
    "        orig_train_gt_file=os.path.join(os.getenv(\"DATA_DIR\"), \"train\", \"gt.txt\")\n",
    "        processed_train_gt_file=os.path.join(os.getenv(\"DATA_DIR\"), \"train\", \"gt_new.txt\")\n",
    "        orig_test_gt_file=os.path.join(os.getenv(\"DATA_DIR\"), \"test\", \"Challenge4_Test_Task3_GT.txt\")\n",
    "        processed_test_gt_file=os.path.join(os.getenv(\"DATA_DIR\"), \"test\", \"gt_new.txt\")\n",
    "        !python3 ocrnet/preprocess_label.py $orig_train_gt_file $processed_train_gt_file\n",
    "        !python3 ocrnet/preprocess_label.py $orig_test_gt_file $processed_test_gt_file\n",
    "\n",
    "    elif model_name == \"pointpillars\":\n",
    "        !python3 -m pip install scikit-image numpy\n",
    "        obtain_subset = True\n",
    "        if obtain_subset:\n",
    "            !python pointpillars/obtain_subset.py --source-data-dir=$DATA_DIR/training --out-data-dir=$DATA_DIR/subset_data/training/ --training True --num-images=100\n",
    "            !python pointpillars/obtain_subset.py --source-data-dir=$DATA_DIR/testing --out-data-dir=$DATA_DIR/subset_data/testing/ --num-images=100\n",
    "            DATA_DIR += \"/subset_data\"\n",
    "            os.environ['DATA_DIR'] += \"/subset_data\"\n",
    "        !mkdir -p $DATA_DIR/train/lidar $DATA_DIR/train/label $DATA_DIR/val/lidar $DATA_DIR/val/label\n",
    "\n",
    "        # Convert labels from Camera coordinate system to LIDAR coordinate system, etc\n",
    "        !python3 pointpillars/gen_lidar_points.py -p $DATA_DIR/training/velodyne \\\n",
    "                                               -c $DATA_DIR/training/calib    \\\n",
    "                                               -i $DATA_DIR/training/image_2  \\\n",
    "                                               -o $DATA_DIR/train/lidar\n",
    "        assert os.listdir(f\"{DATA_DIR}/train/lidar\")\n",
    "        # Drop DontCare class\n",
    "        !python3 pointpillars/gen_lidar_labels.py -l $DATA_DIR/training/label_2 \\\n",
    "                                               -c $DATA_DIR/training/calib \\\n",
    "                                               -o $DATA_DIR/train/label\n",
    "        # train/val split\n",
    "        !python3 pointpillars/drop_class.py $DATA_DIR/train/label DontCare\n",
    "        assert os.listdir(f\"{DATA_DIR}/train/label\")\n",
    "        # Change the val set id's if you need a different set of validation images\n",
    "        !python3 pointpillars/kitti_split.py pointpillars/val.txt \\\n",
    "                                          $DATA_DIR/train/lidar \\\n",
    "                                          $DATA_DIR/train/label \\\n",
    "                                          $DATA_DIR/val/lidar \\\n",
    "                                          $DATA_DIR/val/label\n",
    "        assert os.listdir(f\"{DATA_DIR}/val/label\")\n",
    "        assert os.listdir(f\"{DATA_DIR}/val/lidar\")\n",
    "\n",
    "    elif model_name == \"pose_classification\" and model_type == \"kinetics\":\n",
    "        !pip3 install numpy\n",
    "        # select actions\n",
    "        !python3 pose_classification/select_subset_actions.py\n",
    "        assert os.path.exists(f\"{DATA_DIR}/{model_type}/train_data.npy\") and os.path.exists(f\"{DATA_DIR}/{model_type}/train_label.pkl\") and os.path.exists(f\"{DATA_DIR}/{model_type}/val_data.npy\") and os.path.exists(f\"{DATA_DIR}/{model_type}/val_label.pkl\")\n",
    "\n",
    "    elif model_name == \"re_identification\":\n",
    "        #100 is the number of samples to be present in the subset data - you can choose any number <= total samples in the dataset\n",
    "        !python3 re_identification/obtain_subset_data.py 100\n",
    "        assert os.path.exists(f\"{DATA_DIR}/market1501/sample_train\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/market1501/sample_test\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/market1501/sample_query\")\n",
    "\n",
    "    elif model_name == \"centerpose\":\n",
    "        # Select the training categories from: bike, book, bottle, camera, cereal_box, chair, laptop, shoe\n",
    "        # Please set the \"n\" to -1 if you want to run the whole dataset training.\n",
    "        testing_categories = 'bike'\n",
    "        !pip3 install numpy opencv-python tqdm scipy==1.9.2 tensorflow==2.14.0\n",
    "        !python3 centerpose/prepare_centerpose_dataset.py \\\n",
    "                                            -c $testing_categories \\\n",
    "                                            -n 100\n",
    "        assert os.path.exists(f\"{DATA_DIR}/{testing_categories}/train\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/{testing_categories}/test\")\n",
    "        assert os.path.exists(f\"{DATA_DIR}/{testing_categories}/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tar the datasets <a class=\"anchor\" id=\"head-1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name == \"action_recognition\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/test\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/train.tar.gz train\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/test.tar.gz test\n",
    "    !tar -C $DATA_DIR/test -czf $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/test/smile.tar.gz smile\n",
    "if model_name == \"bevfusion\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/test\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/train.tar.gz train\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/test.tar.gz test\n",
    "    !tar -C $DATA_DIR/test -czf $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train/test/smile.tar.gz smile\n",
    "elif model_name == \"ml_recog\":\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/reference.tar.gz reference\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/test.tar.gz test\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/train.tar.gz train\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/val.tar.gz val\n",
    "    !rm -rf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/reference $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/test $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/train $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes/val\n",
    "\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/reference.tar.gz reference\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/test.tar.gz test\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/train.tar.gz train\n",
    "    !tar -C $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/ -czf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/val.tar.gz val\n",
    "    !rm -rf $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/reference $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/test $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/train $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes/val\n",
    "elif model_name == \"ocdnet\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_train/train $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_val/test\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_train/train.tar.gz train\n",
    "    !tar -C $DATA_DIR/train -czf $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_train/train/img.tar.gz img\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_val/test.tar.gz test\n",
    "    !tar -C $DATA_DIR/test -czf $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_val/test/img.tar.gz img\n",
    "elif model_name == \"ocrnet\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_train $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_val\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_train/train.tar.gz train\n",
    "    !cp $DATA_DIR/character_list $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_train/\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_val/test.tar.gz test\n",
    "    !cp $DATA_DIR/character_list $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_val/\n",
    "elif model_name in (\"optical_inspection\", \"visual_changenet_classify\"):\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_train $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_val $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_test\n",
    "    !tar -C $DATA_DIR/train -czf $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_train/images.tar.gz images\n",
    "    !cp $DATA_DIR/train/dataset.csv $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_train/\n",
    "    !tar -C $DATA_DIR/val -czf $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_val/images.tar.gz images\n",
    "    !cp $DATA_DIR/val/dataset.csv $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_val/\n",
    "    !tar -C $DATA_DIR/test -czf $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_test/images.tar.gz images\n",
    "    !cp $DATA_DIR/test/dataset.csv $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_test/\n",
    "elif model_name == \"visual_changenet_segment\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_visual_changenet_segment_train\n",
    "    !tar -C $DATA_DIR/LEVIR-CD256 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_visual_changenet_segment_train/A.tar.gz A\n",
    "    !tar -C $DATA_DIR/LEVIR-CD256 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_visual_changenet_segment_train/B.tar.gz B\n",
    "    !tar -C $DATA_DIR/LEVIR-CD256 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_visual_changenet_segment_train/list.tar.gz list\n",
    "    !tar -C $DATA_DIR/LEVIR-CD256 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_visual_changenet_segment_train/label.tar.gz label\n",
    "elif model_name == \"pointpillars\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_pointpillars_train\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_pointpillars_train/train.tar.gz train\n",
    "    !tar -C $DATA_DIR -czf $DATA_DIR/cloud_folders/data/purpose_built_models_pointpillars_train/val.tar.gz val\n",
    "elif model_name == \"re_identification\":\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_re_identification_train\n",
    "    !tar -C $DATA_DIR/market1501 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_re_identification_train/sample_train.tar.gz sample_train\n",
    "    !tar -C $DATA_DIR/market1501 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_re_identification_train/sample_test.tar.gz sample_test\n",
    "    !tar -C $DATA_DIR/market1501 -czf $DATA_DIR/cloud_folders/data/purpose_built_models_re_identification_train/sample_query.tar.gz sample_query\n",
    "elif model_name == 'centerpose':\n",
    "    !mkdir -p $DATA_DIR/cloud_folders/data/purpose_built_models_centerpose_train\n",
    "    !tar -C $DATA_DIR/{testing_categories} -czf $DATA_DIR/cloud_folders/data/purpose_built_models_centerpose_train/train.tar.gz train\n",
    "    !tar -C $DATA_DIR/{testing_categories} -czf $DATA_DIR/cloud_folders/data/purpose_built_models_centerpose_train/val.tar.gz val\n",
    "    !tar -C $DATA_DIR/{testing_categories} -czf $DATA_DIR/cloud_folders/data/purpose_built_models_centerpose_train/test.tar.gz test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: Upload the tar files to your cloud storage and move on to running the API requests example notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade awscli\n",
    "ACCESS_KEY=FIXME4.1\n",
    "SECRET_KEY=FIXME4.2\n",
    "BUCKET_NAME=FIXME4.3\n",
    "\n",
    "if model_name == \"action_recognition\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_action_recognition_train s3://{BUCKET_NAME}/data/purpose_built_models_action_recognition_train/$model_type --recursive\n",
    "\n",
    "elif model_name == \"ml_recog\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes s3://{BUCKET_NAME}/data/purpose_built_models_ml_recog_train/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/known_classes --recursive\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes s3://{BUCKET_NAME}/data/purpose_built_models_ml_recog_train/metric_learning_recognition/retail-product-checkout-dataset_classification_demo/unknown_classes --recursive\n",
    "\n",
    "elif model_name == \"ocdnet\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_train s3://{BUCKET_NAME}/data/purpose_built_models_ocdnet_train --recursive\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_ocdnet_val s3://{BUCKET_NAME}/data/purpose_built_models_ocdnet_val --recursive\n",
    "\n",
    "elif model_name == \"ocrnet\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_train s3://{BUCKET_NAME}/data/purpose_built_models_ocrnet_train --recursive\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_ocrnet_val s3://{BUCKET_NAME}/data/purpose_built_models_ocrnet_val --recursive\n",
    "\n",
    "elif model_name in (\"optical_inspection\", \"visual_changenet_classify\"):\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_train s3://{BUCKET_NAME}/data/purpose_built_models_optical_inspection_train --recursive\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_val s3://{BUCKET_NAME}/data/purpose_built_models_optical_inspection_val --recursive\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_optical_inspection_test s3://{BUCKET_NAME}/data/purpose_built_models_optical_inspection_test --recursive\n",
    "\n",
    "elif model_name == \"visual_changenet_segment\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_visual_changenet_segment_train s3://{BUCKET_NAME}/data/purpose_built_models_visual_changenet_segment_train --recursive\n",
    "\n",
    "elif model_name == \"pointpillars\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_pointpillars_train s3://{BUCKET_NAME}/data/purpose_built_models_pointpillars_train --recursive\n",
    "\n",
    "elif model_name == \"pose_classification\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/$model_type s3://{BUCKET_NAME}/data/purpose_built_models_pose_classification_train/$model_type --recursive\n",
    "\n",
    "elif model_name == \"re_identification\":\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_re_identification_train s3://{BUCKET_NAME}/data/purpose_built_models_re_identification_train --recursive\n",
    "\n",
    "elif model_name == 'centerpose':\n",
    "    !AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp $DATA_DIR/cloud_folders/data/purpose_built_models_centerpose_train s3://{BUCKET_NAME}/data/purpose_built_models_centerpose_train --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the paths in your API/TAO-CLIENT Notebooks\n",
    "train_dataset_path = f\"/data/purpose_built_models_{model_name}_train\" # /data/purpose_built_models_{model_name}_train/$model_type for action_recognition \n",
    "eval_dataset_path = f\"/data/purpose_built_models_{model_name}_val\"  # ocdnet, ocrnet, optical_inspection, visual_changenet_classify\n",
    "test_dataset_path = f\"/data/purpose_built_models_{model_name}_test\"  # optical_inspection, visual_changenet_classify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
