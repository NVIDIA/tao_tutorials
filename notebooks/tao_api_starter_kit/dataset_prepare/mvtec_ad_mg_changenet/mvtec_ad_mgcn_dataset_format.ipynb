{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Golden ChangeNet Dataset Formatting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the MVTec anomaly detection (mvtec-ad) dataset in the format compatible with TAO FTMS to train Multi-Golden ChangeNet classification. \n",
    "\n",
    "### Download the Dataset\n",
    "\n",
    "To get started, go to https://www.mvtec.com/company/research/datasets/mvtec-ad, agree to the license terms and click \"Download The Whole Dataset\". \n",
    "\n",
    "Place the downloaded file ```mvtec_anomaly_detection.tar.xz``` in the same folder as this notebook then run all cells. \n",
    "\n",
    "Once complete, upload the output dataset folder  ```mvtec_ad_mgcn``` to your cloud storage. You can then follow the [cradio_mg_changenet.ipynb](https://github.com/NVIDIA/tao_tutorials/blob/main/notebooks/tao_api_starter_kit/api/cradio_mg_changenet.ipynb) notebook to learn how to train a ChangeNet model with this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test -f mvtec_anomaly_detection.tar.xz && echo \"✅ Dataset Found.\" || (echo \"❌ File 'mvtec_anomaly_detection.tar.xz' not found.\"; exit 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the Dataset\n",
    "\n",
    "In this tutorial, we prepare dataset for Multi-Golden ChangeNet classification. The dataset will be structured with good/defective images and 4 golden reference images for each class, along with CSV files describing the dataset structure. The expected directory structure is:\n",
    "\n",
    "```\n",
    "|-- dataset_root:\n",
    "    |-- images\n",
    "        |-- good:\n",
    "            |-- G1.jpg\n",
    "            |-- G2.jpg\n",
    "        |-- defective:\n",
    "            |-- D1.jpg\n",
    "            |-- D2.jpg\n",
    "        |-- golden:\n",
    "            |-- G1.jpg\n",
    "            |-- G2.jpg\n",
    "            ...\n",
    "    |-- dataset.csv\n",
    "```\n",
    "\n",
    "CSV includes:\n",
    "- ``input_path``: The path to the directory containing input compare image.\n",
    "- ``golden_path``: The path to the directory containing corresponding golden reference image.\n",
    "- ``label``: The labels for the pair-wise images (Use `PASS` for non-defective components, and any other specific defect type label for defective components).\n",
    "- ``object_name``: The name of the component. It does not need to match any filenames in the golden folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this cell if it has already been run \n",
    "!mkdir mvtec_ad\n",
    "!tar -xf mvtec_anomaly_detection.tar.xz -C mvtec_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"mvtec_ad\"  # your input dataset root\n",
    "output_dir = \"mvtec_ad_mgcn\"   # where to create new structured dataset\n",
    "\n",
    "# Split percentages\n",
    "split_ratios = {\n",
    "    \"train\": 0.7,\n",
    "    \"val\": 0.15,\n",
    "    \"test\": 0.15\n",
    "}\n",
    "\n",
    "random.seed(42)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dirs():\n",
    "    \"\"\"Create split and images/ subdirs\"\"\"\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        images_path = os.path.join(output_dir, split, \"images\")\n",
    "        os.makedirs(images_path, exist_ok=True)\n",
    "\n",
    "def collect_images(class_path):\n",
    "    good_images = []\n",
    "    defective_images = []\n",
    "    for split_folder in [\"train\", \"test\"]:\n",
    "        split_path = os.path.join(class_path, split_folder)\n",
    "        if not os.path.exists(split_path):\n",
    "            continue\n",
    "        for defect_type in os.listdir(split_path):\n",
    "            defect_path = os.path.join(split_path, defect_type)\n",
    "            if not os.path.isdir(defect_path):\n",
    "                continue\n",
    "            for img in os.listdir(defect_path):\n",
    "                full_path = os.path.join(defect_path, img)\n",
    "                if defect_type == \"good\":\n",
    "                    good_images.append(full_path)\n",
    "                else:\n",
    "                    defective_images.append(full_path)\n",
    "    return good_images, defective_images\n",
    "\n",
    "def split_data(images):\n",
    "    random.shuffle(images)\n",
    "    total = len(images)\n",
    "    train_end = int(total * split_ratios[\"train\"])\n",
    "    val_end = train_end + int(total * split_ratios[\"val\"])\n",
    "    return {\n",
    "        \"train\": images[:train_end],\n",
    "        \"val\": images[train_end:val_end],\n",
    "        \"test\": images[val_end:]\n",
    "    }\n",
    "\n",
    "def copy_images(image_list, split, class_name, label):\n",
    "    output_paths = []\n",
    "    for src_path in image_list:\n",
    "        basename = os.path.basename(src_path)\n",
    "        new_filename = f\"{class_name}_{split}_{label}_{basename}\"\n",
    "\n",
    "        dst_dir = os.path.join(output_dir, split, \"images\", class_name, label)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        dst_path = os.path.join(dst_dir, new_filename)\n",
    "\n",
    "        if os.path.exists(dst_path):\n",
    "            os.remove(dst_path)\n",
    "\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        os.chmod(dst_path, 0o644)\n",
    "        output_paths.append(dst_path)\n",
    "    return output_paths\n",
    "\n",
    "def sample_golden_images(good_images):\n",
    "    if len(good_images) < 4:\n",
    "        print(f\"Warning: Only {len(good_images)} good images available, using all.\")\n",
    "        return good_images\n",
    "    return random.sample(good_images, 4)\n",
    "\n",
    "def copy_golden_images(golden_images, class_name):\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for src_path in golden_images:\n",
    "            basename = os.path.basename(src_path)\n",
    "            new_filename = f\"{class_name}_{split}_4golden_{basename}\"\n",
    "\n",
    "            dst_dir = os.path.join(output_dir, split, \"images\", class_name, \"4golden\")\n",
    "            os.makedirs(dst_dir, exist_ok=True)\n",
    "            dst_path = os.path.join(dst_dir, new_filename)\n",
    "\n",
    "            if os.path.exists(dst_path):\n",
    "                os.remove(dst_path)\n",
    "\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            os.chmod(dst_path, 0o644)\n",
    "\n",
    "def generate_csv(split):\n",
    "    csv_path = os.path.join(output_dir, split, \"dataset.csv\")\n",
    "    rows = []\n",
    "\n",
    "    images_dir = os.path.join(output_dir, split, \"images\")\n",
    "    classes = os.listdir(images_dir)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(images_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for label in [\"good\", \"defective\"]:\n",
    "            label_dir = os.path.join(class_path, label)\n",
    "            if not os.path.exists(label_dir):\n",
    "                continue\n",
    "            for img_file in os.listdir(label_dir):\n",
    "                input_path = f\"{class_name}/{label}\"\n",
    "                golden_path = f\"{class_name}/4golden\"\n",
    "                object_name = os.path.splitext(img_file)[0].zfill(3)\n",
    "                label_value = \"PASS\" if label == \"good\" else \"NG\"\n",
    "                rows.append([input_path, golden_path, label_value, object_name])\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"input_path\", \"golden_path\", \"label\", \"object_name\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "def count_images(output_dir):\n",
    "    counts = Counter()\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        images_dir = os.path.join(output_dir, split, \"images\")\n",
    "        if not os.path.exists(images_dir):\n",
    "            continue\n",
    "        for class_name in os.listdir(images_dir):\n",
    "            for label in [\"good\", \"defective\", \"4golden\"]:\n",
    "                label_dir = os.path.join(images_dir, class_name, label)\n",
    "                if not os.path.exists(label_dir):\n",
    "                    continue\n",
    "                num_files = len([\n",
    "                    f for f in os.listdir(label_dir)\n",
    "                    if os.path.isfile(os.path.join(label_dir, f))\n",
    "                ])\n",
    "                counts[(split, class_name, label)] += num_files\n",
    "\n",
    "    print(f\"{'Split':<10} {'Class':<20} {'Label':<12} {'Count':<6}\")\n",
    "    print(\"-\" * 50)\n",
    "    for (split, class_name, label), count in sorted(counts.items()):\n",
    "        print(f\"{split:<10} {class_name:<20} {label:<12} {count:<6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dirs()\n",
    "\n",
    "classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "\n",
    "for class_name in tqdm(classes):\n",
    "    class_path = os.path.join(source_dir, class_name)\n",
    "    good_images, defective_images = collect_images(class_path)\n",
    "\n",
    "    golden_images = sample_golden_images(good_images)\n",
    "    split_good = split_data(good_images)\n",
    "    split_defective = split_data(defective_images)\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        copy_images(split_good[split], split, class_name, \"good\")\n",
    "        copy_images(split_defective[split], split, class_name, \"defective\")\n",
    "\n",
    "    copy_golden_images(golden_images, class_name)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    generate_csv(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images(\"mvtec_ad_mgcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dataset format\n",
    "def print_folder_structure(directory, prefix=\"\"):\n",
    "    \"\"\"Print the folder structure of a directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} does not exist\")\n",
    "        return\n",
    "        \n",
    "    for item in sorted(os.listdir(directory)):\n",
    "        path = os.path.join(directory, item)\n",
    "        if os.path.isdir(path):\n",
    "            print(f\"{prefix}├── {item}/\")\n",
    "            print_folder_structure(path, prefix + \"│   \")\n",
    "        else:\n",
    "            # For PNG files, only print one\n",
    "            if item.endswith('.png'):\n",
    "                if not hasattr(print_folder_structure, 'png_printed'):\n",
    "                    print(f\"{prefix}├── {item}\")\n",
    "                    print_folder_structure.png_printed = True\n",
    "            else:\n",
    "                print(f\"{prefix}├── {item}\")\n",
    "\n",
    "print_folder_structure(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package dataset and Upload to Cloud\n",
    "\n",
    "FTMS requires each split to have `dataset.csv` and images folder to be archived and compressed with names `images.tar.gz`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C mvtec_ad_mgcn/test -zcf mvtec_ad_mgcn/test/images.tar.gz images\n",
    "!rm -rf mvtec_ad_mgcn/test/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C mvtec_ad_mgcn/train -zcf mvtec_ad_mgcn/train/images.tar.gz images\n",
    "!rm -rf mvtec_ad_mgcn/train/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C mvtec_ad_mgcn/val -zcf mvtec_ad_mgcn/val/images.tar.gz images\n",
    "!rm -rf mvtec_ad_mgcn/val/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Cloud Storage\n",
    "\n",
    "If using an AWS S3 bucket, you can use the following command to upload the formatted dataset through the [AWS CLI](https://aws.amazon.com/cli/): \n",
    "\n",
    "```aws s3 sync mvtec_ad_mgcn s3://bucket_name/datasets/mvtec_ad_mgcn```\n",
    "\n",
    "You should now have dataset paths in your cloud storage at \n",
    "\n",
    "- /bucket_name/datasets/mvtec_ad_mgcn/train\n",
    "- /bucket_name/datasets/mvtec_ad_mgcn/val\n",
    "- /bucket_name/datasets/mvtec_ad_mgcn/test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
