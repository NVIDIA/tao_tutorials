{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Choose between default and custom dataset in FIXME 1\n",
    "1. Assign path of DATA_DIR in FIXME 2\n",
    "1. Assign Cloud credentials in FIXME 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example dataset source and structure <a class=\"anchor\" id=\"head-1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure, and skip running** \"**Split dataset into train and val sets**\"\n",
    "```\n",
    "DATA_DIR\n",
    "├── classes.txt\n",
    "├── classmap.json\n",
    "├── images_test\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_1.jpg\n",
    "│   │   ├── image_name_2.jpg\n",
    "│   │   ├── ...\n",
    "|   |   ... \n",
    "│   └── class_name_n\n",
    "│       ├── image_name_3.jpg\n",
    "│       ├── image_name_4.jpg\n",
    "│       ├── ...\n",
    "├── images_train\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_5.jpg\n",
    "│   │   ├── image_name_6.jpg\n",
    "|   |   ...\n",
    "│   └── class_name_n\n",
    "│       ├── image_name_7.jpg\n",
    "│       ├── image_name_8.jpg\n",
    "│       ├── ...\n",
    "|\n",
    "└── images_val\n",
    "    ├── class_name_1\n",
    "    │   ├── image_name_9.jpg\n",
    "    │   ├── image_name_10.jpg\n",
    "    │   ├── ...\n",
    "    |   ...\n",
    "    └── class_name_n\n",
    "        ├── image_name_11.jpg\n",
    "        ├── image_name_12.jpg\n",
    "        ├── ...\n",
    "```\n",
    "- Each class name folder should contain the images corresponding to that class\n",
    "- Same class name folders should be present across images_test, images_train and images_val\n",
    "- classes.txt is a file which contains the names of all classes (each name in a separate line)\n",
    "- classes.json is a json file where the key is classname value is integer, for VOC: {\"aeroplane\": 0, \"bicycle\": 1, \"bird\": 2, \"boat\": 3, \"bottle\": 4, \"bus\": 5, \"car\": 6, \"cat\": 7, \"chair\": 8, \"cow\": 9, \"diningtable\": 10, \"dog\": 11, \"horse\": 12, \"motorbike\": 13, \"person\": 14, \"pottedplant\": 15, \"sheep\": 16, \"sofa\": 17, \"train\": 18, \"tvmonitor\": 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_to_be_used = \"default\" #FIXME1 #default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset\n",
    "DATA_DIR = \"/data/\" #FIXME2\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download and pre-processing <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    assert os.path.exists(os.path.join(DATA_DIR,\"VOCtrainval_11-May-2012.tar\"))\n",
    "    !tar -xf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/VOCdevkit/\"))\n",
    "    !rm -rf $DATA_DIR/split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and val sets\n",
    "!python3 -m pip install numpy pandas==1.5.1 tqdm\n",
    "!python3 classification/dataset_split.py\n",
    "assert (os.path.exists(f\"{DATA_DIR}/split/images_train/\"))\n",
    "assert (os.path.exists(f\"{DATA_DIR}/split/images_val/\"))\n",
    "assert (os.path.exists(f\"{DATA_DIR}/split/images_test/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tar files to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p $DATA_DIR/cloud_folders/data/classification_train $DATA_DIR/cloud_folders/data/classification_val $DATA_DIR/cloud_folders/data/classification_test\n",
    "\n",
    "!tar -C $DATA_DIR/split/ -czf $DATA_DIR/cloud_folders/data/classification_train/images_train.tar.gz images_train\n",
    "!tar -C $DATA_DIR/split/ -czf $DATA_DIR/cloud_folders/data/classification_val/images_val.tar.gz images_val\n",
    "!tar -C $DATA_DIR/split/ -czf $DATA_DIR/cloud_folders/data/classification_test/images_test.tar.gz images_test\n",
    "\n",
    "!cp $DATA_DIR/split/classes.txt $DATA_DIR/split/classmap.json $DATA_DIR/cloud_folders/data/classification_train/\n",
    "!cp $DATA_DIR/split/classes.txt $DATA_DIR/split/classmap.json $DATA_DIR/cloud_folders/data/classification_val/\n",
    "!cp $DATA_DIR/split/classes.txt $DATA_DIR/split/classmap.json $DATA_DIR/cloud_folders/data/classification_test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: Upload the /data folder to your cloud storage and move on to running the API requests example notebooks\n",
    "When you do a ls of your bucket it should have /data folder and the subfolders we created above within in (classification_train, classification_val, classification_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade awscli\n",
    "ACCESS_KEY=FIXME3.1\n",
    "SECRET_KEY=FIXME3.2\n",
    "BUCKET_NAME=FIXME3.3\n",
    "!AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp {DATA_DIR}/cloud_folders/data/classification_train s3://{BUCKET_NAME}/data/classification_train --recursive\n",
    "!AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp {DATA_DIR}/cloud_folders/data/classification_val s3://{BUCKET_NAME}/data/classification_val --recursive\n",
    "!AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp {DATA_DIR}/cloud_folders/data/classification_test s3://{BUCKET_NAME}/data/classification_test --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will be the paths in your API/TAO-CLIENT Notebooks\n",
    "train_dataset_path =  \"/data/classification_train\"\n",
    "eval_dataset_path = \"/data/classification_val\"\n",
    "test_dataset_path = \"/data/classification_test\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
