{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation using TAO DINO\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n",
    "\n",
    "## What is DINO?\n",
    "\n",
    "[DINO](https://arxiv.org/abs/2203.03605) is a state of the art transformer-based object detection model. Similar to Deformable DETR, DINO does not use heuristics based methods like NMS or IOU assignment found in convolution-based object detection models like Faster RCNN. Compared to Deformable DETR, DINO uses de-noising during training which can help training to converge faster.\n",
    "\n",
    "In TAO, three different types of backbone networks are supported: [ResNet50](https://arxiv.org/abs/1512.03385), [GCViT](https://arxiv.org/abs/2206.09959), and [FAN](https://arxiv.org/abs/2204.12451). In this notebook, we use the most advanced network called FAN which is also a transformer-based classification network. For more details about training FAN backbones, please refer to the classification pytorch notebook.\n",
    "\n",
    "## What is Knowledge Distillation?\n",
    "\n",
    "[Knowledge distillation](https://arxiv.org/pdf/2211.13133) is a technique generally used to transfer learned knowledge from a large model onto a smaller one for various goals, in this case to obtain a reduced size model that can improve throughput on the teacher model. Some terminologies related to this task are :\n",
    "\n",
    "* Student: The model that is intended to be trained which meets the required throughput/latency requirement.\n",
    "\n",
    "* Teacher: A larger model which generally has higher accuracy but might require further optimization to achieve required throughput/latency requirements.\n",
    "\n",
    "* Bindings: In order to formulate the distillation loss, we include the concept of binding the output of a module of the teacher to a corresponding module in the student network. \n",
    "\n",
    "* Distillation loss: To perform distillation, we can specify binding between various layers \n",
    "to compute loss, for instance in this notebook we will be showing how to distill the intermediate feature maps between DINO + FAN Small to a DINO + Resnet50 using L2 loss for the binding 'model.backbone.0.body'.\n",
    "\n",
    "### Sample prediction of FAN-Tiny + DINO model\n",
    "<img align=\"center\" src=\"sample.jpg\" width=\"960\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and train an DINO model on COCO dataset with distillation\n",
    "* Evaluate the distilled model\n",
    "* Run inference with the distilled model and visualize the result\n",
    "* Export the distilled model to a .onnx file for deployment to DeepStream\n",
    "* Generate TensorRT engine using tao-deploy and verify the engine through evaluation\n",
    "\n",
    "At the end of this notebook, you will have generated a distilled `dino` model\n",
    "which you may deploy via [DeepStream](https://developer.nvidia.com/deepstream-sdk).\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of DINO using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2)\n",
    "3. [Provide distillation specification](#head-3)\n",
    "4. [Run TAO distillation](#head-4)\n",
    "5. [Evaluate a trained model](#head-5)\n",
    "6. [Visualize inferences](#head-6)\n",
    "7. [Deploy](#head-7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/dino/results`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "%env LOCAL_PROJECT_DIR=/path/to/local/tao-experiments\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"dino\", \"results\")\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/dino\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid()),\n",
    "        \"network\": \"host\"\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.10. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.7, <=3.10.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.10.x. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the COCO dataset for the tutorial. The following script will download COCO dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $HOST_DATA_DIR\n",
    "# Download the data\n",
    "!bash $HOST_SPECS_DIR/download_coco.sh $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "!ls -l $HOST_DATA_DIR/raw-data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download pre-trained models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.machine() == \"x86_64\":\n",
    "    os.environ[\"CLI\"]=\"ngccli_linux.zip\"\n",
    "else:\n",
    "    os.environ[\"CLI\"]=\"ngccli_arm64.zip\"\n",
    "\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -uqq \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tao/pretrained_dino_coco:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be downloading the pretrained Resnet50 + DINO weights trained on COCO which will be our student network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_dino_coco:dino_resnet_50_trainable_v1.0 --dest $LOCAL_PROJECT_DIR/dino/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_PROJECT_DIR/dino/pretrained_dino_coco_vdino_resnet_50_trainable_v1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will download the pretrained FAN Small + DINO weights pretrained on COCO which will be our teacher network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the teacher model\n",
    "!ngc registry model download-version \"nvidia/tao/pretrained_dino_coco:dino_fan_small_trainable_v1.0\" --dest $LOCAL_PROJECT_DIR/dino/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_PROJECT_DIR/dino/pretrained_dino_coco_vdino_fan_small_trainable_v1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide distillation specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * train_data_sources:\n",
    "        * image_dir: annotation file for train data. required to be in COCO json format\n",
    "        * json_file: the root directory for train images\n",
    "    * val_data_sources: \n",
    "        * image_dir: the root directory for validation images\n",
    "        * json_file: annotation file for validation data. required to be in COCO json format\n",
    "    * num_classes: number of classes of you training data\n",
    "    * batch_size: batch size for dataloader\n",
    "    * workers: number of workers to do data loading\n",
    "* model: configure the model setting\n",
    "    * pretrained_backbone_path: path to the pretrained backbone model. ResNet50, FAN-variants, and GCViT-variants are supported\n",
    "    * num_feature_levels: number of feature levels used from backbone\n",
    "    * dec_layers: number of decoder layers\n",
    "    * enc_layers: number of encoder layers\n",
    "    * num_queries: number of queries for the model\n",
    "    * num_select: number of top-k proposals to select from\n",
    "    * use_dn: flag to enable denoising during training\n",
    "    * dropout_ratio: drop out ratio\n",
    "* train: configure the training hyperparameters\n",
    "    * num_gpus: number of gpus \n",
    "    * num_nodes: number of nodes (num_nodes=1 for single node)\n",
    "    * val_interval: validation interval\n",
    "    * optim:\n",
    "        * lr_backbone: learning rate for backbone\n",
    "        * lr: learning rate for the rest of the model\n",
    "        * lr_steps: learning rate decay step milestone (MultiStep)\n",
    "    * num_epochs: number of epochs\n",
    "    * activation_checkpoint: recompute activations in the backward to save GPU memory. Default is `True`.\n",
    "    * precision: If set to fp16, the training is run on Automatic Mixed Precision (AMP)\n",
    "    * distributed_strategy: Default is `ddp`. `ddp_sharded` is also supported.\n",
    "* distill: configure the distillation hyperparametere\n",
    "    * teacher: configure the teacher model setting\n",
    "        * pretrained_backbone_path: path to the pretrained backbone model. ResNet50, FAN-variants, and GCViT-variants are supported\n",
    "        * num_feature_levels: number of feature levels used from backbone\n",
    "        * dec_layers: number of decoder layers\n",
    "        * enc_layers: number of encoder layers\n",
    "        * num_queries: number of queries for the model\n",
    "        * num_select: number of top-k proposals to select from\n",
    "        * use_dn: flag to enable denoising during training\n",
    "        * dropout_ratio: drop out ratio\n",
    "    * pretrained_teacher_model_path: path to the pretrained DINO model. FAN-variants are supported\n",
    "    * bindings: list of bindings to use for distillation loss\n",
    "        * teacher_module_name: name of module in teacher to use in loss computation\n",
    "        * student_model_name: name of module in student to use in loss computation\n",
    "        * criterion: loss type to use, L1, L2, KL(Divergence) losses supported\n",
    "        * weight: weighting coefficient to use for binding in loss computation\n",
    "\n",
    "* **Note that the sample spec is not meant to produce SOTA accuracy on COCO. To reproduce SOTA, you should set `num_feature_levels` as 4 to match the original params. In addition, the use of NVImageNet weight also cause a slightly lower mAP when compared with ImageNet weight.**\n",
    "\n",
    "Please refer to the TAO documentation about DINO to get all the parameters that are configurable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $HOST_SPECS_DIR/distill.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO distillation <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n",
    "* *WARNING*: [according to the orirginal paper](https://arxiv.org/abs/2203.03605), COCO training was conducted using 8 A100 gpus. As a result, **we highly recommend that you run training with multiple high-end gpus (e.g. V100, A100)**\n",
    "* COCO per-epoch training time on a single GPU (the hours may vary depending on the data location, network speed, and etc).\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>GPU Type</th>\n",
    "    <th>Time (hrs)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1 x V100 32GB</td>\n",
    "    <td>28</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1 x A100 80GB</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "* For this demonstration, we changed the architectures from the original implementation so that the training can be completed faster (e.g. num_queries 900 -> 300, num_feature_levels 4 -> 2, and etc).\n",
    "* Unlike the [original DINO paper](https://arxiv.org/abs/2203.03605), we used more advanced backbone called [FAN](https://arxiv.org/abs/2204.12451) that has proven to achieve higher downstream results compared to ResNet, Swin, and ConvNext. \n",
    "* If you wish to speed up training, you may try to set `train.precision=fp16` for mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR = /data\n",
    "%env SPECS_DIR = /specs\n",
    "%env RESULTS_DIR = /results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, change num_gpus in train.yaml based on your machine.\")\n",
    "print(\"For multi-node, change num_gpus and num_nodes in train.yaml based on your machine.\")\n",
    "# If you face out of memory issue, you may reduce the batch size in the spec file by passing dataset.batch_size=2\n",
    "!tao model dino distill \\\n",
    "          -e $SPECS_DIR/distill.yaml \\\n",
    "          results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distillation checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $HOST_RESULTS_DIR/distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set NUM_EPOCH to the epoch corresponding to any saved checkpoint\n",
    "# %env NUM_EPOCH=029\n",
    "\n",
    "# Get the name of the checkpoint corresponding to your set epoch\n",
    "# !ls $HOST_RESULTS_DIR/distill/*.pth\n",
    "# tmp=!ls $HOST_RESULTS_DIR/distill/*.pth | grep epoch_$NUM_EPOCH\n",
    "# %env CHECKPOINT={tmp[0]}\n",
    "\n",
    "# Or get the latest checkpoint\n",
    "os.environ[\"CHECKPOINT\"] = os.path.join(os.getenv(\"HOST_RESULTS_DIR\"), \"distill/dino_model_latest.pth\")\n",
    "\n",
    "print('Rename a distilled model: ')\n",
    "print('---------------------')\n",
    "!cp $CHECKPOINT $HOST_RESULTS_DIR/distill/dino_model.pth\n",
    "!ls -ltrh $HOST_RESULTS_DIR/distill/dino_model.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a distilled model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the mAP metric.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration.\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * test_data_sources:\n",
    "        * image_dir: the root directory for evaluatation images    \n",
    "        * json_file: annotation file for evaluatation data. required to be in COCO json format.\n",
    "    * num_classes: number of classes you used for training\n",
    "    * eval_class_ids: classes you would like to evaluate. \\\n",
    "                    Note that current config file will evaluate only on class 1 (person in COCO dataset)\\\n",
    "                    If you remove this from config file, it will evaluate and compute the average over entire classes.\n",
    "    * batch_size\n",
    "    * workers\n",
    "* evaluate:\n",
    "    * num_gpus: number of gpus\n",
    "    * conf_threshold: a threshold for confidence scores\n",
    "\n",
    "* **NOTE: You need to change the model path in evaluate.yaml file based on your setting.**\n",
    "\n",
    "This experiment should give close to a 1% improvement over DINO + RN50 trained from scratch on Validation mAP50 in COCO. Users can further experiment with bindings/criterions for their scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TAO model\n",
    "!tao model dino evaluate \\\n",
    "            -e $SPECS_DIR/evaluate_distill.yaml \\\n",
    "            evaluate.checkpoint=$RESULTS_DIR/distill/dino_model.pth \\\n",
    "            results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces annotated image outputs and txt files that contain prediction information.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * infer_data_sources:\n",
    "        * image_dir: the list of directories for inference images\n",
    "        * classmap: \n",
    "    * num_classes: number of classes you used for training\n",
    "    * batch_size\n",
    "    * workers\n",
    "* inference\n",
    "    * conf_threshold: the confidence score threshold\n",
    "    * color_map: the color mapping for each class. The predicted bbox will be drawn with mapped color for each class\n",
    "* **NOTE: You need to change the model path in infer.yaml file based on your setting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy classmap to annotation directory\n",
    "!cp $HOST_SPECS_DIR/classmap.txt $HOST_DATA_DIR/raw-data/annotations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model dino inference \\\n",
    "        -e $SPECS_DIR/infer_distill.yaml \\\n",
    "        inference.checkpoint=$RESULTS_DIR/distill/dino_model.pth \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg']\n",
    "\n",
    "def visualize_images(output_path, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"inference\", \"images_annotated\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the RGB model to ONNX model\n",
    "!tao model dino export \\\n",
    "           -e $SPECS_DIR/export_distill.yaml \\\n",
    "           export.checkpoint=$RESULTS_DIR/distill/dino_model.pth \\\n",
    "           export.onnx_file=$RESULTS_DIR/export/dino_model.onnx \\\n",
    "           results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TensorRT engine using tao deploy\n",
    "!tao deploy dino gen_trt_engine -e $SPECS_DIR/gen_trt_engine.yaml \\\n",
    "                               gen_trt_engine.onnx_file=$RESULTS_DIR/export/dino_model.onnx \\\n",
    "                               gen_trt_engine.trt_engine=$RESULTS_DIR/gen_trt_engine/dino_model.engine \\\n",
    "                               results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with generated TensorRT engine\n",
    "!tao deploy dino evaluate -e $SPECS_DIR/evaluate_distill.yaml \\\n",
    "                              evaluate.trt_engine=$RESULTS_DIR/gen_trt_engine/dino_model.engine \\\n",
    "                              results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with generated TensorRT engine\n",
    "!tao deploy dino inference -e $SPECS_DIR/infer_distill.yaml \\\n",
    "                              inference.trt_engine=$RESULTS_DIR/gen_trt_engine/dino_model.engine \\\n",
    "                              results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"trt_inference\", \"images_annotated\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
