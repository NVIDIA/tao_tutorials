{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation (SDG) using TAO StyleGAN-XL\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is StyleGAN-XL ?\n",
    "\n",
    "StyleGAN-XL is a synthetic dataset generation model for us to generate more images for downstream task such as classification and segmentation. The techniques could be powerful especially when the existing real dataset is not sufficient for training downstream models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generated images of the trained StyleGAN-XL using endoscopy images\n",
    "<div style=\"padding-left: 100px;\">\n",
    "<img src=\"images/fakes011290.png\" width=\"1500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to use TAO to `train`, `evaluate`, and `inference` with StyleGAN-XL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/bevfusion/results`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "%env LOCAL_PROJECT_DIR=/raid/path/to/local/tao-experiments\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"stylegan_xl\", \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"stylegan_xl\", \"results\")\n",
    "os.environ[\"LOCAL_CACHE_DIR\"] = os.path.join(os.environ['HOME'], \".cache\")\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/bevfusion\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.|\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR = /data\n",
    "%env SPECS_DIR = /specs\n",
    "%env RESULTS_DIR = /results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"LOCAL_CACHE_DIR\"],\n",
    "           \"destination\": \"/.cache\"\n",
    "       },\n",
    "   ],\n",
    "   \"Envs\": [\n",
    "        {\n",
    "            \"variable\": \"TAO_TOOLKIT_CACHE\",\n",
    "            \"value\": \"/.cache\",\n",
    "        }\n",
    "    ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        \"network\": \"host\"\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.7, <=3.10.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info\n",
    "!tao model -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the https://www.kaggle.com/datasets/fantacher/neu-metal-surface-defects-data dataset to train a StyleGAN-XL model. This dataset consists of six classes of images, organized into a folder structure where each class corresponds to a subfolder containing its respective images. The hierarchy of the `neu-metal-surface-defects-data` dataset is as follows:\n",
    "\n",
    "<pre style=\"font-family: 'Courier New', monospace; font-size: 14px; line-height: 1.5; color: #333;\">\n",
    "neu-metal-surface-defects-data/\n",
    "├── class_1/\n",
    "│   ├── class_1_image1.png\n",
    "│   ├── class_1_image2.png\n",
    "│   ├── ...\n",
    "│   ├── class_1_imageN.png\n",
    "├── class_2/\n",
    "│   ├── class_2_image1.png\n",
    "│   ├── class_2_image2.png\n",
    "│   ├── ...\n",
    "│   ├── class_2_imageN.png\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "! curl -L -o $HOST_DATA_DIR/neu-metal-surface-defects-data.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/fantacher/neu-metal-surface-defects-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_file_path = os.path.join(os.environ[\"HOST_DATA_DIR\"], \"neu-metal-surface-defects-data.zip\")\n",
    "\n",
    "# Directory to extract files\n",
    "extract_dir = os.environ[\"HOST_DATA_DIR\"]\n",
    "\n",
    "# Desired folder name\n",
    "desired_folder_name = \"NEU_Metal_Surface_Defects_Data\"\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Original extracted folder name\n",
    "original_folder_name = os.path.join(extract_dir, \"NEU Metal Surface Defects Data\")\n",
    "\n",
    "# Rename the folder\n",
    "renamed_folder = os.path.join(extract_dir, desired_folder_name)\n",
    "if os.path.exists(renamed_folder):\n",
    "    shutil.rmtree(renamed_folder)  # Remove if the target folder already exists\n",
    "os.rename(original_folder_name, renamed_folder)\n",
    "\n",
    "# Convert .bmp files to .png\n",
    "for root, dirs, files in os.walk(renamed_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".bmp\"):\n",
    "            bmp_file_path = os.path.join(root, file)\n",
    "            png_file_path = os.path.join(root, file.replace(\".bmp\", \".png\"))\n",
    "\n",
    "            # Convert BMP to PNG\n",
    "            with Image.open(bmp_file_path) as img:\n",
    "                img.convert(\"RGB\").save(png_file_path, \"PNG\")\n",
    "\n",
    "            # Remove the original BMP file\n",
    "            os.remove(bmp_file_path)\n",
    "\n",
    "print(f\"Extraction and conversion completed. Files are in: {renamed_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -all $HOST_DATA_DIR/NEU_Metal_Surface_Defects_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a dataset tool entrypoint, `dataset_convert`, to convert the dataset folder described above into a zipped file. This tool serves two main purposes:\n",
    "\n",
    "1. **Dataset Portability and Performance**: Zipping datasets simplifies transferring them between file servers and clusters and may improve training performance when using network file systems.  \n",
    "2. **Image Preprocessing**: StyleGAN-XL requires square, fixed-resolution images for training. The tool can crop and/or resize images to meet the resolution requirements of StyleGAN-XL's progressive training workflow. This training process involves starting with lower resolutions (e.g., `16x16`) and progressively increasing to higher resolutions (e.g., `256x256`). Consequently, we need multiple versions of the dataset, such as `16x16`, `32x32`, `64x64`, `128x128`, and `256x256`.\n",
    "\n",
    "#### Sample Execution:  \n",
    "To create a zipped dataset with `16x16` resolution images:  \n",
    "```bash\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_16.zip \\\n",
    "                  resolution=[16,16]\n",
    "```\n",
    "\n",
    "By default, the tool resizes images to the target resolution, which may distort the images. To avoid distortion, you can enable the `center-crop` option before resizing by adding `transform=center-crop`. Note that while center-cropping preserves image proportions, it may crop out important border information, especially for images with aspect ratios far from 1:1.  \n",
    "\n",
    "Example with center-cropping:  \n",
    "```bash\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_16.zip \\\n",
    "                  resolution=[16,16] \\\n",
    "                  transform=center-crop\n",
    "```\n",
    "\n",
    "#### Output Structure:  \n",
    "The zipped file reorganizes the dataset by renaming folders and files for better organization and efficiency. For instance, zipping the `NEU_Metal_Surface_Defects_Data/train` folder into `train_16.zip` produces the following structure:\n",
    "\n",
    "<pre style=\"font-family: 'Courier New', monospace; font-size: 14px; line-height: 1.5; color: #333;\">\n",
    "Dataset/\n",
    "├── 00000/\n",
    "│   ├── img00000000.png\n",
    "│   ├── img00000001.png\n",
    "│   ├── ...\n",
    "│   ├── img00000999.png\n",
    "├── 00001/\n",
    "│   ├── img00001000.png\n",
    "│   ├── img00001001.png\n",
    "│   ├── ...\n",
    "│   ├── img00001999.png\n",
    "├── ...\n",
    "├── 0000N/\n",
    "│   ├── img0000N000.png\n",
    "│   ├── img0000N001.png\n",
    "│   ├── ...\n",
    "│   ├── img0000N999.png\n",
    "├── dataset.json\n",
    "├── label_map.json  \n",
    "</pre>\n",
    "\n",
    "#### Metadata Files:\n",
    "1. **`dataset.json`**: This file maps images to their corresponding labels for TAO's StyleGAN-XL data loader. Example:  \n",
    "    ```json\n",
    "    {\n",
    "        \"labels\": [\n",
    "            [\"00000/img00000000.png\", 2],\n",
    "            [\"00000/img00000001.png\", 4],\n",
    "            ...\n",
    "            [\"00049/img00049999.png\", 1]\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "2. **`label_map.json`**: This file maps the original dataset's folder names to integer labels. Example:  \n",
    "    ```json\n",
    "    {\n",
    "        \"class_5\": 0,\n",
    "        \"class_0\": 1,\n",
    "        \"class_1\": 2,\n",
    "        \"class_4\": 3,\n",
    "        \"class_2\": 4,\n",
    "        \"class_3\": 5\n",
    "    }\n",
    "    ```\n",
    "\n",
    "#### Label Mapping:  \n",
    "The integer labels in `dataset.json` (e.g., `2`, `4`, `1`) differ from the original subfolder names. You can use `label_map.json` to interpret the mapping. For example:\n",
    "- Label `2` corresponds to the original `class_1` folder.\n",
    "- Label `4` corresponds to `class_2`.\n",
    "- Label `1` corresponds to `class_0`.\n",
    "\n",
    "#### Next Steps:  \n",
    "We will use `dataset_convert` script to generate zipped versions of the dataset for all required resolutions (`16x16`, `32x32`, `64x64`, `128x128`, and `256x256`) from the original `NEU_Metal_Surface_Defects_Data/train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess and generate train_16.zip\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_16.zip \\\n",
    "                  resolution=[16,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess and generate train_32.zip\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_32.zip \\\n",
    "                  resolution=[32,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess and generate train_64.zip\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_64.zip \\\n",
    "                  resolution=[64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess and generate train_128.zip\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_128.zip \\\n",
    "                  resolution=[128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess and generate train_256.zip\n",
    "!tao model stylegan_xl dataset_convert \\\n",
    "                  -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                  source=$DATA_DIR/NEU_Metal_Surface_Defects_Data/train \\\n",
    "                  results_dir=$RESULTS_DIR/dataset_convert \\\n",
    "                  dest_file_name=train_256.zip \\\n",
    "                  resolution=[256,256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare required pretrained modules for training StyleGAN-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need 2 pretrained modules for training StyleGAN-XL: \n",
    "1. tf_efficientnet_lite0_embed.pth: Input embedding for input fed into the StyleGAN-XL\n",
    "2. InceptionV3.pth: Evaluating FID score when training StyleGAN-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd $HOST_DATA_DIR && \\\n",
    "    git clone https://github.com/autonomousvision/stylegan-xl.git && \\\n",
    "    cd stylegan-xl && wget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "sys.path.append(os.path.join(os.environ[\"HOST_DATA_DIR\"], 'stylegan-xl'))\n",
    "InceptionV3_file_path = os.path.join(os.environ[\"HOST_DATA_DIR\"], 'stylegan-xl', \"inception-2015-12-05.pkl\")\n",
    "tf_efficientnet_lite0_embed_file_path = os.path.join(os.environ[\"HOST_DATA_DIR\"], 'stylegan-xl', \"in_embeddings/tf_efficientnet_lite0.pkl\")\n",
    "\n",
    "# Try loading the checkpoint using pickle\n",
    "with open(InceptionV3_file_path, 'rb') as f:\n",
    "    InceptionV3 = pickle.load(f)\n",
    "with open(tf_efficientnet_lite0_embed_file_path, 'rb') as f:\n",
    "    tf_efficientnet_lite0_embed = pickle.load(f)\n",
    "\n",
    "torch.save(InceptionV3.state_dict(), os.path.join(os.environ[\"HOST_DATA_DIR\"], \"InceptionV3.pth\"))\n",
    "torch.save(tf_efficientnet_lite0_embed['embed'].state_dict(), os.path.join(os.environ[\"HOST_DATA_DIR\"], \"tf_efficientnet_lite0_embed.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run TAO train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Provide training specification <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "We provide specification files to configure the training process. Please ensure you update the following settings to suit your environment:  \n",
    "\n",
    "1. **`results_dir`**: Update this field if the default path is not suitable for your setup.  \n",
    "2. **Dataset Paths**: Modify the `images_dir` under `train_dataset`, `validation_dataset`, and `test_dataset` to point to **your** dataset zip files as outlined in **Section 2.2**.\n",
    "\n",
    "#### Progressive Training  \n",
    "Since StyleGAN-XL utilizes a progressive training approach, we have prepared six specification files to accommodate this workflow:  \n",
    "- `stylegan_xl_16.yaml`  \n",
    "- `stylegan_xl_16_to32.yaml`  \n",
    "- `stylegan_xl_16_to32_to64.yaml`  \n",
    "- `stylegan_xl_16_to32_to64_to128.yaml`  \n",
    "- `stylegan_xl_16_to32_to64_to128_to256.yaml`    \n",
    "\n",
    "These files allow the model to progressively learn to generate images at increasing resolutions, ultimately achieving a final resolution of `256x256`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training the Stem (Base Low Resolution, e.g., 16x16) Model\n",
    "\n",
    "In the specification file, the `superres` key is set to `False`, indicating that this is the base stem model. The `resolution` for the stem model is configured to `16`, matching the `img_resolution` of the dataset. The training dataset used is the `train_16.zip` file, which contains images at a resolution of 16x16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl train \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16 \\\n",
    "                  model.input_embeddings_path=$DATA_DIR/tf_efficientnet_lite0_embed.pth \\\n",
    "                  model.stylegan.metrics.inception_fid_path=$DATA_DIR/InceptionV3.pth \\\n",
    "                  dataset.stylegan.train_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_16.zip \\\n",
    "                  dataset.stylegan.validation_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_16.zip \\\n",
    "                  dataset.stylegan.batch_gpu_size=8 \\\n",
    "                  dataset.batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training a Super-Resolution Model at 2x Resolution (32x32)\n",
    "\n",
    "In the specification file, the `superres` key is set to `True`, and the `up_factor` is `[2]`, indicating a single upscaling step with a 2x magnification. The image dataset has an `img_resolution` of `32`, corresponding to the zipped dataset file `train_32.zip`. For this training phase, we use the latest checkpoint, `/results/stylegan_xl_16/train/styleganxl_model_latest.pth`, as the base model, which was trained at the previous resolution (16x16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl train \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16_to32.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16_to32 \\\n",
    "                  model.input_embeddings_path=$DATA_DIR/tf_efficientnet_lite0_embed.pth \\\n",
    "                  model.stylegan.metrics.inception_fid_path=$DATA_DIR/InceptionV3.pth \\\n",
    "                  dataset.stylegan.train_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_32.zip \\\n",
    "                  dataset.stylegan.validation_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_32.zip \\\n",
    "                  dataset.stylegan.batch_gpu_size=8 \\\n",
    "                  dataset.batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training a Super-Resolution Model at 4x Resolution (64x64)\n",
    "\n",
    "In the specification file, the `superres` key is set to `True`, and the `up_factor` is `[2, 2]`, indicating two upscaling steps, each with a 2x magnification. The image dataset has an `img_resolution` of `64`, corresponding to the zipped dataset file `train_64.zip`. For this training phase, we use the latest checkpoint, `/results/stylegan_xl_16_to32/train/styleganxl_model_latest.pth`, as the base model, which was trained at the previous resolution (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl train \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16_to32_to64.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16_to32_to64 \\\n",
    "                  model.input_embeddings_path=$DATA_DIR/tf_efficientnet_lite0_embed.pth \\\n",
    "                  model.stylegan.metrics.inception_fid_path=$DATA_DIR/InceptionV3.pth \\\n",
    "                  dataset.stylegan.train_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_64.zip \\\n",
    "                  dataset.stylegan.validation_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_64.zip \\\n",
    "                  dataset.stylegan.batch_gpu_size=8 \\\n",
    "                  dataset.batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training a Super-Resolution Model at 8x Resolution (128x128)\n",
    "\n",
    "In the specification file, the `superres` key is set to `True`, and the `up_factor` is `[2, 2, 2]`, indicating three upscaling steps, each with a 2x magnification. The image dataset has an `img_resolution` of `128`, corresponding to the zipped dataset file `train_128.zip`. For this training phase, we use the latest checkpoint, `/results/stylegan_xl_16_to32_to64/train/styleganxl_model_latest.pth`, as the base model, which was trained at the previous resolution (64x64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl train \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16_to32_to64_to128.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16_to32_to64_to128 \\\n",
    "                  model.input_embeddings_path=$DATA_DIR/tf_efficientnet_lite0_embed.pth \\\n",
    "                  model.stylegan.metrics.inception_fid_path=$DATA_DIR/InceptionV3.pth \\\n",
    "                  dataset.stylegan.train_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_128.zip \\\n",
    "                  dataset.stylegan.validation_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_128.zip \\\n",
    "                  dataset.stylegan.batch_gpu_size=8 \\\n",
    "                  dataset.batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training a Super-Resolution Model at 16x Resolution (256x256)\n",
    "\n",
    "In the specification file, the `superres` key is set to `True`, and the `up_factor` is `[2, 2, 2, 2]`, indicating four upscaling steps, each with a 2x magnification. The image dataset has an `img_resolution` of `256`, corresponding to the zipped dataset file `train_256.zip`. For this training phase, we use the latest checkpoint, `/results/stylegan_xl_16_to32_to64_to128/train/styleganxl_model_latest.pth`, as the base model, which was trained at the previous resolution (128x128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl train \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16_to32_to64_to128_to256.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256 \\\n",
    "                  model.input_embeddings_path=$DATA_DIR/tf_efficientnet_lite0_embed.pth \\\n",
    "                  model.stylegan.metrics.inception_fid_path=$DATA_DIR/InceptionV3.pth \\\n",
    "                  dataset.stylegan.train_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_256.zip \\\n",
    "                  dataset.stylegan.validation_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_256.zip \\\n",
    "                  dataset.stylegan.batch_gpu_size=8 \\\n",
    "                  dataset.batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO Evaluation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "In this section, we run the `stylegan_xl` evaluation script to assess the performance of the trained StyleGAN-XL model by comparing the FID scores between real and synthetic images. Assuming that we have trained the 256x256 model, we set the `checkpoint` path in the `evaluate` section to the location of the `$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256/train/styleganxl_model_latest.pth` checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl evaluate \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16_to32_to64_to128_to256.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256 \\\n",
    "                  evaluate.checkpoint=$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256/train/styleganxl_model_latest.pth \\\n",
    "                  dataset.stylegan.test_dataset.images_dir=$RESULTS_DIR/dataset_convert/train_256.zip \\\n",
    "                  dataset.batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run TAO Inference <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "In this section, we run the `stylegan_xl` inference script to generate synthetic images using the trained StyleGAN-XL model. Assuming that we have trained the 256x256 model, we set the `checkpoint` path in the `inference` section to the location of the `$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256/train/styleganxl_model_latest.pth` checkpoint file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This StyleGAN-XL model is trained as a conditional GAN that can generate images from one of the six classes in the `NEU_Metal_Surface_Defects_Data/train` dataset. In the `stylegan_xl_16_to32_to64_to128_to256.yaml` specification file, the `class_idx` is currently set to `0`, but you can override it to any value between `0` and `5` to generate images from different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model stylegan_xl inference \\\n",
    "                  -e $SPECS_DIR/stylegan_xl_16_to32_to64_to128_to256.yaml \\\n",
    "                  results_dir=$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256 \\\n",
    "                  inference.checkpoint=$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256/train/styleganxl_model_latest.pth \\\n",
    "                  dataset.stylegan.infer_dataset.start_seed=0 \\\n",
    "                  dataset.stylegan.infer_dataset.end_seed=50 \\\n",
    "                  inference.class_idx=0 \\\n",
    "                  dataset.batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the generated images from `$RESULTS_DIR` directory. The default path is `$RESULTS_DIR/stylegan_xl_16_to32_to64_to128_to256/inference/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tao_launcher",
   "language": "python",
   "name": "tao_launcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
