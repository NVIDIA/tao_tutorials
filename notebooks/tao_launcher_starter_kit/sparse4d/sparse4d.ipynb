{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Multi-Camera Detection & Tracking using TAO Sparse4D\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n",
    "\n",
    "## What is Sparse4D?\n",
    "\n",
    "[Sparse4D](https://arxiv.org/pdf/2311.11722) is a state of the art Multi-Camera Multi-Target 3D (MTMC-3D) detection & tracking model. We adpot & modify the Sparse4D from the AV usecase to an indoor environment usecase for larger spaces such as warehouses, retail stores, and hospitals which usually consist of static cameras. \n",
    "\n",
    "Our pretrained models support classes such as Person, Humanoids, Boxes, Pallets, Crates & Autonomous Mobile Robots (AMRs). We utlize the Resnet-101 backbone for our training.\n",
    "\n",
    "### Sample prediction of Sparse4D model\n",
    "<img width=\"800\" align=\"center\" src=\"https://raw.githubusercontent.com/vpraveen-nv/model_card_images/main/cv/purpose_built_models/sparse4d/sample_output.gif\">\n",
    "\n",
    "\n",
    "The above output shows a warehouse environment recorded by four temporally synchronized cameras. The scene includes a variety of real-world warehouse assets such as Person, Humanoids, Boxes, Pallets, Crates, and Autonomous Mobile Robots (AMRs). Our model accurately detects each object, fits a 3D bounding box around it, and assigns a unique ID per object class across all cameras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and train an Sparse4D model on MTMC Tracking 2025 dataset\n",
    "* Evaluate the trained model\n",
    "* Run inference with the trained model and visualize the result\n",
    "* Export the trained model to a .onnx file for deployment to DeepStream\n",
    "\n",
    "At the end of this notebook, you will have generated a trained `Sparse4D` model\n",
    "which you may deploy via [DeepStream](https://developer.nvidia.com/deepstream-sdk).\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of Sparse4D using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate a trained model](#head-5)\n",
    "6. [Visualize inferences](#head-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/sparse4d/results`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "%env LOCAL_PROJECT_DIR=./\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"sparse4d\", \"results\")\n",
    "os.environ[\"HOST_MODEL_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"sparse4d\", \"model\")\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/sparse4d\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR\n",
    "! mkdir -p $HOST_MODEL_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_MODEL_DIR\"],\n",
    "           \"destination\": \"/model\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        \"network\": \"host\"\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao info --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `MTMC Tracking 2025` dataset in this finetuning tutorial for trianing, evaluation & inference. This is a synthetically generated dataset from Nvidia Omniverse & belongs in the Nvidia PhysicalAI smart spaces category. You may expand on the dataset size by picking multiple different scenes from the MTMC Tracking 2025 dataset & having unique & disjoint sets for training, validation & testing.\n",
    " \n",
    "The dataset consits of multiple scenes such as warehouse, retail, hospital, etc. In this tutorial, we will utilize the Warehouse scene `Warehouse_014` from the training set which consits of temporally consistent multi-camera videos (mp4), depth maps (.h5), camera calibration defined in OV coordiantes & ground truth consists of 3D bounding boxes & object IDs present for all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $HOST_DATA_DIR\n",
    "\n",
    "# Download the MTMC Tracking 2025 from Hugging Face\n",
    "!pip install --upgrade huggingface-hub[cli]\n",
    "\n",
    "# # Run script\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "host_data_dir_path = os.getenv(\"HOST_DATA_DIR\")\n",
    "\n",
    "# Download train dataset\n",
    "snapshot_download(\n",
    "    repo_id=\"nvidia/PhysicalAI-SmartSpaces\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"MTMC_Tracking_2025/train/Warehouse_014/*\",\n",
    "    local_dir=host_data_dir_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset is downloaded correctly (Folder should have a calibration.json, ground_truth.json, videos & depth_maps directory)\n",
    "!ls -lh $HOST_DATA_DIR/MTMC_Tracking_2025/train/Warehouse_014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR = /data\n",
    "%env MODEL_DIR = /model\n",
    "%env SPECS_DIR = /specs\n",
    "%env RESULTS_DIR = /results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert the dataset from a OV format to a suitable training fromat. The Sparse4D model consumes raw images & pkl files consisting of image paths, calibration & GT for its trianing. The TAO dataset convert tool will help us perform this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $HOST_DATA_DIR/MTMC_Tracking_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate annotation pickle (OVPKL) files for training using TAO DataService\n",
    "!ls $HOST_SPECS_DIR/convert.yaml\n",
    "!mkdir -p $HOST_DATA_DIR/anno_pkls\n",
    "\n",
    "# Create annotation pkl files on all frames. (Conversion process will take ~20 minutes to process a 12 camera scene with 9000 frames each)\n",
    "!tao dataset annotations convert \\\n",
    "        -e $SPECS_DIR/convert.yaml \\\n",
    "        aicity.root=$DATA_DIR/MTMC_Tracking_2025 \\\n",
    "        aicity.camera_grouping_mode=random \\\n",
    "        results_dir=$DATA_DIR/anno_pkls\n",
    "\n",
    "# List down all training pkl files.\n",
    "print(\"Listing all training annotation pkl files:\")\n",
    "!ls $HOST_DATA_DIR/anno_pkls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not Required for this dataset: Optionally you may create annotation pkl files for test & validation for your own dataset.\n",
    "# # Update the aicity.split config to test,val \n",
    "\n",
    "# !tao dataset annotations convert \\\n",
    "#         -e $SPECS_DIR/convert.yaml \\\n",
    "#         aicity.root=$DATA_DIR/MTMC_Tracking_2025/ \\\n",
    "#         aicity.split=test \\\n",
    "#         aicity.num_frames=10 \\\n",
    "#         results_dir=$DATA_DIR/anno_pkls\n",
    "\n",
    "# # List down all validation pkl files.\n",
    "# print(\"Listing all validation annotation pkl files:\")\n",
    "# !ls $HOST_DATA_DIR/anno_pkls/test\n",
    "# !ls $HOST_DATA_DIR/anno_pkls/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Required for this dataset: If you generated a dataset using Nvidia Issac Sim Replicator tool use the below experiment spec.\n",
    "\n",
    "# !tao dataset annotations convert \\\n",
    "#         -e $SPECS_DIR/convert.yaml \\\n",
    "#         aicity.root=$DATA_DIR/MTMC_Tracking_2025/ \\\n",
    "#         aicity.rgb_format='h5' \\\n",
    "#         aicity.depth_format='h5'\n",
    "#         aicity.split=\"\" \\\n",
    "#         results_dir=$DATA_DIR/anno_pkls\n",
    "\n",
    "# # List down all validation pkl files.\n",
    "# print(\"Listing all validation annotation pkl files:\")\n",
    "# !ls $HOST_DATA_DIR/anno_pkls/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.machine() == \"x86_64\":\n",
    "    os.environ[\"CLI\"]=\"ngccli_linux.zip\"\n",
    "else:\n",
    "    os.environ[\"CLI\"]=\"ngccli_arm64.zip\"\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tao/sparse4d_rn101:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/sparse4d_rn101:trainable_v1.0 --dest $HOST_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $HOST_MODEL_DIR/sparse4d_rn101_vtrainable_v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * num_frames: number of frames present in 1 scene\n",
    "    * batch_size: batch size for dataloader\n",
    "    * num_bev_groups: number of Birds Eye View (BEV) group pkl files used for training\n",
    "    * num_workers: number of workers\n",
    "    * classes: list of training classes \n",
    "    * data_root: path to the main dataset folder consisting of the raw image folders, calibration & ground truth\n",
    "    * use_h5_file: flag to identify the dataset folder type\n",
    "    * train_dataset:\n",
    "        ann_file: path to the annotation pkl file generated using the TAO dataset convert tool\n",
    "        sequences_split_num: 100\n",
    "    * val_dataset:\n",
    "        ann_file: path to the annotation pkl file generated using the TAO dataset convert tool\n",
    "    * test_dataset:\n",
    "        ann_file: path to the annotation pkl file generated using the TAO dataset convert tool\n",
    "* model: configure the model setting\n",
    "    * use_temporal_align: enable enhanced anchor matching across multiple frames for better tracking\n",
    "    * instance_bank:\n",
    "        anchor: path to the initilaized anchor numpy file\n",
    "* train: configure the training hyperparameters\n",
    "    * num_gpus: number of gpus \n",
    "    * num_nodes: number of nodes (num_nodes=1 for single node)\n",
    "    * validation_interval: evaluate the model every n intervals\n",
    "    * checkpoint_interval: saves a checkpoint every n intervals\n",
    "    * optim:\n",
    "        * lr: learning rate for the rest of the model\n",
    "    * num_epochs: number of epochs\n",
    "    * pretrained_backbone_path: path to the pretrained model\n",
    "    * precision: If set to bf16, the training is run on Automatic Mixed Precision (AMP)\n",
    "\n",
    "Please refer to the TAO documentation about Sparse4D to get all the parameters that are configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $HOST_SPECS_DIR/experiment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* *WARNING*: We train Sparse4D on 1408x512p images, 12 cameras x 5 mins @ 30 FPS which requires significant amount of time, **we highly recommend that you run training with multiple high-end gpus (e.g. H100, A100, etc)**. Please refer to the model card & documentation to get an estimate on the training time.\n",
    "* Sparse4D model per-epoch training time on a single GPU (the hours may vary depending on the data location, network speed, and etc)\n",
    "* By default model is running on `train.precision=bf16-mixed` for mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, change train.num_gpus in train.yaml based on your machine\")\n",
    "print(\"For multi-node, change train.num_gpus and num_nodes in train.yaml based on your machine\")\n",
    "# If you face out of memory issue, you may reduce the batch size in the spec file by passing dataset.batch_size=2\n",
    "\n",
    "!tao model sparse4d train \\\n",
    "        -e $SPECS_DIR/experiment.yaml \\\n",
    "        train.num_gpus=1 \\\n",
    "        train.num_nodes=1 \\\n",
    "        dataset.data_root=$DATA_DIR/MTMC_Tracking_2025/train \\\n",
    "        dataset.train_dataset.ann_file=$DATA_DIR/anno_pkls/train/ \\\n",
    "        dataset.val_dataset.ann_file=$DATA_DIR/anno_pkls/train/Warehouse_014+bev-sensor-random-0_infos_train.pkl \\\n",
    "        model.head.instance_bank.anchor=$DATA_DIR/anno_pkls/anchor_init_kmeans900.npy \\\n",
    "        results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set NUM_EPOCH to the epoch corresponding to any saved checkpoint\n",
    "# %env NUM_EPOCH=029\n",
    "\n",
    "# Get the name of the checkpoint corresponding to your set epoch\n",
    "# tmp=!ls $HOST_RESULTS_DIR/train/*.pth | grep epoch_$NUM_EPOCH\n",
    "# %env CHECKPOINT={tmp[0]}\n",
    "\n",
    "# Or get the latest checkpoint\n",
    "os.environ[\"CHECKPOINT\"] = os.path.join(os.getenv(\"HOST_RESULTS_DIR\"), \"train/sparse4d_model_latest.pth\")\n",
    "\n",
    "print('Rename a trained model: ')\n",
    "print('---------------------')\n",
    "!sudo cp $CHECKPOINT $HOST_RESULTS_DIR/train/sparse4d_model_finetuned.pth\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train/sparse4d_model_finetuned.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a trained model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the mAP metric.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration.\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * data_root: path to the main dataset folder consisting of the raw image folders, calibration & ground truth\n",
    "    * test_dataset:\n",
    "        ann_file: path to the annotation pkl file generated using the TAO dataset convert tool\n",
    "    * batch_size: evaluation is supported on only 1 GPU.\n",
    "* evaluate:\n",
    "    * checkpoint: path to the model checkpoint\n",
    "\n",
    "* **NOTE: You need to change the model path in evaluate.yaml file based on your setting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TAO model\n",
    "!tao model sparse4d evaluate \\\n",
    "        -e $SPECS_DIR/experiment.yaml \\\n",
    "        evaluate.checkpoint=$RESULTS_DIR/train/sparse4d_model_finetuned.pth \\\n",
    "        dataset.data_root=$DATA_DIR/MTMC_Tracking_2025/train \\\n",
    "        dataset.test_dataset.ann_file=$DATA_DIR/anno_pkls/train/Warehouse_014+bev-sensor-random-0_infos_train.pkl \\\n",
    "        model.head.instance_bank.anchor=$DATA_DIR/anno_pkls/anchor_init_kmeans900.npy \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces annotated image outputs and 3D bounding box & object id output file in NVSchema.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * test_dataset:\n",
    "        ann_file: path to the annotation pkl file generated using the TAO dataset convert tool\n",
    "    * batch_size: evaluation is supported on only 1 GPU.\n",
    "* inference:\n",
    "    * checkpoint: path to the model checkpoint\n",
    "    * output_nvschema: boolean to save output in NVSchema format\n",
    "* vis:\n",
    "  * show: boolean to enable visualization\n",
    "  * vis_dir: path to store the output visualization results\n",
    "  * vis_score_threshold: threshold to filter away objects with low confidence for visualization\n",
    "  * n_images_col: no of images to be set horizontally in the grid visualization \n",
    "\n",
    "* **NOTE: You need to change the model path in infer.yaml file based on your setting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model sparse4d inference \\\n",
    "        -e $SPECS_DIR/experiment.yaml \\\n",
    "        inference.checkpoint=$RESULTS_DIR/train/sparse4d_model_finetuned.pth \\\n",
    "        dataset.data_root=$DATA_DIR/MTMC_Tracking_2025/train \\\n",
    "        dataset.test_dataset.ann_file=$DATA_DIR/anno_pkls/train/Warehouse_014+bev-sensor-random-0_infos_train.pkl \\\n",
    "        model.head.instance_bank.anchor=$DATA_DIR/anno_pkls/anchor_init_kmeans900.npy \\\n",
    "        visualize.show=True \\\n",
    "        visualize.vis_dir=$RESULTS_DIR \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine images to video via ffmpeg.\n",
    "!sudo apt install ffmpeg\n",
    "\n",
    "import os\n",
    "from IPython.display import Video\n",
    "\n",
    "visdir = os.path.join(os.environ['HOST_RESULTS_DIR'], 'visual_trk')\n",
    "\n",
    "!ffmpeg -framerate 24 -pattern_type glob -i '{visdir}/*.jpg' -c:v libx264 -pix_fmt yuv420p '{visdir}/output.mp4'\n",
    "\n",
    "Video(os.path.join(visdir, \"output.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export model to ONNX <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the model to ONNX model. The obtained ONNX model file can now be used for Deepstream deployment.\n",
    "!tao model sparse4d export \\\n",
    "        -e $SPECS_DIR/experiment.yaml \\\n",
    "        export.checkpoint=$RESULTS_DIR/train/sparse4d_model_finetuned.pth \\\n",
    "        export.onnx_file=$RESULTS_DIR/export/sparse4d_model_finetuned.onnx \\\n",
    "        model.head.instance_bank.anchor=$DATA_DIR/anno_pkls/anchor_init_kmeans900.npy \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
